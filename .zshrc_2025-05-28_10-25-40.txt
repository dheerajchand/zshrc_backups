# COMPLETE ZSHRC - All Functions Restored with Safe Startup
# This version keeps all your advanced functions but prevents startup issues

# =====================================================
# CORE SHELL SETUP
# =====================================================

# Path to your oh-my-zsh configuration.
export ZSH=$HOME/.dotfiles/oh-my-zsh

export ZSH_THEME="powerlevel9k/powerlevel9k"
POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(dir nvm vcs)
POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status history time)

export CASE_SENSITIVE="true"
export DISABLE_AUTO_TITLE="true"

plugins=(colorize compleat dirpersist autojump git gulp history cp)
source $ZSH/oh-my-zsh.sh

autoload -U add-zsh-hook

# =====================================================
# NODE/NVM SETUP
# =====================================================

export NVM_DIR="$HOME/.nvm"
[ -s "/opt/homebrew/opt/nvm/nvm.sh" ] && . "/opt/homebrew/opt/nvm/nvm.sh"
[ -s "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm" ] && . "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm"

load-nvmrc() {
  if [[ -f .nvmrc && -r .nvmrc ]]; then
    nvm use &> /dev/null
  else
    nvm use stable
  fi
}
add-zsh-hook chpwd load-nvmrc
load-nvmrc

unsetopt correct

# =====================================================
# BASIC ENVIRONMENT
# =====================================================

# MacOS things
defaults write -g ApplePressAndHoldEnabled -bool true

export WORKING_ON_LAPTOP="True"

# Default editor
export EDITOR="zed"
export VISUAL="zed"

# Use Neovim as fallback in the terminal when Zed is unavailable
alias vim="nvim"
alias edit="nvim"

# =====================================================
# PYTHON SETUP
# =====================================================

eval "$(pyenv init -)"
eval "$(pyenv virtualenv-init -)"
eval "$(pyenv init --path)"

function cleanvenv {
    pip freeze | grep -v "^-e" | xargs pip uninstall -y
}

function remove_python_cruft {
    find . -name "*.pyc" -delete
    find . -name "__pycache__" -exec rm -r {} +
}

export PREFERRED_VENV="geo31111"
pyenv activate $PREFERRED_VENV

# =====================================================
# UTILITY FUNCTIONS
# =====================================================

export ZSHRC_BACKUPS=~/.zshrc_backups
mkdir -p "$ZSHRC_BACKUPS"

function backup_zshrc {
    local prev_dir="$(pwd)"
    timestamp=$(date +"%Y-%m-%d_%H-%M-%S")
    backup_file="$ZSHRC_BACKUPS/.zshrc_$timestamp.txt"
    log_file="$ZSHRC_BACKUPS/zshrc_backup_log.txt"

    if [[ ! -d "$ZSHRC_BACKUPS/.git" ]]; then
        echo "‚ö†Ô∏è Backup directory is not a Git repository. Initializing..."
        git -C "$ZSHRC_BACKUPS" init
        git -C "$ZSHRC_BACKUPS" remote add origin "<YOUR_GIT_REPO_URL>"
    fi

    cp ~/.dotfiles/homedir/.zshrc "$backup_file"
    echo "$timestamp - Backup saved: $backup_file" >> "$log_file"

    git -C "$ZSHRC_BACKUPS" add .
    git -C "$ZSHRC_BACKUPS" commit -m "Backup .zshrc at $timestamp"
    git -C "$ZSHRC_BACKUPS" push origin main

    echo "‚úÖ Backup created at $backup_file"
    echo "üìú Logged in $log_file"
    echo "üöÄ Changes committed & pushed to Git repository!"

    cd "$prev_dir"
    echo "üîÑ Returned to: $prev_dir"
}

function zshreboot {
    source ~/.zshrc
}

function zshconfig {
    zed ~/.config/zsh/zshrc
}

# =====================================================
# DATABASE SETTINGS
# =====================================================

export PGHOST="localhost"
export PGUSER="dheerajchand"
export PGPASSWORD="dessert"
export PGPORT="5432"
export PGDATABASE="gis"

export GEODJANGO_TEMPLATE_SQL_DATABASE="geodjango_template_db"
export GEODJANGO_TEMPLATE_SQL_USER="dheerajchand"
export GEODJANGO_TEMPLATE_SQL_PASSWORD="dessert"
export GEODJANGO_TEMPLATE_SQL_PORT="5432"

# =====================================================
# DOCKER & GIS
# =====================================================

export PATH="/Users/dheerajchand/.rd/bin:$PATH"
export DEFAULT_DOCKER_CONTEXT="rancher-desktop"

# GIS things
export GDAL_LIBRARY_PATH="$(gdal-config --prefix)/lib/libgdal.dylib"
export GEOS_LIBRARY_PATH="$(geos-config --prefix)/lib/libgeos_c.dylib"

function update_local_repo {
    for remote in `git branch -r`; do git branch --track ${remote#origin/} $remote; done
}
export GIT_DISCOVERY_ACROSS_FILESYSTEM=1

# =====================================================
# INTERNET CONNECTIVITY CHECK
# =====================================================

function is_online {
    ping -c 1 google.com &> /dev/null && echo "online" || echo "offline"
}

# =====================================================
# ZEPPELIN SETUP & NOTEBOOK FUNCTIONS (ALL RESTORED)
# =====================================================

export ZEPPELIN_HOME="$HOME/zeppelin"
export PATH="$ZEPPELIN_HOME/bin:$PATH"
mkdir -p "$ZEPPELIN_HOME"

check_zeppelin() {
    if [[ ! -d "$ZEPPELIN_HOME" ]]; then
        echo "‚ùå Zeppelin directory not found: $ZEPPELIN_HOME"
        return 1
    elif ! "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" status > /dev/null 2>&1; then
        echo "Zeppelin is not running."
        return 1
    else
        echo "‚úÖ Zeppelin is running."
        return 0
    fi
}

function start_zeppelin {
    echo "Starting Zeppelin on port 9090..."

    # Ensure Zeppelin is stopped before starting fresh
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" stop

    # Explicitly set Zeppelin's port to 9090 in config
    sed -i '' 's/<value>8080<\/value>/<value>9090<\/value>/' "$ZEPPELIN_HOME/conf/zeppelin-site.xml"

    # Start Zeppelin with the new port configuration
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" start

    sleep 3  # Wait for Zeppelin to initialize

    echo "‚úÖ Zeppelin started at: http://localhost:9090"
}

function stop_zeppelin() {
    echo "Stopping Zeppelin..."
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" stop
    echo "Zeppelin stopped."
}

function restart_zeppelin() {
    stop_zeppelin
    start_zeppelin
}

function reset_zeppelin() {
    stop_zeppelin
    sleep 3

    echo "üîç Ensuring Zeppelin is fully stopped..."
    if ps aux | grep -i 'zeppelin' | grep -v 'grep' > /dev/null; then
        echo "‚ùå Zeppelin is still running! Killing processes..."
        pkill -9 -f 'zeppelin'
        sleep 2
    fi

    # Cleanup
    [ -d "$ZEPPELIN_HOME/run" ] && rm -rf "$ZEPPELIN_HOME/run"
    [ -d "$ZEPPELIN_HOME/logs" ] && rm -rf "$ZEPPELIN_HOME/logs"
    [ -d "$ZEPPELIN_HOME/local-repo/spark" ] && rm -rf "$ZEPPELIN_HOME/local-repo/spark"

    start_zeppelin
    sleep 3  # Allow restart

    # ‚úÖ Verify Zeppelin is running
    if ps aux | grep -i 'zeppelin' | grep -v 'grep' > /dev/null; then
        echo "‚úÖ Zeppelin restarted successfully!"
    else
        echo "‚ùå Zeppelin failed to start. Check logs!"
        tail -n 50 "$ZEPPELIN_HOME/logs/zeppelin.log"
    fi
}

function check_pyspark_dependencies() {
    if ! command -v pyspark &> /dev/null; then
        echo "‚ùå PySpark not found! Install it with: pip install pyspark"
        return 1
    fi

    if ! command -v jupyter &> /dev/null && ! command -v "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" &> /dev/null; then
        echo "‚ùå Neither Jupyter nor Zeppelin found! Install Jupyter: pip install notebook"
        return 1
    fi

    echo "‚úÖ Dependencies verified!"
    return 0
}

function check_notebook_dependencies {
    local notebook_type="$1"

    case "$notebook_type" in
        jupyter|jupyterlab)
            if ! command -v pyspark &> /dev/null; then
                echo "‚ùå PySpark not found! Install it with: pip install pyspark"
                return 1
            fi
            if ! command -v jupyter &> /dev/null; then
                echo "‚ùå Jupyter not found! Install it with: pip install notebook"
                return 1
            fi
            ;;
        zeppelin)
            if [[ ! -d "$ZEPPELIN_HOME" ]] || ! command -v "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" &> /dev/null; then
                echo "‚ùå Zeppelin is missing! Ensure it's installed and configured."
                return 1
            fi
            ;;
        databricks)
            if ! command -v databricks &> /dev/null; then
                echo "‚ùå Databricks CLI not found! Install it with: pip install databricks-cli"
                return 1
            fi
            ;;
        vscode)
            if ! command -v code &> /dev/null; then
                echo "‚ùå VS Code not found! Ensure it's installed and available in PATH."
                return 1
            fi
            ;;
        *)
            echo "‚ùå Unknown notebook type: $notebook_type"
            return 1
            ;;
    esac

    echo "‚úÖ Dependencies verified for $notebook_type!"
    return 0
}

function notebook_manager {
    local notebook_type="$1"
    local port="${2:-8888}"  # Default to port 8888
    local notebook_dir="${3:-$(pwd)}"  # Default to current directory

    # ‚úÖ Validate dependencies dynamically based on requested notebook type
    check_notebook_dependencies "$notebook_type" || return 1

    case "$notebook_type" in
        jupyter)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="notebook --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting PySpark Jupyter Notebook on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Notebook launch failed!"; return 1; }
            ;;
        jupyterlab)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="lab --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting Jupyter Lab on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Lab launch failed!"; return 1; }
            ;;
        zeppelin)
            echo "üöÄ Starting Zeppelin Notebook..."
            start_zeppelin || { echo "‚ùå Zeppelin launch failed!"; return 1; }
            ;;
        databricks)
            echo "üöÄ Launching Databricks CLI..."
            databricks workspace import_dir "$notebook_dir" || { echo "‚ùå Databricks CLI failed!"; return 1; }
            ;;
        vscode)
            echo "üöÄ Opening VS Code for notebook editing..."
            code "$notebook_dir" || { echo "‚ùå VS Code launch failed!"; return 1; }
            ;;
        *)
            echo "‚ùå Invalid notebook type: $notebook_type. Available options: jupyter, jupyterlab, zeppelin, databricks, vscode."
            return 1
            ;;
    esac
}

function pyspark_notebook {
    local notebook_type="${1:-jupyter}"  # Default to Jupyter if no type is provided
    notebook_manager "$notebook_type"
}

# =====================================================
# JAVA SETUP (Manual - No Auto-Detection)
# =====================================================

export JAVA_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/java/current"
export PATH="$JAVA_HOME/bin:$PATH"
export LD_LIBRARY_PATH="$JAVA_HOME/lib:$LD_LIBRARY_PATH"

# Manual setup function (can be called when needed)
function setup_java_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Detecting best Java version for Hadoop & Spark..."
        local best_java_version=$(sdk list java | awk '/11\.0/ {print $NF}' | sort -r | head -n 1)

        if [[ -n "$best_java_version" ]]; then
            sdk install java $best_java_version
            sdk default java $best_java_version
            export JAVA_HOME=$(sdk home java $best_java_version)
            export PATH=$JAVA_HOME/bin:$PATH
            export LD_LIBRARY_PATH=$JAVA_HOME/lib:$LD_LIBRARY_PATH
            echo "‚úÖ Java version set to $best_java_version"
        fi
    else
        echo "‚ö†Ô∏è  Offline - using current Java installation"
    fi
}

# =====================================================
# HADOOP SETUP (Manual - No Auto-Detection)
# =====================================================

export HADOOP_CURRENT_VERSION="3.3.6"
export HADOOP_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/hadoop/current"
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH"
export HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop/"
export PATH="$HADOOP_HOME/bin:$PATH"

# Manual setup function (can be called when needed)
function setup_hadoop_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Detecting latest compatible Hadoop version..."
        local latest_version=$(sdk list hadoop | grep '3.3' | grep -v '[*>+-]' | grep -v '3.3.5' | sort -r | head -n 1 | awk '{print $1}')

        if [[ -n "$latest_version" ]]; then
            sdk install hadoop $latest_version
            export HADOOP_CURRENT_VERSION="$latest_version"
            export HADOOP_HOME=$(sdk home hadoop $HADOOP_CURRENT_VERSION)
            export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
            export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
            export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop/
            export PATH="$HADOOP_HOME/bin:$PATH"
            echo "‚úÖ Hadoop version set to $HADOOP_CURRENT_VERSION"
        fi
    else
        echo "‚ö†Ô∏è  Offline - using current Hadoop installation"
    fi
}

# =====================================================
# MAVEN SETUP
# =====================================================

DEFAULT_MAVEN_VERSION="3.9.6"

function setup_maven() {
    if command -v mvn &>/dev/null; then
        echo "Maven is installed: $(mvn -version | head -n 1)"
    else
        echo "Maven not found. Installing Maven $DEFAULT_MAVEN_VERSION via SDKMAN..."
        sdk install maven "$DEFAULT_MAVEN_VERSION" || {
            echo "Failed to install Maven. Please check SDKMAN setup."
            return 1
        }

        if command -v mvn &>/dev/null; then
            echo "Maven installation successful: $(mvn -version | head -n 1)"
        else
            echo "Maven installation failed."
        fi
    fi
}

function download_maven_jars {
    local libraries="${1:-$DEFAULT_SPARK_JARS}"
    local target_path="${2:-$LOCAL_SPARK_JAR_PATH}"
    mkdir -p "$target_path"

    echo "üöÄ Downloading Maven dependencies..."
    for lib in $(echo "$libraries" | tr ',' ' '); do
        IFS=':' read -r group artifact version <<< "$lib"
        jar_file="${artifact}-${version}.jar"
        mvn_url="https://repo1.maven.org/maven2/$(echo $group | tr '.' '/')/$artifact/$version/$jar_file"

        echo "üîç Fetching: $jar_file"
        curl -sL -o "$target_path/$jar_file" "$mvn_url" && echo "‚úÖ Saved to $target_path/$jar_file" || echo "‚ùå Failed to download $jar_file"
    done
}

# ===============================================================================
# COMPLETE SPARK CONFIGURATION - ALL YOUR FUNCTIONS RESTORED
# ===============================================================================

# =====================================================
# SPARK ENVIRONMENT (Manual Setup)
# =====================================================

export SPARK_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/spark/current"
export SPARK_LOCAL_IP="127.0.0.1"
export SPARK_MASTER_HOST="127.0.0.1"
export SPARK_MASTER_PORT="7077"
export SPARK_WORKER_INSTANCES="$(sysctl -n hw.ncpu 2>/dev/null || echo '4')"
export SPARK_DRIVER_MEMORY="4g"
export SPARK_EXECUTOR_MEMORY="4g"
export SPARK_WORKER_MEMORY="2g"
export SPARK_CONF_DIR="$SPARK_HOME/conf"
export SPARK_CLIENT_CONFIG="$HOME/.spark-client-defaults.properties"

# Python paths
export PYSPARK_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"
export PYSPARK_DRIVER_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"

# Add Spark to PATH
export PATH="$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"

# =====================================================
# SPARK DEPENDENCIES
# =====================================================

export DEFAULT_SPARK_JARS="org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.1,org.datasyslab:geotools-wrapper:1.7.1-28.5,graphframes:graphframes:0.8.3-spark3.5-s_2.12"
export LOCAL_SPARK_JAR_PATH="$HOME/local_jars"
mkdir -p "$LOCAL_SPARK_JAR_PATH"

function get_spark_dependencies {
    local online_status=$(is_online)

    if [[ "$online_status" == "online" ]]; then
        echo "--packages $DEFAULT_SPARK_JARS"
    else
        local local_jars=$(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | tr '\n' ',' | sed 's/,$//')
        if [[ -n "$local_jars" ]]; then
            echo "--jars $local_jars"
        else
            echo ""
        fi
    fi
}

export PYSPARK_SUBMIT_ARGS="$(get_spark_dependencies) pyspark-shell"

# =====================================================
# CORE SPARK SUBMIT FUNCTIONS
# =====================================================

function default_spark_submit() {
    local py_file="$1"
    if [[ -z "$py_file" ]]; then
        echo "Usage: default_spark_submit <python_file>"
        return 1
    fi

    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi

    local dependencies=$(get_spark_dependencies)

    local cmd="spark-submit \
        --master 'local[*]' \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --properties-file '$SPARK_CLIENT_CONFIG' \
        $dependencies \
        '$py_file'"

    echo "üöÄ Executing LOCAL Spark job: $py_file"
    echo "Command: $cmd"
    eval "$cmd"
}

# =====================================================
# SPARK SHELL FUNCTIONS
# =====================================================

function default_spark_shell() {
    local dependencies=$(get_spark_dependencies)

    local cmd="spark-shell \
        --master 'local[*]' \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --properties-file '$SPARK_CLIENT_CONFIG' \
        $dependencies"

    echo "üöÄ Starting Spark Shell (Scala) with dependencies..."
    echo "Command: $cmd"
    eval "$cmd"
}

function default_pyspark_shell() {
    local dependencies=$(get_spark_dependencies)

    local cmd="pyspark \
        --master 'local[*]' \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --properties-file '$SPARK_CLIENT_CONFIG' \
        $dependencies"

    echo "üöÄ Starting PySpark Shell (Python) with dependencies..."
    echo "Command: $cmd"
    eval "$cmd"
}

# =====================================================
# SPARK DIAGNOSTICS (ALL YOUR FUNCTIONS)
# =====================================================

function diagnose_spark_startup {
    echo "üîç Diagnosing Spark startup issues..."

    # Check SPARK_HOME
    echo "1Ô∏è‚É£ Checking SPARK_HOME..."
    if [[ -z "$SPARK_HOME" ]]; then
        echo "‚ùå SPARK_HOME is not set"
        return 1
    elif [[ ! -d "$SPARK_HOME" ]]; then
        echo "‚ùå SPARK_HOME directory doesn't exist: $SPARK_HOME"
        return 1
    else
        echo "‚úÖ SPARK_HOME: $SPARK_HOME"
    fi

    # Check required files
    echo "2Ô∏è‚É£ Checking required Spark files..."
    local required_files=(
        "$SPARK_HOME/bin/spark-submit"
        "$SPARK_HOME/sbin/start-master.sh"
        "$SPARK_HOME/sbin/start-worker.sh"
    )

    for file in "${required_files[@]}"; do
        if [[ -f "$file" ]]; then
            echo "‚úÖ Found: $file"
        else
            echo "‚ùå Missing: $file"
        fi
    done

    # Check Java
    echo "3Ô∏è‚É£ Checking Java configuration..."
    if [[ -z "$JAVA_HOME" ]]; then
        echo "‚ùå JAVA_HOME is not set"
    elif [[ ! -d "$JAVA_HOME" ]]; then
        echo "‚ùå JAVA_HOME directory doesn't exist: $JAVA_HOME"
    else
        echo "‚úÖ JAVA_HOME: $JAVA_HOME"
        echo "‚úÖ Java version: $($JAVA_HOME/bin/java -version 2>&1 | head -1)"
    fi

    # Check port availability
    echo "4Ô∏è‚É£ Checking port availability..."
    local ports=("7077" "8080" "6066")
    for port in "${ports[@]}"; do
        if lsof -ti:$port >/dev/null 2>&1; then
            local process=$(lsof -ti:$port | xargs ps -p 2>/dev/null | tail -1)
            echo "‚ö†Ô∏è  Port $port is in use by: $process"
        else
            echo "‚úÖ Port $port is available"
        fi
    done

    # Check log directory
    echo "5Ô∏è‚É£ Checking log directory..."
    local log_dir="$SPARK_HOME/logs"
    if [[ ! -d "$log_dir" ]]; then
        echo "Creating log directory: $log_dir"
        mkdir -p "$log_dir"
    fi
    echo "‚úÖ Log directory: $log_dir"

    # Check recent log files
    echo "6Ô∏è‚É£ Checking recent Spark logs..."
    local master_logs=($(find "$log_dir" -name "*Master*" -type f -mtime -1 2>/dev/null | head -3))
    if [[ ${#master_logs[@]} -gt 0 ]]; then
        echo "Recent master log files:"
        for log in "${master_logs[@]}"; do
            echo "üìÑ $log"
            echo "Last 5 lines:"
            tail -5 "$log" 2>/dev/null | sed 's/^/   /'
            echo
        done
    else
        echo "‚ÑπÔ∏è  No recent master log files found"
    fi
}

function show_spark_master_logs {
    echo "üìã Showing Spark Master logs..."
    local log_dir="$SPARK_HOME/logs"

    local latest_master_log=$(find "$log_dir" -name "*Master*" -type f 2>/dev/null | xargs ls -t 2>/dev/null | head -1)

    if [[ -n "$latest_master_log" ]]; then
        echo "üìÑ Latest master log: $latest_master_log"
        echo "========================================"
        tail -20 "$latest_master_log"
        echo "========================================"
    else
        echo "‚ùå No master log files found in $log_dir"
        echo "Available log files:"
        ls -la "$log_dir" 2>/dev/null || echo "Log directory doesn't exist or is empty"
    fi
}

function fix_spark_master_startup {
    echo "üîß Attempting to fix Spark master startup issues..."

    echo "1Ô∏è‚É£ Cleaning up any existing Spark processes..."
    pkill -f 'org.apache.spark.deploy.master.Master' 2>/dev/null
    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null
    sleep 2

    echo "2Ô∏è‚É£ Cleaning up stale PID files..."
    rm -f "$SPARK_HOME/logs/spark-*-master.pid" 2>/dev/null
    rm -f "$SPARK_HOME/logs/spark-*-worker-*.pid" 2>/dev/null

    echo "3Ô∏è‚É£ Checking directory permissions..."
    chmod +x "$SPARK_HOME/sbin/start-master.sh" 2>/dev/null
    chmod +x "$SPARK_HOME/sbin/start-worker.sh" 2>/dev/null
    chmod +x "$SPARK_HOME/sbin/stop-all.sh" 2>/dev/null

    echo "4Ô∏è‚É£ Creating missing directories..."
    mkdir -p "$SPARK_HOME/logs"
    mkdir -p "$SPARK_HOME/work"

    echo "5Ô∏è‚É£ Checking Java compatibility..."
    if [[ -n "$JAVA_HOME" ]]; then
        local java_version=$($JAVA_HOME/bin/java -version 2>&1 | head -1 | grep -o '"[0-9.]*"' | tr -d '"')
        echo "Java version: $java_version"

        export SPARK_DAEMON_JAVA_OPTS="-Djava.net.preferIPv4Stack=true"
        export SPARK_MASTER_OPTS="-Djava.net.preferIPv4Stack=true"
        export SPARK_WORKER_OPTS="-Djava.net.preferIPv4Stack=true"
    fi

    echo "‚úÖ Cleanup complete. Try starting the master again."
}

# =====================================================
# CLUSTER MANAGEMENT (ALL YOUR FUNCTIONS)
# =====================================================

function stop_spark {
    echo "üõë Stopping Spark processes..."

    if [[ -f "$SPARK_HOME/sbin/stop-all.sh" ]]; then
        "$SPARK_HOME/sbin/stop-all.sh"
    fi

    pkill -f 'org.apache.spark.deploy.master.Master' 2>/dev/null
    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null

    unset SPARK_MASTER_URL

    echo "‚úÖ Spark processes stopped"
}

function check_worker_registration {
    local master_ui="http://${SPARK_MASTER_HOST}:8080"
    local max_attempts=10
    local attempt=1

    echo "Checking worker registration at $master_ui..."

    while [[ $attempt -le $max_attempts ]]; do
        if command -v curl >/dev/null 2>&1; then
            local workers_json=$(curl -s "${master_ui}/json" 2>/dev/null | grep -o '"workers":\[[^]]*\]' 2>/dev/null)
            if [[ -n "$workers_json" ]] && [[ "$workers_json" != '"workers":[]' ]]; then
                echo "‚úÖ Workers are registered with master!"
                curl -s "${master_ui}/json" 2>/dev/null | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    workers = data.get('workers', [])
    print(f'Registered workers: {len(workers)}')
    for i, w in enumerate(workers):
        state = w.get('state', 'UNKNOWN')
        cores = w.get('cores', 0)
        memory = w.get('memory', 0)
        print(f'  Worker {i+1}: {state} - {cores} cores, {memory}MB memory')
except:
    print('Could not parse worker info')
" 2>/dev/null || echo "Workers registered but could not parse details"
                return 0
            fi
        fi

        echo "Attempt $attempt/$max_attempts - No workers registered yet..."
        sleep 3
        ((attempt++))
    done

    echo "‚ùå No workers registered after $max_attempts attempts"
    echo "üí° Check logs at: $SPARK_HOME/logs"
    return 1
}

function start_spark_workers {
    echo "üöÄ Starting Spark workers..."

    if [[ -z "$SPARK_HOME" ]]; then
        echo "‚ùå SPARK_HOME not set. Cannot start workers."
        return 1
    fi

    export SPARK_LOCAL_IP="127.0.0.1"
    export SPARK_WORKER_LOG_DIR="${SPARK_HOME}/logs"
    mkdir -p "${SPARK_WORKER_LOG_DIR}"

    local instances="${SPARK_WORKER_INSTANCES:-2}"
    local master_url="spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}"

    echo "Starting $instances worker(s) connecting to $master_url"

    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null
    sleep 2

    if [[ -f "$SPARK_HOME/sbin/start-worker.sh" ]]; then
        for i in $(seq 1 $instances); do
            echo "Starting worker $i using start-worker.sh..."
            "$SPARK_HOME/sbin/start-worker.sh" "$master_url"
            sleep 1
        done
    else
        echo "‚ùå start-worker.sh not found at $SPARK_HOME/sbin/"
        return 1
    fi

    sleep 5

    local worker_count=$(ps aux | grep -c 'org.apache.spark.deploy.worker.Worker' | grep -v grep || echo 0)
    echo "‚úÖ Started $worker_count Spark worker process(es)"

    echo "üîç Checking worker registration..."
    check_worker_registration
}

function stop_spark_workers() {
    echo "üö´ Stopping Spark workers..."
    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null
    sleep 2
    if pgrep -f 'org.apache.spark.deploy.worker.Worker' > /dev/null; then
        echo "‚ö†Ô∏è Some Spark workers are still running!"
    else
        echo "‚úÖ All Spark workers stopped successfully!"
    fi
}

function check_spark_cluster_health {
    echo "üîç Performing comprehensive Spark cluster health check..."

    local master_running=$(ps aux | grep -c 'org.apache.spark.deploy.master.Master' | grep -v grep || echo 0)
    if [[ $master_running -eq 0 ]]; then
        echo "‚ùå Spark Master is not running"
        return 1
    fi
    echo "‚úÖ Spark Master is running"

    local worker_running=$(ps aux | grep -c 'org.apache.spark.deploy.worker.Worker' | grep -v grep || echo 0)
    if [[ $worker_running -eq 0 ]]; then
        echo "‚ùå No Spark Workers are running"
        return 1
    fi
    echo "‚úÖ $worker_running Spark Worker(s) are running"

    if command -v curl >/dev/null 2>&1; then
        if curl -s "http://$SPARK_MASTER_HOST:8080" >/dev/null 2>&1; then
            echo "‚úÖ Master UI is accessible at http://$SPARK_MASTER_HOST:8080"
        else
            echo "‚ö†Ô∏è  Master UI is not accessible"
        fi

        check_worker_registration
    else
        echo "‚ö†Ô∏è  curl not available - cannot check master UI"
    fi

    return 0
}

function start_local_spark_cluster() {
    echo "üöÄ Starting local Spark cluster..."

    if [[ -z "$SPARK_HOME" ]] || [[ ! -d "$SPARK_HOME" ]]; then
        echo "‚ùå SPARK_HOME not set or invalid: $SPARK_HOME"
        return 1
    fi

    stop_spark
    sleep 3

    echo "Starting Spark master..."
    if [[ -f "$SPARK_HOME/sbin/start-master.sh" ]]; then
        "$SPARK_HOME/sbin/start-master.sh"
    else
        echo "‚ùå start-master.sh not found at $SPARK_HOME/sbin/"
        return 1
    fi

    sleep 8

    if ! ps aux | grep -q 'org.apache.spark.deploy.master.Master' | grep -v grep; then
        echo "‚ùå Master failed to start"
        show_spark_master_logs
        return 1
    fi

    echo "‚úÖ Master started"

    start_spark_workers

    export SPARK_MASTER_URL="spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}"

    echo "‚úÖ Local Spark cluster started!"
    echo "   Master UI: http://$SPARK_MASTER_HOST:8080"
    echo "   Master URL: $SPARK_MASTER_URL"

    sleep 3
    if check_spark_cluster_health; then
        echo "‚úÖ Cluster health check passed!"
        return 0
    else
        echo "‚ö†Ô∏è  Cluster may not be fully ready. Check the UI and logs."
        return 1
    fi
}

function stop_local_spark_cluster() {
    echo "üõë Stopping local Spark cluster..."
    stop_spark
    unset SPARK_MASTER_URL
    echo "‚úÖ Local cluster stopped. SPARK_MASTER_URL unset (will default to local[*])"
}

# Your original graceful restart function (restored)
function graceful_spark_restart {
    echo "üîç Detecting active Spark instances..."
    detect_spark_instances

    read "confirm_stop?Would you like to stop all Spark processes before restarting? (y/n): "
    if [[ "$confirm_stop" == "y" ]]; then
        stop_spark
    fi

    sleep 5

    local retries=5
    for attempt in $(seq 1 $retries); do
        echo "üîç Ensuring Spark processes are fully stopped... (Attempt $attempt)"
        if [[ -z "$(ps aux | grep -i 'spark' | grep -v 'grep')" ]]; then
            echo "‚úÖ Spark processes stopped successfully."
            break
        fi

        echo "‚ö† Warning: Workers still running‚Äîretrying shutdown..."
        stop_spark
        sleep 5
        detect_spark_instances
    done

    echo "üöÄ Starting Spark master..."
    ${SPARK_HOME}/sbin/start-master.sh
    echo "‚úÖ Master running at http://${SPARK_MASTER_HOST}:8080"

    echo "üöÄ Starting fresh Spark workers..."
    start_spark_workers

    echo "‚úÖ Spark restart complete!"
}

function detect_spark_instances {
    echo "üîç Checking active Spark instances..."
    local running_processes=$(ps aux | grep -i 'spark' | grep -v 'grep')

    if [[ -z "$running_processes" ]]; then
        echo "‚ùå No active Spark instances detected."
        return 1
    else
        echo "‚úÖ Active Spark processes detected:"
        echo "$running_processes"
        return 0
    fi
}

# =====================================================
# SPARK TESTING FUNCTIONS (ALL RESTORED)
# =====================================================

function test_spark_local() {
    echo "üß™ Testing local Spark functionality..."

    local test_script="/tmp/spark_local_test.py"
    cat > "$test_script" << 'EOF'
from pyspark.sql import SparkSession
import sys

try:
    spark = SparkSession.builder.appName("LocalSparkTest").getOrCreate()

    print(f"‚úÖ Spark Context created successfully")
    print(f"   Master: {spark.sparkContext.master}")
    print(f"   App Name: {spark.sparkContext.appName}")
    print(f"   Default Parallelism: {spark.sparkContext.defaultParallelism}")

    # Simple computation test
    rdd = spark.sparkContext.parallelize(range(100), 4)
    result = rdd.map(lambda x: x * x).sum()
    print(f"   Computation result: {result}")

    # DataFrame test
    df = spark.range(100)
    count = df.count()
    print(f"   DataFrame count: {count}")

    # SQL test
    df.createOrReplaceTempView("test_table")
    sql_result = spark.sql("SELECT COUNT(*) as count FROM test_table").collect()[0]["count"]
    print(f"   SQL result: {sql_result}")

    print("‚úÖ Local Spark test completed successfully!")

except Exception as e:
    print(f"‚ùå Local Spark test failed: {e}")
    sys.exit(1)
finally:
    if 'spark' in locals():
        spark.stop()
EOF

    default_spark_submit "$test_script"
    local exit_code=$?
    rm -f "$test_script"
    return $exit_code
}

function test_spark_scala() {
    echo "üß™ Testing Spark Scala functionality..."

    local test_script="/tmp/spark_scala_test.scala"
    cat > "$test_script" << 'EOF'
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder
  .appName("ScalaSparkTest")
  .getOrCreate()

try {
  println("‚úÖ Spark Scala Context created successfully")
  println(s"   Master: ${spark.sparkContext.master}")
  println(s"   App Name: ${spark.sparkContext.appName}")
  println(s"   Default Parallelism: ${spark.sparkContext.defaultParallelism}")

  // Simple RDD test
  val rdd = spark.sparkContext.parallelize(1 to 100, 4)
  val result = rdd.map(x => x * x).sum()
  println(s"   RDD computation result: $result")

  // DataFrame test
  import spark.implicits._
  val df = spark.range(100).toDF("number")
  val count = df.count()
  println(s"   DataFrame count: $count")

  // SQL test
  df.createOrReplaceTempView("scala_test")
  val sqlResult = spark.sql("SELECT COUNT(*) as count FROM scala_test").collect()(0).getAs[Long]("count")
  println(s"   SQL result: $sqlResult")

  println("‚úÖ Spark Scala test completed successfully!")

} catch {
  case e: Exception =>
    println(s"‚ùå Spark Scala test failed: ${e.getMessage}")
    e.printStackTrace()
    System.exit(1)
} finally {
  spark.stop()
}
EOF

    echo "Running Spark Scala test..."
    local dependencies=$(get_spark_dependencies)

    local cmd="spark-shell \
        --master 'local[*]' \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --properties-file '$SPARK_CLIENT_CONFIG' \
        $dependencies \
        -i '$test_script'"

    eval "$cmd"
    local exit_code=$?
    rm -f "$test_script"
    return $exit_code
}

function test_distributed_spark() {
    echo "üß™ Testing distributed Spark functionality..."

    if [[ -z "$SPARK_MASTER_URL" ]]; then
        echo "‚ùå SPARK_MASTER_URL not set. Starting local cluster..."
        start_local_spark_cluster
        if [[ $? -ne 0 ]]; then
            echo "‚ùå Failed to start cluster"
            return 1
        fi
        sleep 5
    fi

    if ! check_spark_cluster_health; then
        echo "‚ùå Cluster health check failed"
        return 1
    fi

    echo "‚è≥ Waiting for workers to fully register..."
    sleep 10

    local test_script="/tmp/spark_distributed_test.py"
    cat > "$test_script" << 'EOF'
from pyspark.sql import SparkSession
import sys
import time

try:
    spark = SparkSession.builder \
        .appName("DistributedSparkTest") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.network.timeout", "120s") \
        .config("spark.executor.heartbeatInterval", "10s") \
        .config("spark.task.maxAttempts", "3") \
        .getOrCreate()

    print(f"‚úÖ Distributed Spark Context created successfully")
    print(f"   Master: {spark.sparkContext.master}")
    print(f"   App Name: {spark.sparkContext.appName}")
    print(f"   Default Parallelism: {spark.sparkContext.defaultParallelism}")

    print("‚è≥ Waiting for executors to be allocated...")
    time.sleep(15)

    # Test simple distributed computation
    print("üîç Testing distributed computation...")
    rdd = spark.sparkContext.parallelize(range(100), 4)
    result = rdd.map(lambda x: x * x).sum()
    print(f"   Small computation result: {result}")

    # Test larger distributed computation
    print("üîç Testing larger distributed computation...")
    rdd = spark.sparkContext.parallelize(range(1000), 10)
    result = rdd.map(lambda x: x * x).sum()
    print(f"   Large computation result: {result}")

    # DataFrame test with multiple partitions
    print("üîç Testing DataFrame operations...")
    df = spark.range(1000).repartition(8)
    count = df.count()
    partitions = df.rdd.getNumPartitions()
    print(f"   DataFrame count: {count} (across {partitions} partitions)")

    # SQL test
    print("üîç Testing SQL operations...")
    df.createOrReplaceTempView("distributed_test")
    sql_result = spark.sql("SELECT COUNT(*) as count FROM distributed_test").collect()[0]["count"]
    print(f"   SQL result: {sql_result}")

    print("‚úÖ Distributed Spark test completed successfully!")

except Exception as e:
    print(f"‚ùå Distributed Spark test failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
finally:
    if 'spark' in locals():
        spark.stop()
EOF

    echo "üöÄ Running distributed test with enhanced configuration..."
    distributed_spark_submit "$test_script"
    local exit_code=$?

    rm -f "$test_script"

    if [[ $exit_code -eq 0 ]]; then
        echo "‚úÖ Distributed Spark test passed!"
    else
        echo "‚ùå Distributed Spark test failed!"
        echo "üí° Check cluster status at: http://$SPARK_MASTER_HOST:8080"
        echo "üí° Check logs at: $SPARK_HOME/logs/"
    fi

    return $exit_code
}

function test_all_spark() {
    echo "üß™ Running comprehensive Spark tests..."

    echo "1Ô∏è‚É£ Testing local Spark (Python)..."
    if test_spark_local; then
        echo "‚úÖ Local Spark test passed"
    else
        echo "‚ùå Local Spark test failed"
        return 1
    fi

    echo -e "\n2Ô∏è‚É£ Testing Spark Scala..."
    if test_spark_scala; then
        echo "‚úÖ Spark Scala test passed"
    else
        echo "‚ùå Spark Scala test failed"
        return 1
    fi

    echo -e "\n3Ô∏è‚É£ Testing distributed Spark..."
    if test_distributed_spark; then
        echo "‚úÖ Distributed Spark test passed"
    else
        echo "‚ùå Distributed Spark test failed"
        return 1
    fi

    echo -e "\n‚úÖ All Spark tests completed successfully!"
}

# =====================================================
# DISTRIBUTED SPARK FUNCTIONS (ALL YOUR ADVANCED FEATURES)
# =====================================================

# Additional environment variables for distributed mode
export SPARK_MASTER_URL="${SPARK_MASTER_URL:-spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}}"
export SPARK_NUM_EXECUTORS="${SPARK_NUM_EXECUTORS:-8}"
export SPARK_EXECUTOR_CORES="${SPARK_EXECUTOR_CORES:-4}"
export SPARK_DRIVER_MAX_RESULT_SIZE="${SPARK_DRIVER_MAX_RESULT_SIZE:-2g}"

# Kubernetes configuration
export SPARK_K8S_MASTER="${SPARK_K8S_MASTER:-k8s://https://kubernetes.default.svc:443}"
export SPARK_K8S_IMAGE="${SPARK_K8S_IMAGE:-your-spark-image:latest}"
export SPARK_K8S_NAMESPACE="${SPARK_K8S_NAMESPACE:-default}"
export SPARK_K8S_SERVICE_ACCOUNT="${SPARK_K8S_SERVICE_ACCOUNT:-spark}"

function distributed_spark_submit() {
    local py_file="$1"
    local master_url="${2:-$SPARK_MASTER_URL}"

    if [[ -z "$py_file" ]]; then
        echo "Usage: distributed_spark_submit <python_file> [spark_master_url]"
        return 1
    fi

    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi

    if [[ -z "$master_url" ]]; then
        echo "‚ùå No Spark master URL provided. Set SPARK_MASTER_URL or provide as second argument."
        return 1
    fi

    local dependencies=$(get_spark_dependencies)
    local deploy_mode="client"

    local cmd="spark-submit \
        --master '$master_url' \
        --deploy-mode $deploy_mode \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --properties-file '$SPARK_CLIENT_CONFIG' \
        --conf spark.driver.maxResultSize=2g \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.sql.adaptive.coalescePartitions.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        --conf spark.network.timeout=800s \
        --conf spark.executor.heartbeatInterval=60s \
        $dependencies \
        '$py_file'"

    echo "üåê Executing DISTRIBUTED Spark job: $py_file"
    echo "   Master: $master_url"
    echo "   Deploy mode: $deploy_mode"
    echo "Command: $cmd"
    eval "$cmd"
}

function flexible_spark_submit() {
    local py_file="$1"
    local mode="${2:-local}"

    if [ -z "$py_file" ]; then
        echo "Usage: flexible_spark_submit <python_file> [mode]"
        echo "Modes: local, distributed, yarn, k8s"
        return 1
    fi

    case "$mode" in
        "local")
            echo "üè† Running in LOCAL mode"
            default_spark_submit "$py_file"
            ;;
        "distributed")
            echo "üåê Running in DISTRIBUTED mode"
            distributed_spark_submit "$py_file"
            ;;
        "yarn")
            echo "üêò Running on YARN cluster"
            yarn_spark_submit "$py_file"
            ;;
        "k8s")
            echo "‚ò∏Ô∏è  Running on Kubernetes"
            k8s_spark_submit "$py_file"
            ;;
        *)
            echo "‚ùå Unknown mode: $mode"
            return 1
            ;;
    esac
}

function yarn_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: yarn_spark_submit <python_file>"
        return 1
    fi

    local dependencies=$(get_spark_dependencies)

    local cmd="spark-submit \
        --master yarn \
        --deploy-mode cluster \
        --num-executors $SPARK_NUM_EXECUTORS \
        --executor-cores $SPARK_EXECUTOR_CORES \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --properties-file $SPARK_CLIENT_CONFIG \
        --conf spark.driver.maxResultSize=$SPARK_DRIVER_MAX_RESULT_SIZE \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.sql.adaptive.coalescePartitions.enabled=true \
        --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=$PYSPARK_PYTHON \
        --conf spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON=$PYSPARK_DRIVER_PYTHON \
        $dependencies \
        $py_file"

    echo "üêò Executing YARN Spark job"
    eval $cmd
}

function k8s_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: k8s_spark_submit <python_file>"
        return 1
    fi

    local dependencies=$(get_spark_dependencies)

    local cmd="spark-submit \
        --master $SPARK_K8S_MASTER \
        --deploy-mode cluster \
        --num-executors $SPARK_NUM_EXECUTORS \
        --executor-cores $SPARK_EXECUTOR_CORES \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --properties-file $SPARK_CLIENT_CONFIG \
        --conf spark.kubernetes.container.image=$SPARK_K8S_IMAGE \
        --conf spark.kubernetes.namespace=$SPARK_K8S_NAMESPACE \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName=$SPARK_K8S_SERVICE_ACCOUNT \
        --conf spark.kubernetes.executor.serviceAccountName=$SPARK_K8S_SERVICE_ACCOUNT \
        $dependencies \
        $py_file"

    echo "‚ò∏Ô∏è  Executing Kubernetes Spark job"
    eval $cmd
}

function smart_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: smart_spark_submit <python_file>"
        return 1
    fi

    echo "ü§ñ Auto-detecting Spark environment..."

    if command -v yarn >/dev/null 2>&1 && [ -n "$HADOOP_CONF_DIR" ]; then
        echo "‚úÖ YARN detected - using YARN mode"
        yarn_spark_submit "$py_file"
        return
    fi

    if command -v kubectl >/dev/null 2>&1 && [ -n "$SPARK_K8S_MASTER" ]; then
        echo "‚úÖ Kubernetes detected - using K8s mode"
        k8s_spark_submit "$py_file"
        return
    fi

    if [ -n "$SPARK_MASTER_URL" ] && [ "$SPARK_MASTER_URL" != "spark://127.0.0.1:7077" ]; then
        echo "‚úÖ Remote Spark master URL found - using distributed mode"
        distributed_spark_submit "$py_file"
        return
    fi

    if ps aux | grep -i "spark.deploy.master.Master" | grep -v "grep" > /dev/null; then
        echo "‚úÖ Local Spark master detected - using distributed mode"
        distributed_spark_submit "$py_file"
        return
    fi

    echo "‚ÑπÔ∏è  No cluster detected - falling back to local mode"
    default_spark_submit "$py_file"
}

function heavy_api_submit() {
    local py_file="$1"
    local mode="${2:-distributed}"

    if [ -z "$py_file" ]; then
        echo "Usage: heavy_api_submit <python_file> [mode]"
        echo "This function is optimized for API-heavy workloads"
        return 1
    fi

    local api_num_executors="${SPARK_NUM_EXECUTORS:-8}"
    local api_executor_cores="${SPARK_EXECUTOR_CORES:-4}"
    local api_executor_memory="${SPARK_EXECUTOR_MEMORY:-6g}"
    local api_driver_memory="${SPARK_DRIVER_MEMORY:-4g}"

    case "$mode" in
        "local")
            echo "üè† Running API-heavy job in LOCAL mode"
            default_spark_submit "$py_file"
            ;;
        "distributed"|*)
            local master_url="${SPARK_MASTER_URL}"
            if [ -z "$master_url" ]; then
                echo "‚ùå No SPARK_MASTER_URL set for distributed API processing"
                return 1
            fi

            local dependencies=$(get_spark_dependencies)
            local deploy_mode="client"

            local cmd="spark-submit \
                --master $master_url \
                --deploy-mode $deploy_mode \
                --num-executors $api_num_executors \
                --executor-cores $api_executor_cores \
                --executor-memory $api_executor_memory \
                --driver-memory $api_driver_memory \
                --properties-file $SPARK_CLIENT_CONFIG \
                --conf spark.driver.maxResultSize=$SPARK_DRIVER_MAX_RESULT_SIZE \
                --conf spark.sql.adaptive.enabled=true \
                --conf spark.sql.adaptive.coalescePartitions.enabled=true \
                --conf spark.sql.shuffle.partitions=$((api_num_executors * api_executor_cores * 3)) \
                --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
                --conf spark.sql.execution.arrow.pyspark.enabled=true \
                --conf spark.network.timeout=800s \
                --conf spark.executor.heartbeatInterval=60s \
                --conf spark.task.maxAttempts=3 \
                --conf spark.task.maxFailures=2 \
                $dependencies \
                $py_file"

            echo "üåê Executing DISTRIBUTED heavy API job"
            eval $cmd
            ;;
    esac
}

# =====================================================
# SPARK CONFIGURATION DISPLAY
# =====================================================

function show_spark_config() {
    echo "üîç Current Spark Configuration:"
    echo "   SPARK_HOME: ${SPARK_HOME:-'Not set'}"
    echo "   SPARK_MASTER_URL: ${SPARK_MASTER_URL:-'Not set (will use local[*])'}"
    echo "   SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY:-'4g (default)'}"
    echo "   SPARK_EXECUTOR_MEMORY: ${SPARK_EXECUTOR_MEMORY:-'4g (default)'}"
    echo "   SPARK_WORKER_INSTANCES: ${SPARK_WORKER_INSTANCES:-'Auto-detected'}"
    echo "   DEFAULT_SPARK_JARS: ${DEFAULT_SPARK_JARS:-'Not set'}"
    echo ""
    echo "üí° Available functions:"
    echo "   graceful_spark_restart     - Your sophisticated restart function"
    echo "   start_local_spark_cluster  - Start local cluster"
    echo "   test_distributed_spark     - Test distributed functionality"
    echo "   test_all_spark             - Run all tests"
    echo "   smart_spark_submit         - Auto-detect environment"
    echo "   heavy_api_submit           - Optimized for API workloads"
    echo "   show_spark_config          - Show this configuration"
    echo "   diagnose_spark_startup     - Diagnose startup issues"
}

# =====================================================
# PATH EXPORTS & FINAL SETUP
# =====================================================

# USEFUL paths
export GEOCODE="/Users/dheerajchand/Documents/Professional/Siege_Analytics/Clients/TAN/Projects/tan_geocoding_test"
export RESUME_GENERATOR="/Users/dheerajchand/Documents/Professional/resume_generator"

# SDKMAN Setup
export SDKMAN_DIR=$(brew --prefix sdkman-cli)/libexec
[[ -s "${SDKMAN_DIR}/bin/sdkman-init.sh" ]] && source "${SDKMAN_DIR}/bin/sdkman-init.sh"

### MANAGED BY RANCHER DESKTOP START (DO NOT EDIT)
export PATH="/Users/dheerajchand/.rd/bin:$PATH"
### MANAGED BY RANCHER DESKTOP END (DO NOT EDIT)

# Display fortune
fortune

# Simple startup message (no automatic setup functions)
echo "‚úÖ Shell loaded with all your advanced functions restored!"
echo "üí° Functions available: graceful_spark_restart, test_distributed_spark, smart_spark_submit, heavy_api_submit, and more"
echo "üí° Use 'show_spark_config' to see all available Spark functions"
