# COMPLETE WORKING ZSHRC - Enhanced Spark Functions Integrated
# This is a complete, tested zshrc file with advanced Spark features restored

# =====================================================
# CORE SHELL SETUP
# =====================================================

# Path to your oh-my-zsh configuration.
export ZSH=$HOME/.dotfiles/oh-my-zsh

export ZSH_THEME="powerlevel9k/powerlevel9k"
POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(dir nvm vcs)
POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status history time)

export CASE_SENSITIVE="true"
export DISABLE_AUTO_TITLE="true"

plugins=(colorize compleat dirpersist autojump git gulp history cp)
source $ZSH/oh-my-zsh.sh

autoload -U add-zsh-hook

# =====================================================
# NODE/NVM SETUP
# =====================================================

export NVM_DIR="$HOME/.nvm"
[ -s "/opt/homebrew/opt/nvm/nvm.sh" ] && . "/opt/homebrew/opt/nvm/nvm.sh"
[ -s "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm" ] && . "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm"

load-nvmrc() {
  if [[ -f .nvmrc && -r .nvmrc ]]; then
    nvm use &> /dev/null
  else
    nvm use stable
  fi
}
add-zsh-hook chpwd load-nvmrc
load-nvmrc

unsetopt correct

# =====================================================
# BASIC ENVIRONMENT
# =====================================================

# MacOS things
defaults write -g ApplePressAndHoldEnabled -bool true

export WORKING_ON_LAPTOP="True"

# Default editor
export EDITOR="zed"
export VISUAL="zed"

# Use Neovim as fallback in the terminal when Zed is unavailable
alias vim="nvim"
alias edit="nvim"

# =====================================================
# PYTHON SETUP
# =====================================================

eval "$(pyenv init -)"
eval "$(pyenv virtualenv-init -)"
eval "$(pyenv init --path)"

function cleanvenv {
    pip freeze | grep -v "^-e" | xargs pip uninstall -y
}

function remove_python_cruft {
    find . -name "*.pyc" -delete
    find . -name "__pycache__" -exec rm -r {} +
}

export PREFERRED_VENV="geo31111"
pyenv activate $PREFERRED_VENV

# =====================================================
# UTILITY FUNCTIONS
# =====================================================

export ZSHRC_BACKUPS=~/.zshrc_backups
mkdir -p "$ZSHRC_BACKUPS"

function backup_zshrc {
    local prev_dir="$(pwd)"
    timestamp=$(date +"%Y-%m-%d_%H-%M-%S")
    backup_file="$ZSHRC_BACKUPS/.zshrc_$timestamp.txt"
    log_file="$ZSHRC_BACKUPS/zshrc_backup_log.txt"

    if [[ ! -d "$ZSHRC_BACKUPS/.git" ]]; then
        echo "‚ö†Ô∏è Backup directory is not a Git repository. Initializing..."
        git -C "$ZSHRC_BACKUPS" init
        git -C "$ZSHRC_BACKUPS" remote add origin "<YOUR_GIT_REPO_URL>"
    fi

    cp ~/.dotfiles/homedir/.zshrc "$backup_file"
    echo "$timestamp - Backup saved: $backup_file" >> "$log_file"

    git -C "$ZSHRC_BACKUPS" add .
    git -C "$ZSHRC_BACKUPS" commit -m "Backup .zshrc at $timestamp"
    git -C "$ZSHRC_BACKUPS" push origin main

    echo "‚úÖ Backup created at $backup_file"
    echo "üìú Logged in $log_file"
    echo "üöÄ Changes committed & pushed to Git repository!"

    cd "$prev_dir"
    echo "üîÑ Returned to: $prev_dir"
}

function zshreboot {
    source ~/.zshrc
}

function zshconfig {
    zed ~/.config/zsh/zshrc
}

# =====================================================
# DATABASE SETTINGS
# =====================================================

export PGHOST="localhost"
export PGUSER="dheerajchand"
export PGPASSWORD="dessert"
export PGPORT="5432"
export PGDATABASE="gis"

export GEODJANGO_TEMPLATE_SQL_DATABASE="geodjango_template_db"
export GEODJANGO_TEMPLATE_SQL_USER="dheerajchand"
export GEODJANGO_TEMPLATE_SQL_PASSWORD="dessert"
export GEODJANGO_TEMPLATE_SQL_PORT="5432"

# =====================================================
# DOCKER & GIS
# =====================================================

export PATH="/Users/dheerajchand/.rd/bin:$PATH"
export DEFAULT_DOCKER_CONTEXT="rancher-desktop"

# GIS things
export GDAL_LIBRARY_PATH="$(gdal-config --prefix)/lib/libgdal.dylib"
export GEOS_LIBRARY_PATH="$(geos-config --prefix)/lib/libgeos_c.dylib"

function update_local_repo {
    for remote in `git branch -r`; do git branch --track ${remote#origin/} $remote; done
}
export GIT_DISCOVERY_ACROSS_FILESYSTEM=1

# =====================================================
# INTERNET CONNECTIVITY CHECK
# =====================================================

function is_online {
    ping -c 1 google.com &> /dev/null && echo "online" || echo "offline"
}

# =====================================================
# ZEPPELIN SETUP & NOTEBOOK FUNCTIONS
# =====================================================

export ZEPPELIN_HOME="$HOME/zeppelin"
export PATH="$ZEPPELIN_HOME/bin:$PATH"
mkdir -p "$ZEPPELIN_HOME"

check_zeppelin() {
    if [[ ! -d "$ZEPPELIN_HOME" ]]; then
        echo "‚ùå Zeppelin directory not found: $ZEPPELIN_HOME"
        return 1
    elif ! "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" status > /dev/null 2>&1; then
        echo "Zeppelin is not running."
        return 1
    else
        echo "‚úÖ Zeppelin is running."
        return 0
    fi
}

function start_zeppelin {
    echo "Starting Zeppelin on port 9090..."

    # Ensure Zeppelin is stopped before starting fresh
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" stop

    # Explicitly set Zeppelin's port to 9090 in config
    sed -i '' 's/<value>8080<\/value>/<value>9090<\/value>/' "$ZEPPELIN_HOME/conf/zeppelin-site.xml"

    # Start Zeppelin with the new port configuration
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" start

    sleep 3  # Wait for Zeppelin to initialize

    echo "‚úÖ Zeppelin started at: http://localhost:9090"
}

function stop_zeppelin() {
    echo "Stopping Zeppelin..."
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" stop
    echo "Zeppelin stopped."
}

function restart_zeppelin() {
    stop_zeppelin
    start_zeppelin
}

function reset_zeppelin() {
    stop_zeppelin
    sleep 3

    echo "üîç Ensuring Zeppelin is fully stopped..."
    if ps aux | grep -i 'zeppelin' | grep -v 'grep' > /dev/null; then
        echo "‚ùå Zeppelin is still running! Killing processes..."
        pkill -9 -f 'zeppelin'
        sleep 2
    fi

    # Cleanup
    [ -d "$ZEPPELIN_HOME/run" ] && rm -rf "$ZEPPELIN_HOME/run"
    [ -d "$ZEPPELIN_HOME/logs" ] && rm -rf "$ZEPPELIN_HOME/logs"
    [ -d "$ZEPPELIN_HOME/local-repo/spark" ] && rm -rf "$ZEPPELIN_HOME/local-repo/spark"

    start_zeppelin
    sleep 3  # Allow restart

    # ‚úÖ Verify Zeppelin is running
    if ps aux | grep -i 'zeppelin' | grep -v 'grep' > /dev/null; then
        echo "‚úÖ Zeppelin restarted successfully!"
    else
        echo "‚ùå Zeppelin failed to start. Check logs!"
        tail -n 50 "$ZEPPELIN_HOME/logs/zeppelin.log"
    fi
}

function check_pyspark_dependencies() {
    if ! command -v pyspark &> /dev/null; then
        echo "‚ùå PySpark not found! Install it with: pip install pyspark"
        return 1
    fi

    if ! command -v jupyter &> /dev/null && ! command -v "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" &> /dev/null; then
        echo "‚ùå Neither Jupyter nor Zeppelin found! Install Jupyter: pip install notebook"
        return 1
    fi

    echo "‚úÖ Dependencies verified!"
    return 0
}

function check_notebook_dependencies {
    local notebook_type="$1"

    case "$notebook_type" in
        jupyter|jupyterlab)
            if ! command -v pyspark &> /dev/null; then
                echo "‚ùå PySpark not found! Install it with: pip install pyspark"
                return 1
            fi
            if ! command -v jupyter &> /dev/null; then
                echo "‚ùå Jupyter not found! Install it with: pip install notebook"
                return 1
            fi
            ;;
        zeppelin)
            if [[ ! -d "$ZEPPELIN_HOME" ]] || ! command -v "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" &> /dev/null; then
                echo "‚ùå Zeppelin is missing! Ensure it's installed and configured."
                return 1
            fi
            ;;
        databricks)
            if ! command -v databricks &> /dev/null; then
                echo "‚ùå Databricks CLI not found! Install it with: pip install databricks-cli"
                return 1
            fi
            ;;
        vscode)
            if ! command -v code &> /dev/null; then
                echo "‚ùå VS Code not found! Ensure it's installed and available in PATH."
                return 1
            fi
            ;;
        *)
            echo "‚ùå Unknown notebook type: $notebook_type"
            return 1
            ;;
    esac

    echo "‚úÖ Dependencies verified for $notebook_type!"
    return 0
}

function notebook_manager {
    local notebook_type="$1"
    local port="${2:-8888}"  # Default to port 8888
    local notebook_dir="${3:-$(pwd)}"  # Default to current directory

    # ‚úÖ Validate dependencies dynamically based on requested notebook type
    check_notebook_dependencies "$notebook_type" || return 1

    case "$notebook_type" in
        jupyter)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="notebook --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting PySpark Jupyter Notebook on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Notebook launch failed!"; return 1; }
            ;;
        jupyterlab)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="lab --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting Jupyter Lab on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Lab launch failed!"; return 1; }
            ;;
        zeppelin)
            echo "üöÄ Starting Zeppelin Notebook..."
            start_zeppelin || { echo "‚ùå Zeppelin launch failed!"; return 1; }
            ;;
        databricks)
            echo "üöÄ Launching Databricks CLI..."
            databricks workspace import_dir "$notebook_dir" || { echo "‚ùå Databricks CLI failed!"; return 1; }
            ;;
        vscode)
            echo "üöÄ Opening VS Code for notebook editing..."
            code "$notebook_dir" || { echo "‚ùå VS Code launch failed!"; return 1; }
            ;;
        *)
            echo "‚ùå Invalid notebook type: $notebook_type. Available options: jupyter, jupyterlab, zeppelin, databricks, vscode."
            return 1
            ;;
    esac
}

function pyspark_notebook {
    local notebook_type="${1:-jupyter}"  # Default to Jupyter if no type is provided
    notebook_manager "$notebook_type"
}

# =====================================================
# JAVA SETUP (Manual - No Auto-Detection)
# =====================================================

export JAVA_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/java/current"
export PATH="$JAVA_HOME/bin:$PATH"
export LD_LIBRARY_PATH="$JAVA_HOME/lib:$LD_LIBRARY_PATH"

# =====================================================
# AUTO-SETUP SYSTEM
# =====================================================

# Manual setup function (can be called when needed)
function setup_java_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Detecting best Java version for Hadoop & Spark..."
        local best_java_version=$(sdk list java | awk '/11\.0/ {print $NF}' | sort -r | head -n 1)

        if [[ -n "$best_java_version" ]]; then
            sdk install java $best_java_version
            sdk default java $best_java_version
            export JAVA_HOME=$(sdk home java $best_java_version)
            export PATH=$JAVA_HOME/bin:$PATH
            export LD_LIBRARY_PATH=$JAVA_HOME/lib:$LD_LIBRARY_PATH
            echo "‚úÖ Java version set to $best_java_version"
        fi
    else
        echo "‚ö†Ô∏è  Offline - using current Java installation"
    fi
}

# =====================================================
# AUTO-SETUP SYSTEM
# =====================================================

# Control flags
export AUTO_SETUP_ON_STARTUP="${AUTO_SETUP_ON_STARTUP:-false}"
export AUTO_SETUP_CHECK_ONLINE="${AUTO_SETUP_CHECK_ONLINE:-true}"
export AUTO_SETUP_VERBOSE="${AUTO_SETUP_VERBOSE:-false}"

function setup_scala_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Detecting best Scala version for Spark 3.5..."
        local best_scala_version="2.12.18"  # Spark 3.5 uses Scala 2.12

        if ! command -v scala &>/dev/null || [[ $(scala -version 2>&1 | grep -o '2\.[0-9]*\.[0-9]*') != "$best_scala_version" ]]; then
            echo "üì¶ Installing Scala $best_scala_version..."
            sdk install scala $best_scala_version
            sdk default scala $best_scala_version
            echo "‚úÖ Scala version set to $best_scala_version"
        else
            echo "‚úÖ Scala already optimal: $(scala -version 2>&1 | head -1)"
        fi
    else
        echo "‚ö†Ô∏è  Offline - using current Scala installation"
    fi
}

function setup_spark_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Detecting latest Spark 3.5.x version..."
        local latest_spark=$(sdk list spark | grep '3\.5\.' | grep -v '[*>+-]' | sort -V | tail -n 1 | awk '{print $1}')

        if [[ -n "$latest_spark" ]] && [[ ! -d "$HOME/.sdkman/candidates/spark/$latest_spark" ]]; then
            echo "üì¶ Installing Spark $latest_spark..."
            sdk install spark $latest_spark
            sdk default spark $latest_spark
            export SPARK_HOME=$(sdk home spark $latest_spark)
            echo "‚úÖ Spark version set to $latest_spark"
        else
            echo "‚úÖ Spark already up to date"
        fi
    else
        echo "‚ö†Ô∏è  Offline - using current Spark installation"
    fi
}

function auto_setup_environment {
    local start_time=$(date +%s)
    echo "üöÄ Auto-setting up development environment..."

    # Quick connectivity check first
    if [[ "$AUTO_SETUP_CHECK_ONLINE" == "true" ]]; then
        local online_status=$(is_online)
        if [[ "$online_status" == "offline" ]]; then
            echo "‚ö†Ô∏è  Offline mode - skipping version updates"
            return 0
        fi
    fi

    # Run setup functions
    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Java..."
    setup_java_version 2>/dev/null || echo "‚ö†Ô∏è  Java setup skipped"

    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Scala..."
    setup_scala_version 2>/dev/null || echo "‚ö†Ô∏è  Scala setup skipped"

    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Hadoop..."
    setup_hadoop_version 2>/dev/null || echo "‚ö†Ô∏è  Hadoop setup skipped"

    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Spark..."
    setup_spark_version 2>/dev/null || echo "‚ö†Ô∏è  Spark setup skipped"

    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Maven..."
    setup_maven 2>/dev/null || echo "‚ö†Ô∏è  Maven setup skipped"

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    echo "‚úÖ Environment auto-setup completed in ${duration}s"
}

function enable_auto_setup {
    export AUTO_SETUP_ON_STARTUP="true"
    echo "‚úÖ Auto-setup enabled for future shell sessions"
    echo "üí° Run 'disable_auto_setup' to turn off"
    echo "üí° Run 'auto_setup_environment' to run it now"
}

function disable_auto_setup {
    export AUTO_SETUP_ON_STARTUP="false"
    echo "‚úÖ Auto-setup disabled"
    echo "üí° Run 'enable_auto_setup' to turn back on"
    echo "üí° You can still run 'auto_setup_environment' manually"
}

function setup_environment_status {
    echo "üîç Environment Setup Status:"
    echo "   Auto-setup on startup: $AUTO_SETUP_ON_STARTUP"
    echo "   Check online: $AUTO_SETUP_CHECK_ONLINE"
    echo "   Verbose mode: $AUTO_SETUP_VERBOSE"
    echo ""
    echo "Current versions:"
    echo "   Java: $(java -version 2>&1 | head -1 || echo 'Not found')"
    echo "   Scala: $(scala -version 2>&1 | head -1 || echo 'Not found')"
    echo "   Spark: $(spark-submit --version 2>&1 | head -1 || echo 'Not found')"
    echo "   Hadoop: $(hadoop version 2>/dev/null | head -1 || echo 'Not found')"
    echo "   Maven: $(mvn -version 2>/dev/null | head -1 || echo 'Not found')"
    echo ""
    echo "üí° Controls:"
    echo "   enable_auto_setup    - Enable auto-setup on shell startup"
    echo "   disable_auto_setup   - Disable auto-setup"
    echo "   auto_setup_environment - Run setup manually"
}

# Smart conditional startup - only run if enabled
if [[ "$AUTO_SETUP_ON_STARTUP" == "true" ]]; then
    echo "üîÑ Auto-setup is enabled, running environment setup..."
    auto_setup_environment &  # Run in background to not block shell startup
fi

# =====================================================
# HADOOP SETUP (Manual - No Auto-Detection)
# =====================================================

export HADOOP_CURRENT_VERSION="3.3.6"
export HADOOP_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/hadoop/current"
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH"
export HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop/"
export PATH="$HADOOP_HOME/bin:$PATH"

# Manual setup function (can be called when needed)
function setup_hadoop_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Detecting latest compatible Hadoop version..."
        local latest_version=$(sdk list hadoop | grep '3.3' | grep -v '[*>+-]' | grep -v '3.3.5' | sort -r | head -n 1 | awk '{print $1}')

        if [[ -n "$latest_version" ]]; then
            sdk install hadoop $latest_version
            export HADOOP_CURRENT_VERSION="$latest_version"
            export HADOOP_HOME=$(sdk home hadoop $HADOOP_CURRENT_VERSION)
            export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
            export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
            export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop/
            export PATH="$HADOOP_HOME/bin:$PATH"
            echo "‚úÖ Hadoop version set to $HADOOP_CURRENT_VERSION"
        fi
    else
        echo "‚ö†Ô∏è  Offline - using current Hadoop installation"
    fi
}

# =====================================================
# MAVEN SETUP
# =====================================================

DEFAULT_MAVEN_VERSION="3.9.6"

function setup_maven() {
    if command -v mvn &>/dev/null; then
        echo "Maven is installed: $(mvn -version | head -n 1)"
    else
        echo "Maven not found. Installing Maven $DEFAULT_MAVEN_VERSION via SDKMAN..."
        sdk install maven "$DEFAULT_MAVEN_VERSION" || {
            echo "Failed to install Maven. Please check SDKMAN setup."
            return 1
        }

        if command -v mvn &>/dev/null; then
            echo "Maven installation successful: $(mvn -version | head -n 1)"
        else
            echo "Maven installation failed."
        fi
    fi
}

function download_maven_jars {
    local libraries="${1:-$DEFAULT_SPARK_JARS}"
    local target_path="${2:-$LOCAL_SPARK_JAR_PATH}"
    mkdir -p "$target_path"

    echo "üöÄ Downloading Maven dependencies..."
    for lib in $(echo "$libraries" | tr ',' ' '); do
        IFS=':' read -r group artifact version <<< "$lib"
        jar_file="${artifact}-${version}.jar"
        mvn_url="https://repo1.maven.org/maven2/$(echo $group | tr '.' '/')/$artifact/$version/$jar_file"

        echo "üîç Fetching: $jar_file"
        curl -sL -o "$target_path/$jar_file" "$mvn_url" && echo "‚úÖ Saved to $target_path/$jar_file" || echo "‚ùå Failed to download $jar_file"
    done
}

# ===============================================================================
# ENHANCED SPARK FUNCTIONS - ADVANCED FEATURES RESTORED
# ===============================================================================

# =====================================================
# SPARK ENVIRONMENT (Manual Setup)
# =====================================================

export SPARK_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/spark/current"
export SPARK_LOCAL_IP="127.0.0.1"
export SPARK_MASTER_HOST="127.0.0.1"
export SPARK_MASTER_PORT="7077"
export SPARK_WORKER_INSTANCES="4"  # Reduced for stability
export SPARK_DRIVER_MEMORY="2g"    # Reduced to fit workers
export SPARK_EXECUTOR_MEMORY="1g"  # Fits in 2GB workers
export SPARK_WORKER_MEMORY="2g"
export SPARK_CONF_DIR="$SPARK_HOME/conf"
export SPARK_CLIENT_CONFIG="$HOME/.spark-client-defaults.properties"

# Python paths
export PYSPARK_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"
export PYSPARK_DRIVER_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"

# Add Spark to PATH
export PATH="$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"

# Additional environment variables for distributed mode
export SPARK_NUM_EXECUTORS="${SPARK_NUM_EXECUTORS:-4}"
export SPARK_EXECUTOR_CORES="${SPARK_EXECUTOR_CORES:-1}"
export SPARK_DRIVER_MAX_RESULT_SIZE="${SPARK_DRIVER_MAX_RESULT_SIZE:-2g}"

# Kubernetes configuration (if using K8s)
export SPARK_K8S_MASTER="${SPARK_K8S_MASTER:-k8s://https://kubernetes.default.svc:443}"
export SPARK_K8S_IMAGE="${SPARK_K8S_IMAGE:-your-spark-image:latest}"
export SPARK_K8S_NAMESPACE="${SPARK_K8S_NAMESPACE:-default}"
export SPARK_K8S_SERVICE_ACCOUNT="${SPARK_K8S_SERVICE_ACCOUNT:-spark}"

# =====================================================
# ENHANCED DEPENDENCY MANAGEMENT
# =====================================================

export DEFAULT_SPARK_JARS="org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.1,org.datasyslab:geotools-wrapper:1.7.1-28.5,graphframes:graphframes:0.8.3-spark3.5-s_2.12"
export LOCAL_SPARK_JAR_PATH="$HOME/local_jars"
mkdir -p "$LOCAL_SPARK_JAR_PATH"

# =====================================================
# SEDONA UPGRADE FUNCTIONS
# =====================================================

function upgrade_sedona {
    echo "üöÄ Upgrading Apache Sedona to latest version..."

    echo "üì¶ Current Python package version:"
    pip show apache-sedona | grep Version || echo "   Not installed"

    echo ""
    echo "üîÑ Updating Python package..."
    pip install --upgrade apache-sedona

    echo ""
    echo "‚úÖ Upgrade complete! Current version:"
    pip show apache-sedona | grep Version

    echo ""
    echo "üí° JAR versions in your config are already latest:"
    echo "   - sedona-spark-shaded-3.5_2.12:1.7.1"
    echo "   - geotools-wrapper:1.7.1-28.5"
    echo ""
    echo "üß™ Test the upgrade:"
    echo "   test_spark_comprehensive"
}

function check_sedona_versions {
    echo "üîç Checking Sedona version compatibility..."

    echo "Python package:"
    pip show apache-sedona | grep Version || echo "   Not installed"

    echo ""
    echo "JAR versions in config:"
    echo "   DEFAULT_SPARK_JARS: $DEFAULT_SPARK_JARS"

    echo ""
    echo "Spark version:"
    spark-submit --version 2>&1 | head -n 1

    echo ""
    echo "üí° Sedona 1.7.1 is compatible with Spark 3.3, 3.4, and 3.5"
    echo "üí° Your setup should be fully compatible!"
}

# Enhanced dependency resolution with debugging
function get_spark_dependencies {
    local online_status=$(is_online)

    echo "üîç Dependency resolution:" >&2
    echo "   Online status: $online_status" >&2

    if [[ "$online_status" == "online" ]]; then
        echo "   Using online packages: $DEFAULT_SPARK_JARS" >&2
        echo "--packages $DEFAULT_SPARK_JARS"
    else
        echo "   Checking local JARs in: $LOCAL_SPARK_JAR_PATH" >&2
        local local_jars=$(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | tr '\n' ',' | sed 's/,$//')

        if [[ -n "$local_jars" ]]; then
            echo "   Using local JARs: $(echo $local_jars | tr ',' ' ' | wc -w) found" >&2
            echo "--jars $local_jars"
        else
            echo "   ‚ö†Ô∏è  No local JARs found, downloading recommended..." >&2
            download_spark_jars_if_needed

            # Try again after download
            local_jars=$(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | tr '\n' ',' | sed 's/,$//')
            if [[ -n "$local_jars" ]]; then
                echo "   Using downloaded JARs" >&2
                echo "--jars $local_jars"
            else
                echo "   ‚ö†Ô∏è  Proceeding without additional JARs" >&2
                echo ""
            fi
        fi
    fi
}

# Download essential JARs when offline
function download_spark_jars_if_needed {
    echo "üì¶ Downloading essential Spark JARs for offline use..."

    # Core JAR URLs (Maven Central)
    local jar_urls=(
        "https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.5_2.12/1.7.1/sedona-spark-shaded-3.5_2.12-1.7.1.jar"
        "https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.7.1-28.5/geotools-wrapper-1.7.1-28.5.jar"
        "https://repo1.maven.org/maven2/graphframes/graphframes/0.8.3-spark3.5-s_2.12/graphframes-0.8.3-spark3.5-s_2.12.jar"
    )

    for url in "${jar_urls[@]}"; do
        local jar_name=$(basename "$url")
        local jar_path="$LOCAL_SPARK_JAR_PATH/$jar_name"

        if [[ ! -f "$jar_path" ]]; then
            echo "  Downloading: $jar_name"
            if curl -sL "$url" -o "$jar_path"; then
                echo "  ‚úÖ Downloaded: $jar_name"
            else
                echo "  ‚ùå Failed: $jar_name"
                rm -f "$jar_path"  # Remove failed download
            fi
        else
            echo "  ‚úÖ Already exists: $jar_name"
        fi
    done
}

# Test dependency resolution
function test_spark_dependencies {
    echo "üß™ Testing Spark dependency resolution..."
    echo ""

    echo "Current status:"
    local deps=$(get_spark_dependencies 2>&1)
    echo "Dependencies resolved: $(echo "$deps" | tail -1)"
    echo ""

    echo "Local JAR inventory:"
    if [[ -d "$LOCAL_SPARK_JAR_PATH" ]]; then
        find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" -exec basename {} \; | sort
        echo "Total JARs: $(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" | wc -l)"
    else
        echo "No local JAR directory found"
    fi
}

# =====================================================
# ENHANCED SPARK CLUSTER MANAGEMENT
# =====================================================

function spark_start {
    echo "üöÄ Starting Spark cluster..."

    # Stop any existing processes
    pkill -f 'org.apache.spark.deploy.master.Master' 2>/dev/null
    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null
    sleep 3

    # Start master
    echo "Starting master..."
    $SPARK_HOME/sbin/start-master.sh
    sleep 5

    # Start workers (4 workers for stability)
    echo "Starting workers..."
    for i in {1..4}; do
        $SPARK_HOME/sbin/start-worker.sh spark://127.0.0.1:7077
        sleep 1
    done

    # Set master URL
    export SPARK_MASTER_URL="spark://127.0.0.1:7077"

    echo "‚úÖ Cluster started!"
    echo "üìä Master UI: http://127.0.0.1:8080"
    echo "üéØ Master URL: $SPARK_MASTER_URL"

    # Simple functionality test
    sleep 10
    spark_test_simple
}

function spark_stop {
    echo "üõë Stopping Spark cluster..."

    # Use Spark's stop scripts
    if [[ -f "$SPARK_HOME/sbin/stop-all.sh" ]]; then
        $SPARK_HOME/sbin/stop-all.sh
    fi

    # Force kill any remaining
    pkill -f 'org.apache.spark.deploy.master.Master' 2>/dev/null
    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null

    unset SPARK_MASTER_URL
    echo "‚úÖ Cluster stopped"
}

function spark_restart {
    echo "üîÑ Restarting Spark cluster..."
    spark_stop
    sleep 3
    spark_start
}

function spark_status {
    echo "üìä Spark Cluster Status:"
    echo "   Master processes: $(ps aux | grep 'spark.deploy.master.Master' | grep -v grep | wc -l)"
    echo "   Worker processes: $(ps aux | grep 'spark.deploy.worker.Worker' | grep -v grep | wc -l)"
    echo "   Master URL: ${SPARK_MASTER_URL:-'Not set'}"
    echo "   Master UI: http://127.0.0.1:8080"

    # Quick functional test
    if [[ -n "$SPARK_MASTER_URL" ]]; then
        echo "   Testing functionality..."
        spark_test_simple
    fi
}

# =====================================================
# ENHANCED SPARK TESTING FUNCTIONS
# =====================================================

function spark_test_simple {
    echo "üß™ Quick Spark functionality test..."

    python3 -c "
import sys
try:
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.master('spark://127.0.0.1:7077').appName('QuickTest').getOrCreate()
    result = spark.sparkContext.parallelize([1,2,3,4,5]).sum()
    print(f'‚úÖ Cluster functional: sum = {result}')
    spark.stop()
except Exception as e:
    print(f'‚ùå Test failed: {e}')
    sys.exit(1)
" 2>/dev/null
}

function test_spark_comprehensive {
    echo "üß™ Comprehensive Spark functionality test (Sedona + GraphFrames)..."
    echo "üî• RUNNING UPDATED VERSION FROM ARTIFACT - BANANA HAMMOCK! üî•"

    local test_script="/tmp/spark_comprehensive_test.py"
    cat > "$test_script" << 'EOF'
from pyspark.sql import SparkSession

print("üöÄ Starting comprehensive Spark test...")

spark = SparkSession.builder \
    .appName("ComprehensiveSparkTest") \
    .config("spark.sql.extensions", "org.apache.sedona.sql.SedonaSqlExtensions") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator") \
    .getOrCreate()

print("‚úÖ Spark Context created successfully")
print(f"   Master: {spark.sparkContext.master}")
print(f"   Default Parallelism: {spark.sparkContext.defaultParallelism}")

print("\nüîç Test 1: Basic RDD operations...")
rdd = spark.sparkContext.parallelize(range(100), 4)
result = rdd.map(lambda x: x * x).sum()
print(f"   ‚úÖ RDD computation result: {result}")

print("\nüîç Test 2: DataFrame operations...")
df = spark.range(100)
count = df.count()
print(f"   ‚úÖ DataFrame count: {count}")

print("\nüîç Test 3: SQL operations...")
df.createOrReplaceTempView("test_table")
sql_result = spark.sql("SELECT COUNT(*) as count FROM test_table").collect()[0]["count"]
print(f"   ‚úÖ SQL result: {sql_result}")

print("\nüîç Test 4: Sedona functionality...")
sedona_works = False
try:
    # Modern Sedona 1.7.1+ initialization
    from sedona.spark import SedonaContext
    sedona = SedonaContext.create(spark)
    print("   ‚úÖ Sedona context created (modern method)")

    # Test basic spatial function
    point_result = sedona.sql("SELECT ST_Point(1.0, 2.0) as point").collect()
    print("   ‚úÖ Sedona ST_Point works")

    # Test distance calculation
    distance_result = sedona.sql("""
        SELECT ST_Distance(
            ST_Point(-0.1275, 51.5072),
            ST_Point(-74.0060, 40.7128)
        ) as distance_degrees
    """).collect()[0]["distance_degrees"]
    print(f"   ‚úÖ Sedona distance calculation: {distance_result:.4f} degrees")

    # Test spatial operations with DataFrame
    spatial_df = sedona.createDataFrame([
        ("London", -0.1275, 51.5072),
        ("NYC", -74.0060, 40.7128),
    ], ["city", "longitude", "latitude"])

    spatial_with_geom = spatial_df.selectExpr(
        "city",
        "ST_Point(longitude, latitude) as geom"
    )
    geom_count = spatial_with_geom.count()
    print(f"   ‚úÖ Sedona DataFrame operations: {geom_count} geometries created")

    sedona_works = True

except Exception as e:
    print(f"   ‚ö†Ô∏è  Sedona failed: {e}")

print("\nüîç Test 5: GraphFrames functionality...")
graphframes_works = False
try:
    from graphframes import GraphFrame
    vertices = spark.createDataFrame([("A", "Node A"), ("B", "Node B"), ("C", "Node C")], ["id", "name"])
    edges = spark.createDataFrame([("A", "B", "edge1"), ("B", "C", "edge2")], ["src", "dst", "relationship"])
    g = GraphFrame(vertices, edges)
    v_count = g.vertices.count()
    e_count = g.edges.count()
    print(f"   ‚úÖ GraphFrame created with {v_count} vertices, {e_count} edges")

    # Test PageRank
    pagerank_result = g.pageRank(resetProbability=0.01, maxIter=2)
    pr_vertices = pagerank_result.vertices.count()
    print(f"   ‚úÖ PageRank completed: {pr_vertices} vertices processed")

    graphframes_works = True
except Exception as e:
    print(f"   ‚ö†Ô∏è  GraphFrames failed: {e}")

print("\nüéâ Summary:")
print("   ‚úÖ Core Spark: Working")
print(f"   {'‚úÖ' if sedona_works else '‚ö†Ô∏è '} Sedona: {'Working (Modern)' if sedona_works else 'Failed'}")
print(f"   {'‚úÖ' if graphframes_works else '‚ö†Ô∏è '} GraphFrames: {'Working' if graphframes_works else 'Failed'}")

spark.stop()
print("üõë Test completed")
EOF

    echo "Running test..."
    if [[ -n "$SPARK_MASTER_URL" ]]; then
        distributed_spark_submit "$test_script"
    else
        default_spark_submit "$test_script"
    fi

    rm -f "$test_script"
}

function spark_test_distributed {
    echo "üß™ Testing distributed Spark functionality..."

    if [[ -z "$SPARK_MASTER_URL" ]]; then
        echo "‚ö†Ô∏è  SPARK_MASTER_URL not set. Starting cluster..."
        spark_start
    fi

    # Create test script
    cat > /tmp/spark_test.py << 'EOF'
from pyspark.sql import SparkSession
import time

try:
    spark = SparkSession.builder \
        .appName("DistributedTest") \
        .getOrCreate()

    print(f"‚úÖ Connected to: {spark.sparkContext.master}")
    print(f"‚úÖ Default Parallelism: {spark.sparkContext.defaultParallelism}")

    # Test computation
    rdd = spark.sparkContext.parallelize(range(1000), 8)
    result = rdd.map(lambda x: x * x).sum()
    print(f"‚úÖ Computation result: {result}")

    # DataFrame test
    df = spark.range(1000)
    count = df.count()
    print(f"‚úÖ DataFrame count: {count}")

    print("‚úÖ Distributed test completed successfully!")

except Exception as e:
    print(f"‚ùå Test failed: {e}")
    raise
finally:
    if 'spark' in locals():
        spark.stop()
EOF

    # Run with minimal logging - use eval for proper argument expansion
    eval "spark-submit \
        --master '$SPARK_MASTER_URL' \
        --deploy-mode client \
        --driver-memory 2g \
        --executor-memory 1g \
        --executor-cores 1 \
        --num-executors 4 \
        --conf spark.sql.adaptive.enabled=true \
        /tmp/spark_test.py"

    rm -f /tmp/spark_test.py
}

# =====================================================
# ENHANCED SPARK SUBMIT FUNCTIONS
# =====================================================

# Enhanced default submit with dependency resolution
function default_spark_submit() {
    local py_file="$1"
    if [[ -z "$py_file" ]]; then
        echo "Usage: default_spark_submit <python_file>"
        return 1
    fi

    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi

    echo "üöÄ Local Spark submit with enhanced dependencies..."
    local dependencies=$(get_spark_dependencies)

    # Use eval to properly expand dependencies
    eval "spark-submit \
        --master 'local[*]' \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        $dependencies \
        '$py_file'"
}

# Enhanced distributed submit
function distributed_spark_submit() {
    local py_file="$1"
    local master_url="${2:-$SPARK_MASTER_URL}"

    if [[ -z "$py_file" ]]; then
        echo "Usage: distributed_spark_submit <python_file> [master_url]"
        return 1
    fi

    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi

    if [[ -z "$master_url" ]]; then
        echo "‚ùå No master URL. Run: spark_start"
        return 1
    fi

    echo "üåê Distributed Spark submit with enhanced dependencies..."
    local dependencies=$(get_spark_dependencies)

    # Use eval to properly expand dependencies
    eval "spark-submit \
        --master '$master_url' \
        --deploy-mode client \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --executor-cores 1 \
        --num-executors 4 \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        --conf spark.network.timeout=300s \
        $dependencies \
        '$py_file'"
}

# Enhanced shells with dependencies
function default_spark_shell() {
    echo "üöÄ Starting Spark Shell (Scala) with enhanced dependencies..."
    local dependencies=$(get_spark_dependencies)

    # Use eval to properly expand dependencies
    eval "spark-shell \
        --master 'local[*]' \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        $dependencies"
}

function default_pyspark_shell() {
    echo "üöÄ Starting PySpark Shell (Python) with enhanced dependencies..."
    local dependencies=$(get_spark_dependencies)

    # Use eval to properly expand dependencies
    eval "pyspark \
        --master 'local[*]' \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        $dependencies"
}

# Smart environment detection submit
function smart_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: smart_spark_submit <python_file>"
        return 1
    fi

    echo "ü§ñ Smart environment detection..."

    # Check for Kubernetes
    if command -v kubectl >/dev/null 2>&1 && [ -n "$SPARK_K8S_MASTER" ]; then
        echo "‚úÖ Kubernetes detected - using K8s mode"
        k8s_spark_submit "$py_file"
        return
    fi

    # Check for standalone Spark cluster
    if [ -n "$SPARK_MASTER_URL" ] && ps aux | grep -i "spark.deploy.master.Master" | grep -v "grep" > /dev/null; then
        echo "‚úÖ Local Spark cluster detected - using distributed mode"
        distributed_spark_submit "$py_file"
        return
    fi

    # Check if we can start a local cluster
    if [[ -n "$SPARK_HOME" ]] && [[ -f "$SPARK_HOME/sbin/start-master.sh" ]]; then
        echo "‚ÑπÔ∏è  No running cluster found - would you like to start one? (y/n)"
        read "start_cluster?"
        if [[ "$start_cluster" == "y" ]]; then
            spark_start
            distributed_spark_submit "$py_file"
            return
        fi
    fi

    # Fall back to local mode
    echo "‚ÑπÔ∏è  Using local mode"
    default_spark_submit "$py_file"
}

# Heavy API workload optimization
function heavy_api_submit() {
    local py_file="$1"
    local mode="${2:-distributed}"

    if [ -z "$py_file" ]; then
        echo "Usage: heavy_api_submit <python_file> [mode]"
        echo "This function is optimized for API-heavy workloads (geocoding, web scraping, etc.)"
        echo "Modes: local, distributed (default)"
        return 1
    fi

    echo "üåê Heavy API workload optimization..."

    case "$mode" in
        "local")
            echo "üè† Running API-heavy job in LOCAL mode"

            # Local mode with API optimizations
            local dependencies=$(get_spark_dependencies)

            eval "spark-submit \
                --master 'local[*]' \
                --driver-memory 4g \
                --conf spark.sql.adaptive.enabled=true \
                --conf spark.sql.adaptive.coalescePartitions.enabled=true \
                --conf spark.sql.shuffle.partitions=200 \
                --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
                --conf spark.sql.execution.arrow.pyspark.enabled=true \
                --conf spark.network.timeout=600s \
                --conf spark.task.maxAttempts=3 \
                --conf spark.task.maxFailures=2 \
                $dependencies \
                '$py_file'"
            ;;
        "distributed"|*)
            if [[ -z "$SPARK_MASTER_URL" ]]; then
                echo "‚ùå No SPARK_MASTER_URL set for distributed API processing"
                echo "   Run: spark_start"
                return 1
            fi

            echo "üöÄ Running distributed API-heavy job with optimizations"
            local dependencies=$(get_spark_dependencies)

            eval "spark-submit \
                --master '$SPARK_MASTER_URL' \
                --deploy-mode client \
                --driver-memory 4g \
                --executor-memory 2g \
                --executor-cores 2 \
                --num-executors 6 \
                --conf spark.sql.adaptive.enabled=true \
                --conf spark.sql.adaptive.coalescePartitions.enabled=true \
                --conf spark.sql.shuffle.partitions=400 \
                --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
                --conf spark.sql.execution.arrow.pyspark.enabled=true \
                --conf spark.network.timeout=800s \
                --conf spark.executor.heartbeatInterval=60s \
                --conf spark.task.maxAttempts=5 \
                --conf spark.task.maxFailures=3 \
                --conf spark.dynamicAllocation.enabled=false \
                $dependencies \
                '$py_file'"
            ;;
    esac
}

# Kubernetes Spark submit
function k8s_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: k8s_spark_submit <python_file>"
        return 1
    fi

    if ! command -v kubectl >/dev/null 2>&1; then
        echo "‚ùå kubectl not found! Kubernetes mode requires kubectl."
        return 1
    fi

    echo "‚ò∏Ô∏è  Kubernetes Spark submit..."

    # Note: K8s mode typically needs online packages, not local JARs
    local k8s_dependencies="--packages $DEFAULT_SPARK_JARS"

    eval "spark-submit \
        --master '${SPARK_K8S_MASTER}' \
        --deploy-mode cluster \
        --name 'spark-k8s-$(basename $py_file .py)' \
        --conf spark.kubernetes.container.image='${SPARK_K8S_IMAGE}' \
        --conf spark.kubernetes.namespace='${SPARK_K8S_NAMESPACE}' \
        --conf spark.executor.instances=4 \
        --conf spark.executor.memory=2g \
        --conf spark.executor.cores=1 \
        --conf spark.driver.memory=2g \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        $k8s_dependencies \
        '$py_file'"
}

# Enhanced flexible submit
function flexible_spark_submit() {
    local py_file="$1"
    local mode="${2:-local}"

    if [ -z "$py_file" ]; then
        echo "Usage: flexible_spark_submit <python_file> [mode]"
        echo "Modes:"
        echo "  local       - Run locally with all cores (default)"
        echo "  distributed - Run on local Spark cluster"
        echo "  k8s         - Run on Kubernetes"
        echo "  smart       - Auto-detect best environment"
        echo "  heavy-api   - Optimized for API-heavy workloads"
        return 1
    fi

    case "$mode" in
        "local")
            echo "üè† Flexible submit: LOCAL mode"
            default_spark_submit "$py_file"
            ;;
        "distributed")
            echo "üåê Flexible submit: DISTRIBUTED mode"
            distributed_spark_submit "$py_file"
            ;;
        "k8s")
            echo "‚ò∏Ô∏è  Flexible submit: KUBERNETES mode"
            k8s_spark_submit "$py_file"
            ;;
        "smart")
            echo "ü§ñ Flexible submit: SMART detection mode"
            smart_spark_submit "$py_file"
            ;;
        "heavy-api")
            echo "üåê Flexible submit: HEAVY API mode"
            heavy_api_submit "$py_file"
            ;;
        *)
            echo "‚ùå Unknown mode: $mode"
            echo "Available modes: local, distributed, k8s, smart, heavy-api"
            return 1
            ;;
    esac
}

# Backwards compatibility for your existing names
function spark_submit_local {
    default_spark_submit "$@"
}

function spark_submit_cluster {
    distributed_spark_submit "$@"
}

# =====================================================
# ENHANCED CONFIGURATION DISPLAY
# =====================================================

function show_spark_config() {
    echo "‚öôÔ∏è  Enhanced Spark Configuration:"
    echo ""
    echo "üè† Environment:"
    echo "   SPARK_HOME: ${SPARK_HOME:-'Not set'}"
    echo "   SPARK_MASTER_URL: ${SPARK_MASTER_URL:-'Not set'}"
    echo "   Java: ${JAVA_HOME:-'Not set'}"
    echo ""
    echo "ü§ñ Auto-Setup System:"
    echo "   Enabled on startup: $AUTO_SETUP_ON_STARTUP"
    echo "   Check online: $AUTO_SETUP_CHECK_ONLINE"
    echo "   Verbose mode: $AUTO_SETUP_VERBOSE"
    echo ""
    echo "üì¶ Dependencies:"
    echo "   Online status: $(is_online)"
    echo "   Default JARs: $DEFAULT_SPARK_JARS"
    echo "   Local JAR path: $LOCAL_SPARK_JAR_PATH"
    echo "   Local JARs available: $(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | wc -l)"
    echo ""
    echo "üîç Sedona Status:"
    echo "   Python package: $(pip show apache-sedona 2>/dev/null | grep Version | cut -d: -f2 | xargs || echo 'Not installed')"
    echo "   JAR version: 1.7.1 (Latest)"
    echo "   Initialization: Modern SedonaContext.create() method"
    echo ""
    echo "‚ò∏Ô∏è  Kubernetes (if available):"
    echo "   kubectl: $(command -v kubectl >/dev/null && echo 'Available' || echo 'Not available')"
    echo "   K8s Master: ${SPARK_K8S_MASTER}"
    echo "   K8s Image: ${SPARK_K8S_IMAGE}"
    echo "   K8s Namespace: ${SPARK_K8S_NAMESPACE}"
    echo ""
    echo "üí° Available enhanced functions:"
    echo "   # Core Spark functions"
    echo "   default_spark_submit     - Local with dependency resolution"
    echo "   distributed_spark_submit - Cluster with dependency resolution"
    echo "   smart_spark_submit       - Auto-detect execution environment"
    echo "   heavy_api_submit         - Optimized for API workloads"
    echo "   flexible_spark_submit    - Multi-mode submission"
    echo ""
    echo "   # Testing & diagnostics"
    echo "   test_spark_comprehensive - Full Sedona + GraphFrames test"
    echo "   test_spark_dependencies  - Test dependency resolution"
    echo "   upgrade_sedona           - Upgrade Sedona Python package"
    echo "   check_sedona_versions    - Check version compatibility"
    echo ""
    echo "   # Auto-setup system"
    echo "   enable_auto_setup        - Enable auto-setup on startup"
    echo "   disable_auto_setup       - Disable auto-setup"
    echo "   auto_setup_environment   - Run setup manually"
    echo "   setup_environment_status - Check setup status"
    echo ""
}

function spark_config {
    show_spark_config
}

function spark_fix_logging {
    echo "üîß Reducing Spark logging noise..."

    # Create log4j2.properties to reduce INFO spam
    cat > $SPARK_HOME/conf/log4j2.properties << 'EOF'
# Reduce Spark logging noise
rootLogger.level = WARN
rootLogger.appenderRefs = stdout
rootLogger.appenderRef.stdout.ref = console

appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Set Spark components to WARN
logger.spark.name = org.apache.spark
logger.spark.level = WARN

logger.hadoop.name = org.apache.hadoop
logger.hadoop.level = WARN

logger.akka.name = akka
logger.akka.level = WARN

logger.jetty.name = org.eclipse.jetty
logger.jetty.level = WARN
EOF

    echo "‚úÖ Logging reduced to WARN level"
}

# =====================================================
# BACKWARD COMPATIBILITY ALIASES
# =====================================================

# Keep your existing function names working
function graceful_spark_restart {
    spark_restart
}

function test_distributed_spark {
    spark_test_distributed
}

function start_local_spark_cluster {
    spark_start
}

function stop_local_spark_cluster {
    spark_stop
}

function check_spark_cluster_health {
    spark_status
}

# Alias for backward compatibility
function test_spark {
    test_spark_comprehensive
}

# =====================================================
# PATH EXPORTS & FINAL SETUP
# =====================================================

# USEFUL paths
export GEOCODE="/Users/dheerajchand/Documents/Professional/Siege_Analytics/Clients/TAN/Projects/tan_geocoding_test"
export RESUME_GENERATOR="/Users/dheerajchand/Documents/Professional/resume_generator"

# SDKMAN Setup
export SDKMAN_DIR=$(brew --prefix sdkman-cli)/libexec
[[ -s "${SDKMAN_DIR}/bin/sdkman-init.sh" ]] && source "${SDKMAN_DIR}/bin/sdkman-init.sh"

### MANAGED BY RANCHER DESKTOP START (DO NOT EDIT)
export PATH="/Users/dheerajchand/.rd/bin:$PATH"
### MANAGED BY RANCHER DESKTOP END (DO NOT EDIT)

# Auto-fix logging on startup
spark_fix_logging 2>/dev/null

# Display fortune
fortune

echo "üöÄüçå RIDICULOUS BANANA HAMMOCK ZSHRC LOADED SUCCESSFULLY! üçåüöÄ"
echo "üî• Enhanced zshrc loaded with advanced Spark features! üî•"
echo "üí° Key features: Smart dependencies, Modern Sedona 1.7.1, Auto-setup system, GraphFrames, K8s support"
echo "üí° Quick start: test_spark_comprehensive | enable_auto_setup | setup_environment_status"
