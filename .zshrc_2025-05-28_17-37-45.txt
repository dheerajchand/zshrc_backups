# COMPLETE WORKING ZSHRC - Enhanced Spark Functions Integrated
# This is a complete, tested zshrc file with advanced Spark features restored

# =====================================================
# CORE SHELL SETUP
# =====================================================

# Path to your oh-my-zsh configuration.
export ZSH=$HOME/.dotfiles/oh-my-zsh

export ZSH_THEME="powerlevel9k/powerlevel9k"
POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(dir nvm vcs)
POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status history time)

export CASE_SENSITIVE="true"
export DISABLE_AUTO_TITLE="true"

plugins=(colorize compleat dirpersist autojump git gulp history cp)
source $ZSH/oh-my-zsh.sh

autoload -U add-zsh-hook

# =====================================================
# NODE/NVM SETUP
# =====================================================

export NVM_DIR="$HOME/.nvm"
[ -s "/opt/homebrew/opt/nvm/nvm.sh" ] && . "/opt/homebrew/opt/nvm/nvm.sh"
[ -s "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm" ] && . "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm"

load-nvmrc() {
  if [[ -f .nvmrc && -r .nvmrc ]]; then
    nvm use &> /dev/null
  else
    nvm use stable
  fi
}
add-zsh-hook chpwd load-nvmrc
load-nvmrc

unsetopt correct

# =====================================================
# BASIC ENVIRONMENT
# =====================================================

# MacOS things
defaults write -g ApplePressAndHoldEnabled -bool true

export WORKING_ON_LAPTOP="True"

# Default editor
export EDITOR="zed"
export VISUAL="zed"

# Use Neovim as fallback in the terminal when Zed is unavailable
alias vim="nvim"
alias edit="nvim"

# =====================================================
# PYTHON SETUP
# =====================================================

eval "$(pyenv init -)"
eval "$(pyenv virtualenv-init -)"
eval "$(pyenv init --path)"

function cleanvenv {
    pip freeze | grep -v "^-e" | xargs pip uninstall -y
}

function remove_python_cruft {
    find . -name "*.pyc" -delete
    find . -name "__pycache__" -exec rm -r {} +
}

export PREFERRED_VENV="geo31111"
pyenv activate $PREFERRED_VENV

# =====================================================
# UTILITY FUNCTIONS
# =====================================================

export ZSHRC_BACKUPS=~/.zshrc_backups
mkdir -p "$ZSHRC_BACKUPS"

function backup_zshrc {
    local prev_dir="$(pwd)"
    timestamp=$(date +"%Y-%m-%d_%H-%M-%S")
    backup_file="$ZSHRC_BACKUPS/.zshrc_$timestamp.txt"
    log_file="$ZSHRC_BACKUPS/zshrc_backup_log.txt"

    if [[ ! -d "$ZSHRC_BACKUPS/.git" ]]; then
        echo "‚ö†Ô∏è Backup directory is not a Git repository. Initializing..."
        git -C "$ZSHRC_BACKUPS" init
        git -C "$ZSHRC_BACKUPS" remote add origin "<YOUR_GIT_REPO_URL>"
    fi

    cp ~/.dotfiles/homedir/.zshrc "$backup_file"
    echo "$timestamp - Backup saved: $backup_file" >> "$log_file"

    git -C "$ZSHRC_BACKUPS" add .
    git -C "$ZSHRC_BACKUPS" commit -m "Backup .zshrc at $timestamp"
    git -C "$ZSHRC_BACKUPS" push origin main

    echo "‚úÖ Backup created at $backup_file"
    echo "üìú Logged in $log_file"
    echo "üöÄ Changes committed & pushed to Git repository!"

    cd "$prev_dir"
    echo "üîÑ Returned to: $prev_dir"
}

function zshreboot {
    source ~/.zshrc
}

function zshconfig {
    zed ~/.config/zsh/zshrc
}

# =====================================================
# DATABASE SETTINGS
# =====================================================

export PGHOST="localhost"
export PGUSER="dheerajchand"
export PGPASSWORD="dessert"
export PGPORT="5432"
export PGDATABASE="gis"

export GEODJANGO_TEMPLATE_SQL_DATABASE="geodjango_template_db"
export GEODJANGO_TEMPLATE_SQL_USER="dheerajchand"
export GEODJANGO_TEMPLATE_SQL_PASSWORD="dessert"
export GEODJANGO_TEMPLATE_SQL_PORT="5432"

# =====================================================
# DOCKER & GIS
# =====================================================

export PATH="/Users/dheerajchand/.rd/bin:$PATH"
export DEFAULT_DOCKER_CONTEXT="rancher-desktop"

# GIS things
export GDAL_LIBRARY_PATH="$(gdal-config --prefix)/lib/libgdal.dylib"
export GEOS_LIBRARY_PATH="$(geos-config --prefix)/lib/libgeos_c.dylib"

function update_local_repo {
    for remote in `git branch -r`; do git branch --track ${remote#origin/} $remote; done
}
export GIT_DISCOVERY_ACROSS_FILESYSTEM=1

# =====================================================
# INTERNET CONNECTIVITY CHECK
# =====================================================

function is_online {
    ping -c 1 google.com &> /dev/null && echo "online" || echo "offline"
}

# =====================================================
# ZEPPELIN SETUP & NOTEBOOK FUNCTIONS
# =====================================================

export ZEPPELIN_HOME="$HOME/zeppelin"
export PATH="$ZEPPELIN_HOME/bin:$PATH"
mkdir -p "$ZEPPELIN_HOME"

check_zeppelin() {
    if [[ ! -d "$ZEPPELIN_HOME" ]]; then
        echo "‚ùå Zeppelin directory not found: $ZEPPELIN_HOME"
        return 1
    elif ! "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" status > /dev/null 2>&1; then
        echo "Zeppelin is not running."
        return 1
    else
        echo "‚úÖ Zeppelin is running."
        return 0
    fi
}

function start_zeppelin {
    echo "Starting Zeppelin on port 9090..."

    # Ensure Zeppelin is stopped before starting fresh
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" stop

    # Explicitly set Zeppelin's port to 9090 in config
    sed -i '' 's/<value>8080<\/value>/<value>9090<\/value>/' "$ZEPPELIN_HOME/conf/zeppelin-site.xml"

    # Start Zeppelin with the new port configuration
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" start

    sleep 3  # Wait for Zeppelin to initialize

    echo "‚úÖ Zeppelin started at: http://localhost:9090"
}

function stop_zeppelin() {
    echo "Stopping Zeppelin..."
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" stop
    echo "Zeppelin stopped."
}

function restart_zeppelin() {
    stop_zeppelin
    start_zeppelin
}

function reset_zeppelin() {
    stop_zeppelin
    sleep 3

    echo "üîç Ensuring Zeppelin is fully stopped..."
    if ps aux | grep -i 'zeppelin' | grep -v 'grep' > /dev/null; then
        echo "‚ùå Zeppelin is still running! Killing processes..."
        pkill -9 -f 'zeppelin'
        sleep 2
    fi

    # Cleanup
    [ -d "$ZEPPELIN_HOME/run" ] && rm -rf "$ZEPPELIN_HOME/run"
    [ -d "$ZEPPELIN_HOME/logs" ] && rm -rf "$ZEPPELIN_HOME/logs"
    [ -d "$ZEPPELIN_HOME/local-repo/spark" ] && rm -rf "$ZEPPELIN_HOME/local-repo/spark"

    start_zeppelin
    sleep 3  # Allow restart

    # ‚úÖ Verify Zeppelin is running
    if ps aux | grep -i 'zeppelin' | grep -v 'grep' > /dev/null; then
        echo "‚úÖ Zeppelin restarted successfully!"
    else
        echo "‚ùå Zeppelin failed to start. Check logs!"
        tail -n 50 "$ZEPPELIN_HOME/logs/zeppelin.log"
    fi
}

function check_pyspark_dependencies() {
    if ! command -v pyspark &> /dev/null; then
        echo "‚ùå PySpark not found! Install it with: pip install pyspark"
        return 1
    fi

    if ! command -v jupyter &> /dev/null && ! command -v "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" &> /dev/null; then
        echo "‚ùå Neither Jupyter nor Zeppelin found! Install Jupyter: pip install notebook"
        return 1
    fi

    echo "‚úÖ Dependencies verified!"
    return 0
}

function check_notebook_dependencies {
    local notebook_type="$1"

    case "$notebook_type" in
        jupyter|jupyterlab)
            if ! command -v pyspark &> /dev/null; then
                echo "‚ùå PySpark not found! Install it with: pip install pyspark"
                return 1
            fi
            if ! command -v jupyter &> /dev/null; then
                echo "‚ùå Jupyter not found! Install it with: pip install notebook"
                return 1
            fi
            ;;
        zeppelin)
            if [[ ! -d "$ZEPPELIN_HOME" ]] || ! command -v "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" &> /dev/null; then
                echo "‚ùå Zeppelin is missing! Ensure it's installed and configured."
                return 1
            fi
            ;;
        databricks)
            if ! command -v databricks &> /dev/null; then
                echo "‚ùå Databricks CLI not found! Install it with: pip install databricks-cli"
                return 1
            fi
            ;;
        vscode)
            if ! command -v code &> /dev/null; then
                echo "‚ùå VS Code not found! Ensure it's installed and available in PATH."
                return 1
            fi
            ;;
        *)
            echo "‚ùå Unknown notebook type: $notebook_type"
            return 1
            ;;
    esac

    echo "‚úÖ Dependencies verified for $notebook_type!"
    return 0
}

function notebook_manager {
    local notebook_type="$1"
    local port="${2:-8888}"  # Default to port 8888
    local notebook_dir="${3:-$(pwd)}"  # Default to current directory

    # ‚úÖ Validate dependencies dynamically based on requested notebook type
    check_notebook_dependencies "$notebook_type" || return 1

    case "$notebook_type" in
        jupyter)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="notebook --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting PySpark Jupyter Notebook on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Notebook launch failed!"; return 1; }
            ;;
        jupyterlab)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="lab --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting Jupyter Lab on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Lab launch failed!"; return 1; }
            ;;
        zeppelin)
            echo "üöÄ Starting Zeppelin Notebook..."
            start_zeppelin || { echo "‚ùå Zeppelin launch failed!"; return 1; }
            ;;
        databricks)
            echo "üöÄ Launching Databricks CLI..."
            databricks workspace import_dir "$notebook_dir" || { echo "‚ùå Databricks CLI failed!"; return 1; }
            ;;
        vscode)
            echo "üöÄ Opening VS Code for notebook editing..."
            code "$notebook_dir" || { echo "‚ùå VS Code launch failed!"; return 1; }
            ;;
        *)
            echo "‚ùå Invalid notebook type: $notebook_type. Available options: jupyter, jupyterlab, zeppelin, databricks, vscode."
            return 1
            ;;
    esac
}

function pyspark_notebook {
    local notebook_type="${1:-jupyter}"  # Default to Jupyter if no type is provided
    notebook_manager "$notebook_type"
}

# =====================================================
# JAVA SETUP
# =====================================================

export JAVA_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/java/current"
export PATH="$JAVA_HOME/bin:$PATH"
export LD_LIBRARY_PATH="$JAVA_HOME/lib:$LD_LIBRARY_PATH"

# Manual setup function (can be called when needed)
function setup_java_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Setting up Java 17 (optimal for Spark 3.5.3 + Hadoop 3.3.6)..."
        local target_java_version="17.0.12-tem"  # Known good version for Spark/Hadoop

        if ! sdk list java | grep -q "$target_java_version"; then
            echo "üì¶ Installing Java $target_java_version..."
            sdk install java $target_java_version
        fi

        sdk default java $target_java_version
        export JAVA_HOME=$(sdk home java $target_java_version)
        export PATH=$JAVA_HOME/bin:$PATH
        export LD_LIBRARY_PATH=$JAVA_HOME/lib:$LD_LIBRARY_PATH
        echo "‚úÖ Java version set to $target_java_version"
        echo "   Supports: Spark 3.5.3 + Hadoop 3.3.6 integration"
    else
        echo "‚ö†Ô∏è  Offline - using current Java installation"
    fi
}

# =====================================================
# HADOOP SETUP
# =====================================================

export HADOOP_CURRENT_VERSION="3.3.6"
export HADOOP_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/hadoop/current"
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH"
export HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop/"
export PATH="$HADOOP_HOME/bin:$PATH"

# Manual setup function (can be called when needed)
function setup_hadoop_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Setting up Hadoop 3.3.6 (pinned known-good version)..."
        local target_hadoop_version="3.3.6"

        if ! sdk list hadoop | grep -q "$target_hadoop_version"; then
            echo "üì¶ Installing Hadoop $target_hadoop_version..."
            sdk install hadoop $target_hadoop_version
        fi

        sdk default hadoop $target_hadoop_version
        export HADOOP_CURRENT_VERSION="$target_hadoop_version"
        export HADOOP_HOME=$(sdk home hadoop $target_hadoop_version)
        export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
        export LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH"
        export HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop/"
        export PATH="$HADOOP_HOME/bin:$PATH"
        echo "‚úÖ Hadoop version pinned to $target_hadoop_version"
        echo "   Compatible with: Java 17, Spark 3.5.3"
    else
        echo "‚ö†Ô∏è  Offline - using current Hadoop installation"
    fi
}

# =====================================================
# MAVEN SETUP
# =====================================================

DEFAULT_MAVEN_VERSION="3.9.6"

function setup_maven() {
    if command -v mvn &>/dev/null; then
        echo "Maven is installed: $(mvn -version | head -n 1)"
    else
        echo "Maven not found. Installing Maven $DEFAULT_MAVEN_VERSION via SDKMAN..."
        sdk install maven "$DEFAULT_MAVEN_VERSION" || {
            echo "Failed to install Maven. Please check SDKMAN setup."
            return 1
        }

        if command -v mvn &>/dev/null; then
            echo "Maven installation successful: $(mvn -version | head -n 1)"
        else
            echo "Maven installation failed."
        fi
    fi
}

function download_maven_jars {
    local libraries="${1:-$DEFAULT_SPARK_JARS}"
    local target_path="${2:-$LOCAL_SPARK_JAR_PATH}"
    mkdir -p "$target_path"

    echo "üöÄ Downloading Maven dependencies..."
    for lib in $(echo "$libraries" | tr ',' ' '); do
        IFS=':' read -r group artifact version <<< "$lib"
        jar_file="${artifact}-${version}.jar"
        mvn_url="https://repo1.maven.org/maven2/$(echo $group | tr '.' '/')/$artifact/$version/$jar_file"

        echo "üîç Fetching: $jar_file"
        curl -sL -o "$target_path/$jar_file" "$mvn_url" && echo "‚úÖ Saved to $target_path/$jar_file" || echo "‚ùå Failed to download $jar_file"
    done
}

# =====================================================
# AUTO-SETUP SYSTEM
# =====================================================

# Control flags
export AUTO_SETUP_ON_STARTUP="${AUTO_SETUP_ON_STARTUP:-false}"
export AUTO_SETUP_CHECK_ONLINE="${AUTO_SETUP_CHECK_ONLINE:-true}"
export AUTO_SETUP_VERBOSE="${AUTO_SETUP_VERBOSE:-false}"

function setup_scala_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Setting up dual Scala versions (2.12.x for Spark + 3.x for modern development)..."

        # Scala 2.12.18 - Required for Spark 3.5.3
        local scala_2_version="2.12.18"
        if ! sdk list scala | grep -q "$scala_2_version"; then
            echo "üì¶ Installing Scala $scala_2_version (Spark compatibility)..."
            sdk install scala $scala_2_version
        fi

        # Scala 3.3.4 - Latest stable Scala 3
        local scala_3_version="3.3.4"
        if ! sdk list scala | grep -q "$scala_3_version"; then
            echo "üì¶ Installing Scala $scala_3_version (modern Scala)..."
            sdk install scala $scala_3_version
        fi

        # Default to Scala 2.12 for Spark compatibility
        sdk default scala $scala_2_version
        echo "‚úÖ Scala versions installed:"
        echo "   Default: $scala_2_version (Spark 3.5.3 compatible)"
        echo "   Available: $scala_3_version (use: sdk use scala $scala_3_version)"
        echo "üí° Switch versions: sdk use scala [version]"
    else
        echo "‚ö†Ô∏è  Offline - using current Scala installation"
    fi
}

function setup_spark_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Setting up Spark 3.5.3 (pinned known-good version)..."
        local target_spark_version="3.5.3"

        if ! sdk list spark | grep -q "$target_spark_version"; then
            echo "üì¶ Installing Spark $target_spark_version..."
            sdk install spark $target_spark_version
        fi

        sdk default spark $target_spark_version
        export SPARK_HOME=$(sdk home spark $target_spark_version)
        export PATH="$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"
        echo "‚úÖ Spark version pinned to $target_spark_version"
        echo "   Compatible with: Java 17, Scala 2.12.18, Hadoop 3.3.6"
    else
        echo "‚ö†Ô∏è  Offline - using current Spark installation"
    fi
}

function auto_setup_environment {
    local start_time=$(date +%s)
    echo "üöÄ Auto-setting up development environment..."

    # Quick connectivity check first
    if [[ "$AUTO_SETUP_CHECK_ONLINE" == "true" ]]; then
        local online_status=$(is_online)
        if [[ "$online_status" == "offline" ]]; then
            echo "‚ö†Ô∏è  Offline mode - skipping version updates"
            return 0
        fi
    fi

    # Run setup functions
    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Java..."
    setup_java_version 2>/dev/null || echo "‚ö†Ô∏è  Java setup skipped"

    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Scala..."
    setup_scala_version 2>/dev/null || echo "‚ö†Ô∏è  Scala setup skipped"

    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Hadoop..."
    setup_hadoop_version 2>/dev/null || echo "‚ö†Ô∏è  Hadoop setup skipped"

    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Spark..."
    setup_spark_version 2>/dev/null || echo "‚ö†Ô∏è  Spark setup skipped"

    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Maven..."
    setup_maven 2>/dev/null || echo "‚ö†Ô∏è  Maven setup skipped"

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    echo "‚úÖ Environment auto-setup completed in ${duration}s"
}

function enable_auto_setup {
    export AUTO_SETUP_ON_STARTUP="true"
    echo "‚úÖ Auto-setup enabled for future shell sessions"
    echo "üí° Run 'disable_auto_setup' to turn off"
    echo "üí° Run 'auto_setup_environment' to run it now"
}

function disable_auto_setup {
    export AUTO_SETUP_ON_STARTUP="false"
    echo "‚úÖ Auto-setup disabled"
    echo "üí° Run 'enable_auto_setup' to turn back on"
    echo "üí° You can still run 'auto_setup_environment' manually"
}

function show_version_strategy {
    echo "üìå Pinned Known-Good Version Strategy:"
    echo ""
    echo "üîß Target Versions (tested compatibility):"
    echo "   Java:    17.0.12-tem  (LTS with Spark/Hadoop support)"
    echo "   Scala:   2.12.18      (Spark 3.5.3 compatible) + 3.3.4 (modern)"
    echo "   Spark:   3.5.3        (your current working version)"
    echo "   Hadoop:  3.3.6        (stable with Spark 3.5.3)"
    echo "   Maven:   3.9.6        (latest stable)"
    echo ""
    echo "‚úÖ Compatibility Matrix:"
    echo "   Java 17 + Spark 3.5.3 + Hadoop 3.3.6 = ‚úÖ Tested & Working"
    echo "   Scala 2.12.18 required for Spark compatibility"
    echo "   Scala 3.3.4 available for modern Scala development"
    echo ""
    echo "üí° Version Management:"
    echo "   ‚Ä¢ Pinned versions prevent unexpected breaks"
    echo "   ‚Ä¢ All versions tested together as a stack"
    echo "   ‚Ä¢ Scala dual-version setup for flexibility"
    echo "   ‚Ä¢ Switch Scala: sdk use scala [2.12.18|3.3.4]"
    echo ""
    echo "üîÑ To modify pinned versions, edit the setup functions in your zshrc"
}

function verify_version_compatibility {
    echo "üîç Verifying installed version compatibility..."
    echo ""

    local java_version=$(java -version 2>&1 | head -1 | grep -o '"[^"]*"' | tr -d '"' || echo "Not found")
    local scala_version=$(scala -version 2>&1 | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+' | head -1 || echo "Not found")
    local spark_version=$(spark-submit --version 2>&1 | grep -o 'version [0-9]\+\.[0-9]\+\.[0-9]\+' | cut -d' ' -f2 || echo "Not found")
    local hadoop_version=$(hadoop version 2>/dev/null | head -1 | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+' || echo "Not found")

    echo "üìä Installed Versions:"
    echo "   Java:    $java_version"
    echo "   Scala:   $scala_version"
    echo "   Spark:   $spark_version"
    echo "   Hadoop:  $hadoop_version"
    echo ""

    # Check compatibility
    local java_ok="‚ùå"
    local scala_ok="‚ùå"
    local spark_ok="‚ùå"
    local hadoop_ok="‚ùå"

    [[ "$java_version" =~ ^17\. ]] && java_ok="‚úÖ"
    [[ "$scala_version" =~ ^2\.12\. ]] && scala_ok="‚úÖ"
    [[ "$spark_version" == "3.5.3" ]] && spark_ok="‚úÖ"
    [[ "$hadoop_version" =~ ^3\.3\. ]] && hadoop_ok="‚úÖ"

    echo "üéØ Compatibility Check:"
    echo "   Java 17.x:     $java_ok"
    echo "   Scala 2.12.x:  $scala_ok"
    echo "   Spark 3.5.3:   $spark_ok"
    echo "   Hadoop 3.3.x:  $hadoop_ok"
    echo ""

    if [[ "$java_ok$scala_ok$spark_ok$hadoop_ok" == "‚úÖ‚úÖ‚úÖ‚úÖ" ]]; then
        echo "üéâ All versions compatible! Your stack should work perfectly."
    else
        echo "‚ö†Ô∏è  Some versions may need adjustment. Run 'auto_setup_environment' to fix."
    fi
}

function setup_environment_status {
    echo "üîç Environment Setup Status:"
    echo "   Auto-setup on startup: $AUTO_SETUP_ON_STARTUP"
    echo "   Check online: $AUTO_SETUP_CHECK_ONLINE"
    echo "   Verbose mode: $AUTO_SETUP_VERBOSE"
    echo ""
    echo "Current versions:"
    echo "   Java: $(java -version 2>&1 | head -1 || echo 'Not found')"
    echo "   Scala: $(scala -version 2>&1 | head -1 || echo 'Not found')"
    echo "   Spark: $(spark-submit --version 2>&1 | head -1 || echo 'Not found')"
    echo "   Hadoop: $(hadoop version 2>/dev/null | head -1 || echo 'Not found')"
    echo "   Maven: $(mvn -version 2>/dev/null | head -1 || echo 'Not found')"
    echo ""
    echo "üí° Controls:"
    echo "   enable_auto_setup         - Enable auto-setup on shell startup"
    echo "   disable_auto_setup        - Disable auto-setup"
    echo "   auto_setup_environment    - Run setup manually"
    echo "   show_version_strategy     - Show pinned version strategy"
    echo "   verify_version_compatibility - Check version compatibility"
}

# =====================================================
# FUNCTION BACKUP SYSTEM
# =====================================================

function backup_critical_functions {
    echo "üíæ Creating backup of critical functions..."
    local backup_dir="$HOME/.zsh_function_backups"
    mkdir -p "$backup_dir"

    # Backup the working test function
    cat > "$backup_dir/test_spark_comprehensive_backup.sh" << 'EOF'
function test_spark_comprehensive {
    echo "üß™ Comprehensive Spark functionality test (Sedona + GraphFrames)..."
    echo "üî• RUNNING UPDATED VERSION FROM ARTIFACT - BANANA HAMMOCK! üî•"

    local test_script="/tmp/spark_comprehensive_test.py"
    cat > "$test_script" << 'PYTHON_EOF'
from pyspark.sql import SparkSession

print("üöÄ Starting comprehensive Spark test...")

spark = SparkSession.builder \
    .appName("ComprehensiveSparkTest") \
    .config("spark.sql.extensions", "org.apache.sedona.sql.SedonaSqlExtensions") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator") \
    .getOrCreate()

print("‚úÖ Spark Context created successfully")
print(f"   Master: {spark.sparkContext.master}")
print(f"   Default Parallelism: {spark.sparkContext.defaultParallelism}")

print("\nüîç Test 1: Basic RDD operations...")
rdd = spark.sparkContext.parallelize(range(100), 4)
result = rdd.map(lambda x: x * x).sum()
print(f"   ‚úÖ RDD computation result: {result}")

print("\nüîç Test 2: DataFrame operations...")
df = spark.range(100)
count = df.count()
print(f"   ‚úÖ DataFrame count: {count}")

print("\nüîç Test 3: SQL operations...")
df.createOrReplaceTempView("test_table")
sql_result = spark.sql("SELECT COUNT(*) as count FROM test_table").collect()[0]["count"]
print(f"   ‚úÖ SQL result: {sql_result}")

print("\nüîç Test 4: Sedona functionality...")
sedona_works = False
try:
    # Modern Sedona 1.7.1+ initialization
    from sedona.spark import SedonaContext
    sedona = SedonaContext.create(spark)
    print("   ‚úÖ Sedona context created (modern method)")

    # Test basic spatial function
    point_result = sedona.sql("SELECT ST_Point(1.0, 2.0) as point").collect()
    print("   ‚úÖ Sedona ST_Point works")

    # Test distance calculation
    distance_result = sedona.sql("""
        SELECT ST_Distance(
            ST_Point(-0.1275, 51.5072),
            ST_Point(-74.0060, 40.7128)
        ) as distance_degrees
    """).collect()[0]["distance_degrees"]
    print(f"   ‚úÖ Sedona distance calculation: {distance_result:.4f} degrees")

    # Test spatial operations with DataFrame
    spatial_df = sedona.createDataFrame([
        ("London", -0.1275, 51.5072),
        ("NYC", -74.0060, 40.7128),
    ], ["city", "longitude", "latitude"])

    spatial_with_geom = spatial_df.selectExpr(
        "city",
        "ST_Point(longitude, latitude) as geom"
    )
    geom_count = spatial_with_geom.count()
    print(f"   ‚úÖ Sedona DataFrame operations: {geom_count} geometries created")

    sedona_works = True

except Exception as e:
    print(f"   ‚ö†Ô∏è  Sedona failed: {e}")

print("\nüîç Test 5: GraphFrames functionality...")
graphframes_works = False
try:
    from graphframes import GraphFrame
    vertices = spark.createDataFrame([("A", "Node A"), ("B", "Node B"), ("C", "Node C")], ["id", "name"])
    edges = spark.createDataFrame([("A", "B", "edge1"), ("B", "C", "edge2")], ["src", "dst", "relationship"])
    g = GraphFrame(vertices, edges)
    v_count = g.vertices.count()
    e_count = g.edges.count()
    print(f"   ‚úÖ GraphFrame created with {v_count} vertices, {e_count} edges")

    # Test PageRank
    pagerank_result = g.pageRank(resetProbability=0.01, maxIter=2)
    pr_vertices = pagerank_result.vertices.count()
    print(f"   ‚úÖ PageRank completed: {pr_vertices} vertices processed")

    graphframes_works = True
except Exception as e:
    print(f"   ‚ö†Ô∏è  GraphFrames failed: {e}")

print("\nüéâ Summary:")
print("   ‚úÖ Core Spark: Working")
print(f"   {'‚úÖ' if sedona_works else '‚ö†Ô∏è '} Sedona: {'Working (Modern)' if sedona_works else 'Failed'}")
print(f"   {'‚úÖ' if graphframes_works else '‚ö†Ô∏è '} GraphFrames: {'Working' if graphframes_works else 'Failed'}")

spark.stop()
print("üõë Test completed")
PYTHON_EOF

    echo "Running test..."
    if [[ -n "$SPARK_MASTER_URL" ]]; then
        distributed_spark_submit "$test_script"
    else
        default_spark_submit "$test_script"
    fi

    rm -f "$test_script"
}
EOF

    echo "‚úÖ Critical functions backed up to: $backup_dir"
    echo "üí° Use 'restore_critical_functions' if they get lost"
}

function restore_critical_functions {
    echo "üîÑ Restoring critical functions from backup..."
    local backup_dir="$HOME/.zsh_function_backups"

    if [[ -f "$backup_dir/test_spark_comprehensive_backup.sh" ]]; then
        source "$backup_dir/test_spark_comprehensive_backup.sh"
        echo "‚úÖ test_spark_comprehensive function restored!"
        echo "üß™ Test it: test_spark_comprehensive"
    else
        echo "‚ùå No backup found. Run 'backup_critical_functions' first."
    fi
}

function emergency_restore_test_function {
    echo "üö® Emergency restore of test_spark_comprehensive function..."

    function test_spark_comprehensive {
        echo "üß™ Comprehensive Spark functionality test (Sedona + GraphFrames)..."
        echo "üî• RUNNING UPDATED VERSION FROM ARTIFACT - BANANA HAMMOCK! üî•"

        local test_script="/tmp/spark_comprehensive_test.py"
        cat > "$test_script" << 'EOF'
from pyspark.sql import SparkSession

print("üöÄ Starting comprehensive Spark test...")

spark = SparkSession.builder \
    .appName("ComprehensiveSparkTest") \
    .config("spark.sql.extensions", "org.apache.sedona.sql.SedonaSqlExtensions") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator") \
    .getOrCreate()

print("‚úÖ Spark Context created successfully")
print(f"   Master: {spark.sparkContext.master}")
print(f"   Default Parallelism: {spark.sparkContext.defaultParallelism}")

print("\nüîç Test 1: Basic RDD operations...")
rdd = spark.sparkContext.parallelize(range(100), 4)
result = rdd.map(lambda x: x * x).sum()
print(f"   ‚úÖ RDD computation result: {result}")

print("\nüîç Test 2: DataFrame operations...")
df = spark.range(100)
count = df.count()
print(f"   ‚úÖ DataFrame count: {count}")

print("\nüîç Test 3: SQL operations...")
df.createOrReplaceTempView("test_table")
sql_result = spark.sql("SELECT COUNT(*) as count FROM test_table").collect()[0]["count"]
print(f"   ‚úÖ SQL result: {sql_result}")

print("\nüîç Test 4: Sedona functionality...")
sedona_works = False
try:
    # Modern Sedona 1.7.1+ initialization
    from sedona.spark import SedonaContext
    sedona = SedonaContext.create(spark)
    print("   ‚úÖ Sedona context created (modern method)")

    # Test basic spatial function
    point_result = sedona.sql("SELECT ST_Point(1.0, 2.0) as point").collect()
    print("   ‚úÖ Sedona ST_Point works")

    # Test distance calculation
    distance_result = sedona.sql("""
        SELECT ST_Distance(
            ST_Point(-0.1275, 51.5072),
            ST_Point(-74.0060, 40.7128)
        ) as distance_degrees
    """).collect()[0]["distance_degrees"]
    print(f"   ‚úÖ Sedona distance calculation: {distance_result:.4f} degrees")

    # Test spatial operations with DataFrame
    spatial_df = sedona.createDataFrame([
        ("London", -0.1275, 51.5072),
        ("NYC", -74.0060, 40.7128),
    ], ["city", "longitude", "latitude"])

    spatial_with_geom = spatial_df.selectExpr(
        "city",
        "ST_Point(longitude, latitude) as geom"
    )
    geom_count = spatial_with_geom.count()
    print(f"   ‚úÖ Sedona DataFrame operations: {geom_count} geometries created")

    sedona_works = True

except Exception as e:
    print(f"   ‚ö†Ô∏è  Sedona failed: {e}")

print("\nüîç Test 5: GraphFrames functionality...")
graphframes_works = False
try:
    from graphframes import GraphFrame
    vertices = spark.createDataFrame([("A", "Node A"), ("B", "Node B"), ("C", "Node C")], ["id", "name"])
    edges = spark.createDataFrame([("A", "B", "edge1"), ("B", "C", "edge2")], ["src", "dst", "relationship"])
    g = GraphFrame(vertices, edges)
    v_count = g.vertices.count()
    e_count = g.edges.count()
    print(f"   ‚úÖ GraphFrame created with {v_count} vertices, {e_count} edges")

    # Test PageRank
    pagerank_result = g.pageRank(resetProbability=0.01, maxIter=2)
    pr_vertices = pagerank_result.vertices.count()
    print(f"   ‚úÖ PageRank completed: {pr_vertices} vertices processed")

    graphframes_works = True
except Exception as e:
    print(f"   ‚ö†Ô∏è  GraphFrames failed: {e}")

print("\nüéâ Summary:")
print("   ‚úÖ Core Spark: Working")
print(f"   {'‚úÖ' if sedona_works else '‚ö†Ô∏è '} Sedona: {'Working (Modern)' if sedona_works else 'Failed'}")
print(f"   {'‚úÖ' if graphframes_works else '‚ö†Ô∏è '} GraphFrames: {'Working' if graphframes_works else 'Failed'}")

spark.stop()
print("üõë Test completed")
EOF

        echo "Running test..."
        if [[ -n "$SPARK_MASTER_URL" ]]; then
            distributed_spark_submit "$test_script"
        else
            default_spark_submit "$test_script"
        fi

        rm -f "$test_script"
    }

    echo "üöë Emergency function restored! Test it: test_spark_comprehensive"
}

function list_function_backups {
    local backup_dir="$HOME/.zsh_function_backups"
    echo "üìã Available function backups:"
    if [[ -d "$backup_dir" ]]; then
        ls -la "$backup_dir"
    else
        echo "   No backups found. Run 'backup_critical_functions' first."
    fi
}

# =====================================================
# SPARK ENVIRONMENT
# =====================================================

export SPARK_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/spark/current"
export SPARK_LOCAL_IP="127.0.0.1"
export SPARK_MASTER_HOST="127.0.0.1"
export SPARK_MASTER_PORT="7077"
export SPARK_WORKER_INSTANCES="4"
export SPARK_DRIVER_MEMORY="2g"
export SPARK_EXECUTOR_MEMORY="1g"
export SPARK_WORKER_MEMORY="2g"
export SPARK_CONF_DIR="$SPARK_HOME/conf"
export SPARK_CLIENT_CONFIG="$HOME/.spark-client-defaults.properties"

# Python paths
export PYSPARK_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"
export PYSPARK_DRIVER_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"

# Add Spark to PATH
export PATH="$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"

# Additional environment variables for distributed mode
export SPARK_NUM_EXECUTORS="${SPARK_NUM_EXECUTORS:-4}"
export SPARK_EXECUTOR_CORES="${SPARK_EXECUTOR_CORES:-1}"
export SPARK_DRIVER_MAX_RESULT_SIZE="${SPARK_DRIVER_MAX_RESULT_SIZE:-2g}"

# Kubernetes configuration (if using K8s)
export SPARK_K8S_MASTER="${SPARK_K8S_MASTER:-k8s://https://kubernetes.default.svc:443}"
export SPARK_K8S_IMAGE="${SPARK_K8S_IMAGE:-your-spark-image:latest}"
export SPARK_K8S_NAMESPACE="${SPARK_K8S_NAMESPACE:-default}"
export SPARK_K8S_SERVICE_ACCOUNT="${SPARK_K8S_SERVICE_ACCOUNT:-spark}"

# =====================================================
# ENHANCED DEPENDENCY MANAGEMENT
# =====================================================

export DEFAULT_SPARK_JARS="org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.1,org.datasyslab:geotools-wrapper:1.7.1-28.5,graphframes:graphframes:0.8.3-spark3.5-s_2.12"
export LOCAL_SPARK_JAR_PATH="$HOME/local_jars"
mkdir -p "$LOCAL_SPARK_JAR_PATH"

# Enhanced dependency resolution with debugging
function get_spark_dependencies {
    local online_status=$(is_online)

    echo "üîç Dependency resolution:" >&2
    echo "   Online status: $online_status" >&2

    if [[ "$online_status" == "online" ]]; then
        echo "   Using online packages: $DEFAULT_SPARK_JARS" >&2
        echo "--packages $DEFAULT_SPARK_JARS"
    else
        echo "   Checking local JARs in: $LOCAL_SPARK_JAR_PATH" >&2
        local local_jars=$(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | tr '\n' ',' | sed 's/,$//')

        if [[ -n "$local_jars" ]]; then
            echo "   Using local JARs: $(echo $local_jars | tr ',' ' ' | wc -w) found" >&2
            echo "--jars $local_jars"
        else
            echo "   ‚ö†Ô∏è  No local JARs found, downloading recommended..." >&2
            download_spark_jars_if_needed

            # Try again after download
            local_jars=$(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | tr '\n' ',' | sed 's/,$//')
            if [[ -n "$local_jars" ]]; then
                echo "   Using downloaded JARs" >&2
                echo "--jars $local_jars"
            else
                echo "   ‚ö†Ô∏è  Proceeding without additional JARs" >&2
                echo ""
            fi
        fi
    fi
}

# Download essential JARs when offline
function download_spark_jars_if_needed {
    echo "üì¶ Downloading essential Spark JARs for offline use..."

    # Core JAR URLs (Maven Central)
    local jar_urls=(
        "https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.5_2.12/1.7.1/sedona-spark-shaded-3.5_2.12-1.7.1.jar"
        "https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.7.1-28.5/geotools-wrapper-1.7.1-28.5.jar"
        "https://repo1.maven.org/maven2/graphframes/graphframes/0.8.3-spark3.5-s_2.12/graphframes-0.8.3-spark3.5-s_2.12.jar"
    )

    for url in "${jar_urls[@]}"; do
        local jar_name=$(basename "$url")
        local jar_path="$LOCAL_SPARK_JAR_PATH/$jar_name"

        if [[ ! -f "$jar_path" ]]; then
            echo "  Downloading: $jar_name"
            if curl -sL "$url" -o "$jar_path"; then
                echo "  ‚úÖ Downloaded: $jar_name"
            else
                echo "  ‚ùå Failed: $jar_name"
                rm -f "$jar_path"  # Remove failed download
            fi
        else
            echo "  ‚úÖ Already exists: $jar_name"
        fi
    done
}

# Test dependency resolution
function test_spark_dependencies {
    echo "üß™ Testing Spark dependency resolution..."
    echo ""

    echo "Current status:"
    local deps=$(get_spark_dependencies 2>&1)
    echo "Dependencies resolved: $(echo "$deps" | tail -1)"
    echo ""

    echo "Local JAR inventory:"
    if [[ -d "$LOCAL_SPARK_JAR_PATH" ]]; then
        find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" -exec basename {} \; | sort
        echo "Total JARs: $(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" | wc -l)"
    else
        echo "No local JAR directory found"
    fi
}

# =====================================================
# SPARK CLUSTER MANAGEMENT
# =====================================================

function spark_start {
    echo "üöÄ Starting Spark cluster..."

    # Stop any existing processes
    pkill -f 'org.apache.spark.deploy.master.Master' 2>/dev/null
    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null
    sleep 3

    # Start master
    echo "Starting master..."
    $SPARK_HOME/sbin/start-master.sh
    sleep 5

    # Start workers (4 workers for stability)
    echo "Starting workers..."
    for i in {1..4}; do
        $SPARK_HOME/sbin/start-worker.sh spark://127.0.0.1:7077
        sleep 1
    done

    # Set master URL
    export SPARK_MASTER_URL="spark://127.0.0.1:7077"

    echo "‚úÖ Cluster started!"
    echo "üìä Master UI: http://127.0.0.1:8080"
    echo "üéØ Master URL: $SPARK_MASTER_URL"

    # Simple functionality test
    sleep 10
    spark_test_simple
}

function spark_stop {
    echo "üõë Stopping Spark cluster..."

    # Use Spark's stop scripts
    if [[ -f "$SPARK_HOME/sbin/stop-all.sh" ]]; then
        $SPARK_HOME/sbin/stop-all.sh
    fi

    # Force kill any remaining
    pkill -f 'org.apache.spark.deploy.master.Master' 2>/dev/null
    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null

    unset SPARK_MASTER_URL
    echo "‚úÖ Cluster stopped"
}

function spark_restart {
    echo "üîÑ Restarting Spark cluster..."
    spark_stop
    sleep 3
    spark_start
}

function spark_status {
    echo "üìä Spark Cluster Status:"
    echo "   Master processes: $(ps aux | grep 'spark.deploy.master.Master' | grep -v grep | wc -l)"
    echo "   Worker processes: $(ps aux | grep 'spark.deploy.worker.Worker' | grep -v grep | wc -l)"
    echo "   Master URL: ${SPARK_MASTER_URL:-'Not set'}"
    echo "   Master UI: http://127.0.0.1:8080"

    # Quick functional test
    if [[ -n "$SPARK_MASTER_URL" ]]; then
        echo "   Testing functionality..."
        spark_test_simple
    fi
}

# =====================================================
# SPARK TESTING FUNCTIONS
# =====================================================

function spark_test_simple {
    echo "üß™ Quick Spark functionality test..."

    python3 -c "
import sys
try:
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.master('spark://127.0.0.1:7077').appName('QuickTest').getOrCreate()
    result = spark.sparkContext.parallelize([1,2,3,4,5]).sum()
    print(f'‚úÖ Cluster functional: sum = {result}')
    spark.stop()
except Exception as e:
    print(f'‚ùå Test failed: {e}')
    sys.exit(1)
" 2>/dev/null
}

# Enhanced default submit with dependency resolution
function default_spark_submit() {
    local py_file="$1"
    if [[ -z "$py_file" ]]; then
        echo "Usage: default_spark_submit <python_file>"
        return 1
    fi

    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi

    echo "üöÄ Local Spark submit with enhanced dependencies..."
    local dependencies=$(get_spark_dependencies)

    # Use eval to properly expand dependencies
    eval "spark-submit \
        --master 'local[*]' \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        $dependencies \
        '$py_file'"
}

# Enhanced distributed submit
function distributed_spark_submit() {
    local py_file="$1"
    local master_url="${2:-$SPARK_MASTER_URL}"

    if [[ -z "$py_file" ]]; then
        echo "Usage: distributed_spark_submit <python_file> [master_url]"
        return 1
    fi

    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi

    if [[ -z "$master_url" ]]; then
        echo "‚ùå No master URL. Run: spark_start"
        return 1
    fi

    echo "üåê Distributed Spark submit with enhanced dependencies..."
    local dependencies=$(get_spark_dependencies)

    # Use eval to properly expand dependencies
    eval "spark-submit \
        --master '$master_url' \
        --deploy-mode client \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --executor-cores 1 \
        --num-executors 4 \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        --conf spark.network.timeout=300s \
        $dependencies \
        '$py_file'"
}

# Smart environment detection submit
function smart_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: smart_spark_submit <python_file>"
        return 1
    fi

    echo "ü§ñ Smart environment detection..."

    # Check for standalone Spark cluster
    if [ -n "$SPARK_MASTER_URL" ] && ps aux | grep -i "spark.deploy.master.Master" | grep -v "grep" > /dev/null; then
        echo "‚úÖ Local Spark cluster detected - using distributed mode"
        distributed_spark_submit "$py_file"
        return
    fi

    # Check if we can start a local cluster
    if [[ -n "$SPARK_HOME" ]] && [[ -f "$SPARK_HOME/sbin/start-master.sh" ]]; then
        echo "‚ÑπÔ∏è  No running cluster found - would you like to start one? (y/n)"
        read "start_cluster?"
        if [[ "$start_cluster" == "y" ]]; then
            spark_start
            distributed_spark_submit "$py_file"
            return
        fi
    fi

    # Fall back to local mode
    echo "‚ÑπÔ∏è  Using local mode"
    default_spark_submit "$py_file"
}

# Backwards compatibility for your existing names
function spark_submit_local {
    default_spark_submit "$@"
}

function spark_submit_cluster {
    distributed_spark_submit "$@"
}

function graceful_spark_restart {
    spark_restart
}

function start_local_spark_cluster {
    spark_start
}

function stop_local_spark_cluster {
    spark_stop
}

function check_spark_cluster_health {
    spark_status
}

# =====================================================
# SPARK CONFIGURATION
# =====================================================

function spark_fix_logging {
    echo "üîß Reducing Spark logging noise..."

    # Create log4j2.properties to reduce INFO spam
    cat > $SPARK_HOME/conf/log4j2.properties << 'EOF'
# Reduce Spark logging noise
rootLogger.level = WARN
rootLogger.appenderRefs = stdout
rootLogger.appenderRef.stdout.ref = console

appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Set Spark components to WARN
logger.spark.name = org.apache.spark
logger.spark.level = WARN

logger.hadoop.name = org.apache.hadoop
logger.hadoop.level = WARN

logger.akka.name = akka
logger.akka.level = WARN

logger.jetty.name = org.eclipse.jetty
logger.jetty.level = WARN
EOF

    echo "‚úÖ Logging reduced to WARN level"
}

function show_spark_config() {
    echo "‚öôÔ∏è  Enhanced Spark Configuration:"
    echo ""
    echo "üè† Environment:"
    echo "   SPARK_HOME: ${SPARK_HOME:-'Not set'}"
    echo "   SPARK_MASTER_URL: ${SPARK_MASTER_URL:-'Not set'}"
    echo "   Java: ${JAVA_HOME:-'Not set'}"
    echo ""
    echo "ü§ñ Auto-Setup System:"
    echo "   Enabled on startup: $AUTO_SETUP_ON_STARTUP"
    echo "   Check online: $AUTO_SETUP_CHECK_ONLINE"
    echo "   Verbose mode: $AUTO_SETUP_VERBOSE"
    echo ""
    echo "üì¶ Dependencies:"
    echo "   Online status: $(is_online)"
    echo "   Default JARs: $DEFAULT_SPARK_JARS"
    echo "   Local JAR path: $LOCAL_SPARK_JAR_PATH"
    echo "   Local JARs available: $(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | wc -l)"
    echo ""
    echo "üí° Available functions:"
    echo "   # Core Spark functions"
    echo "   default_spark_submit     - Local with dependency resolution"
    echo "   distributed_spark_submit - Cluster with dependency resolution"
    echo "   smart_spark_submit       - Auto-detect execution environment"
    echo ""
    echo "   # Testing & diagnostics"
    echo "   test_spark_comprehensive - Full Sedona + GraphFrames test"
    echo "   test_spark_dependencies  - Test dependency resolution"
    echo ""
    echo "   # Auto-setup system"
    echo "   enable_auto_setup        - Enable auto-setup on startup"
    echo "   disable_auto_setup       - Disable auto-setup"
    echo "   auto_setup_environment   - Run setup manually"
    echo "   setup_environment_status - Check setup status"
    echo "   show_version_strategy    - Show pinned version strategy"
    echo "   verify_version_compatibility - Check version compatibility"
    echo ""
    echo "   # Function backup system"
    echo "   backup_critical_functions - Backup important functions"
    echo "   restore_critical_functions - Restore from backup"
    echo "   emergency_restore_test_function - Emergency restore test function"
    echo ""
}

function spark_config {
    show_spark_config
}

# Smart conditional startup - only run if enabled
if [[ "$AUTO_SETUP_ON_STARTUP" == "true" ]]; then
    echo "üîÑ Auto-setup is enabled, running environment setup..."
    auto_setup_environment &  # Run in background to not block shell startup
fi

# Auto-backup critical functions on startup (silent)
backup_critical_functions > /dev/null 2>&1

# =====================================================
# PATH EXPORTS & FINAL SETUP
# =====================================================

# USEFUL paths
export GEOCODE="/Users/dheerajchand/Documents/Professional/Siege_Analytics/Clients/TAN/Projects/tan_geocoding_test"
export RESUME_GENERATOR="/Users/dheerajchand/Documents/Professional/resume_generator"

# SDKMAN Setup
export SDKMAN_DIR=$(brew --prefix sdkman-cli)/libexec
[[ -s "${SDKMAN_DIR}/bin/sdkman-init.sh" ]] && source "${SDKMAN_DIR}/bin/sdkman-init.sh"

### MANAGED BY RANCHER DESKTOP START (DO NOT EDIT)
export PATH="/Users/dheerajchand/.rd/bin:$PATH"
### MANAGED BY RANCHER DESKTOP END (DO NOT EDIT)

# Auto-fix logging on startup
spark_fix_logging 2>/dev/null

# Display fortune
fortune

echo "üöÄüçå RIDICULOUS BANANA HAMMOCK ZSHRC LOADED SUCCESSFULLY! üçåüöÄ"
echo "üî• Enhanced zshrc loaded with advanced Spark features! üî•"
echo "üí° Key features: Pinned versions (Java 17, Scala 2.12+3, Spark 3.5.3), Modern Sedona 1.7.1, Auto-setup, Function backups"
echo "üí° Quick start: test_spark_comprehensive | show_version_strategy | verify_version_compatibility"
