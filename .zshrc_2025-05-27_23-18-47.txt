# Path to your oh-my-zsh configuration.
export ZSH=$HOME/.dotfiles/oh-my-zsh

export ZSH_THEME="powerlevel9k/powerlevel9k"
POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(dir nvm vcs)
POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status history time)

export CASE_SENSITIVE="true"
export DISABLE_AUTO_TITLE="true"

plugins=(colorize compleat dirpersist autojump git gulp history cp)
source $ZSH/oh-my-zsh.sh

autoload -U add-zsh-hook

export NVM_DIR="$HOME/.nvm"
[ -s "/opt/homebrew/opt/nvm/nvm.sh" ] && . "/opt/homebrew/opt/nvm/nvm.sh"
[ -s "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm" ] && . "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm"

load-nvmrc() {
  if [[ -f .nvmrc && -r .nvmrc ]]; then
    nvm use &> /dev/null
  else
    nvm use stable
  fi
}
add-zsh-hook chpwd load-nvmrc
load-nvmrc

unsetopt correct

# MacOS things
defaults write -g ApplePressAndHoldEnabled -bool true

export WORKING_ON_LAPTOP="True"

# Default editor
# Set Zed as the default GUI editor
export EDITOR="zed"
export VISUAL="zed"

# Use Neovim as fallback in the terminal when Zed is unavailable
alias vim="nvim"
alias edit="nvim"


# Python things
eval "$(pyenv init -)"
eval "$(pyenv virtualenv-init -)"
eval "$(pyenv init --path)"

function cleanvenv {
    pip freeze | grep -v "^-e" | xargs pip uninstall -y
}

function remove_python_cruft {
    find . -name "*.pyc" -delete
    find . -name "__pycache__" -exec rm -r {} +
}

export PREFERRED_VENV="geo31111"
pyenv activate $PREFERRED_VENV

# Useful functions
export ZSHRC_BACKUPS=~/.zshrc_backups

# Ensure backup directory exists
mkdir -p "$ZSHRC_BACKUPS"

function backup_zshrc {
    local prev_dir="$(pwd)"  # ‚úÖ Save current directory
    timestamp=$(date +"%Y-%m-%d_%H-%M-%S")
    backup_file="$ZSHRC_BACKUPS/.zshrc_$timestamp.txt"
    log_file="$ZSHRC_BACKUPS/zshrc_backup_log.txt"

    # Ensure backup directory is initialized as a Git repository
    if [[ ! -d "$ZSHRC_BACKUPS/.git" ]]; then
        echo "‚ö†Ô∏è Backup directory is not a Git repository. Initializing..."
        git -C "$ZSHRC_BACKUPS" init
        git -C "$ZSHRC_BACKUPS" remote add origin "<YOUR_GIT_REPO_URL>"  # üîß Replace with your repo URL
    fi

    # Copy .zshrc file and save as .txt
    cp ~/.dotfiles/homedir/.zshrc "$backup_file"

    # Log the backup in a .txt file
    echo "$timestamp - Backup saved: $backup_file" >> "$log_file"

    # Add changes to Git, commit, and push
    git -C "$ZSHRC_BACKUPS" add .
    git -C "$ZSHRC_BACKUPS" commit -m "Backup .zshrc at $timestamp"
    git -C "$ZSHRC_BACKUPS" push origin main  # üîß Adjust branch name if needed

    echo "‚úÖ Backup created at $backup_file"
    echo "üìú Logged in $log_file"
    echo "üöÄ Changes committed & pushed to Git repository!"

    cd "$prev_dir"  # ‚úÖ Restore original directory
    echo "üîÑ Returned to: $prev_dir"
}

function zshreboot {
    source ~/.zshrc
}

function zshconfig {
    zed ~/.config/zsh/zshrc
}

# PSQL settings
export PGHOST="localhost"
export PGUSER="dheerajchand"
export PGPASSWORD="dessert"
export PGPORT="5432"
export PGDATABASE="gis"

export GEODJANGO_TEMPLATE_SQL_DATABASE="geodjango_template_db"
export GEODJANGO_TEMPLATE_SQL_USER="dheerajchand"
export GEODJANGO_TEMPLATE_SQL_PASSWORD="dessert"
export GEODJANGO_TEMPLATE_SQL_PORT="5432"

export PATH="/Users/dheerajchand/.rd/bin:$PATH"
export DEFAULT_DOCKER_CONTEXT="rancher-desktop"

# GIS things
export GDAL_LIBRARY_PATH="$(gdal-config --prefix)/lib/libgdal.dylib"
export GEOS_LIBRARY_PATH="$(geos-config --prefix)/lib/libgeos_c.dylib"

function update_local_repo {
    for remote in `git branch -r`; do git branch --track ${remote#origin/} $remote; done
}
export GIT_DISCOVERY_ACROSS_FILESYSTEM=1

# useful functions

# -------------------------------
# Function to Check Internet Connectivity
# -------------------------------
function is_online {
    ping -c 1 google.com &> /dev/null && echo "online" || echo "offline"
}


# ------------------------------------------------------------------
# 3. Zeppelin Setup & Notebook Functions
# ------------------------------------------------------------------

export ZEPPELIN_HOME="$HOME/zeppelin"
export PATH="$ZEPPELIN_HOME/bin:$PATH"
mkdir -p "$ZEPPELIN_HOME"

check_zeppelin() {
    if [[ ! -d "$ZEPPELIN_HOME" ]]; then
        echo "‚ùå Zeppelin directory not found: $ZEPPELIN_HOME"
        return 1
    elif ! "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" status > /dev/null 2>&1; then
        echo "Zeppelin is not running."
        return 1
    else
        echo "‚úÖ Zeppelin is running."
        return 0
    fi
}

function start_zeppelin {
    echo "Starting Zeppelin on port 9090..."

    # Ensure Zeppelin is stopped before starting fresh
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" stop

    # Explicitly set Zeppelin's port to 9090 in config
    sed -i '' 's/<value>8080<\/value>/<value>9090<\/value>/' "$ZEPPELIN_HOME/conf/zeppelin-site.xml"

    # Start Zeppelin with the new port configuration
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" start

    sleep 3  # Wait for Zeppelin to initialize

    echo "‚úÖ Zeppelin started at: http://localhost:9090"
}


function stop_zeppelin() {
    echo "Stopping Zeppelin..."
    "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" stop
    echo "Zeppelin stopped."
}

function restart_zeppelin() {
    stop_zeppelin
    start_zeppelin
}

function reset_zeppelin() {
    stop_zeppelin
    sleep 3

    echo "üîç Ensuring Zeppelin is fully stopped..."
    if ps aux | grep -i 'zeppelin' | grep -v 'grep' > /dev/null; then
        echo "‚ùå Zeppelin is still running! Killing processes..."
        pkill -9 -f 'zeppelin'
        sleep 2
    fi

    # Cleanup
    [ -d "$ZEPPELIN_HOME/run" ] && rm -rf "$ZEPPELIN_HOME/run"
    [ -d "$ZEPPELIN_HOME/logs" ] && rm -rf "$ZEPPELIN_HOME/logs"
    [ -d "$ZEPPELIN_HOME/local-repo/spark" ] && rm -rf "$ZEPPELIN_HOME/local-repo/spark"

    start_zeppelin
    sleep 3  # Allow restart

    # ‚úÖ Verify Zeppelin is running
    if ps aux | grep -i 'zeppelin' | grep -v 'grep' > /dev/null; then
        echo "‚úÖ Zeppelin restarted successfully!"
    else
        echo "‚ùå Zeppelin failed to start. Check logs!"
        tail -n 50 "$ZEPPELIN_HOME/logs/zeppelin.log"
    fi
}


# ‚úÖ Function to Check PySpark & Notebook Dependencies
function check_pyspark_dependencies() {
    if ! command -v pyspark &> /dev/null; then
        echo "‚ùå PySpark not found! Install it with: pip install pyspark"
        return 1
    fi

    if ! command -v jupyter &> /dev/null && ! command -v "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" &> /dev/null; then
        echo "‚ùå Neither Jupyter nor Zeppelin found! Install Jupyter: pip install notebook"
        return 1
    fi

    echo "‚úÖ Dependencies verified!"
    return 0
}

# ‚úÖ Function to Dynamically Check Dependencies Based on Notebook Type
function check_notebook_dependencies {
    local notebook_type="$1"

    case "$notebook_type" in
        jupyter|jupyterlab)
            if ! command -v pyspark &> /dev/null; then
                echo "‚ùå PySpark not found! Install it with: pip install pyspark"
                return 1
            fi
            if ! command -v jupyter &> /dev/null; then
                echo "‚ùå Jupyter not found! Install it with: pip install notebook"
                return 1
            fi
            ;;
        zeppelin)
            if [[ ! -d "$ZEPPELIN_HOME" ]] || ! command -v "$ZEPPELIN_HOME/bin/zeppelin-daemon.sh" &> /dev/null; then
                echo "‚ùå Zeppelin is missing! Ensure it's installed and configured."
                return 1
            fi
            ;;
        databricks)
            if ! command -v databricks &> /dev/null; then
                echo "‚ùå Databricks CLI not found! Install it with: pip install databricks-cli"
                return 1
            fi
            ;;
        vscode)
            if ! command -v code &> /dev/null; then
                echo "‚ùå VS Code not found! Ensure it's installed and available in PATH."
                return 1
            fi
            ;;
        *)
            echo "‚ùå Unknown notebook type: $notebook_type"
            return 1
            ;;
    esac

    echo "‚úÖ Dependencies verified for $notebook_type!"
    return 0
}

# ‚úÖ Function to Specifically Check PySpark Dependencies
function check_pyspark_dependencies {
    if ! command -v pyspark &> /dev/null; then
        echo "‚ùå PySpark not found! Install it with: pip install pyspark"
        return 1
    fi

    echo "‚úÖ PySpark dependencies verified!"
    return 0
}

# ‚úÖ Function to Manage Different Notebook Types (Now with Full Dependency Validation)
function notebook_manager {
    local notebook_type="$1"
    local port="${2:-8888}"  # Default to port 8888
    local notebook_dir="${3:-$(pwd)}"  # Default to current directory

    # ‚úÖ Validate dependencies dynamically based on requested notebook type
    check_notebook_dependencies "$notebook_type" || return 1

    case "$notebook_type" in
        jupyter)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="notebook --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting PySpark Jupyter Notebook on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Notebook launch failed!"; return 1; }
            ;;
        jupyterlab)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="lab --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting Jupyter Lab on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Lab launch failed!"; return 1; }
            ;;
        zeppelin)
            echo "üöÄ Starting Zeppelin Notebook..."
            start_zeppelin || { echo "‚ùå Zeppelin launch failed!"; return 1; }
            ;;
        databricks)
            echo "üöÄ Launching Databricks CLI..."
            databricks workspace import_dir "$notebook_dir" || { echo "‚ùå Databricks CLI failed!"; return 1; }
            ;;
        vscode)
            echo "üöÄ Opening VS Code for notebook editing..."
            code "$notebook_dir" || { echo "‚ùå VS Code launch failed!"; return 1; }
            ;;
        *)
            echo "‚ùå Invalid notebook type: $notebook_type. Available options: jupyter, jupyterlab, zeppelin, databricks, vscode."
            return 1
            ;;
    esac
}

# ‚úÖ Wrapper Function for PySpark Notebook Launching (Now Calls the Notebook Manager)
function pyspark_notebook {
    local notebook_type="${1:-jupyter}"  # Default to Jupyter if no type is provided
    notebook_manager "$notebook_type"
}

# -------------------------------
# BIG CONFIGS FOR JAVA RELATED THINGS
# -------------------------------

# java

# export JAVA_HOME=$(sdk home java)
export JAVA_HOME=""
export PATH=$JAVA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$JAVA_HOME/lib:$LD_LIBRARY_PATH

function set_best_java_version {
    if [[ -z "$JAVA_HOME" ]]; then
        echo "üîç JAVA_HOME is not set. Detecting best Java version for Hadoop & Spark..."

        local best_java_version
        local log_file="$HOME/.java_version.log"

        if [[ "$(is_online)" == "online" ]]; then
            best_java_version=$(sdk list java | awk '/11\.0/ {print $NF}' | sort -r | head -n 1)

            if [[ -z "$best_java_version" ]]; then
                echo "‚ö† No recommended Java version found online! Defaulting to Java 11."
                best_java_version="11.0.22-tem"
            fi
        else
            echo "‚ö† No internet connection detected! Defaulting to Java 11."
            best_java_version="11.0.22-tem"
        fi

        if [[ -z "$best_java_version" ]]; then
            echo "‚ùå Java detection failed. Falling back to Java 11."
            best_java_version="11.0.22-tem"
        fi

        sdk install java $best_java_version
        sdk default java $best_java_version

        export JAVA_HOME=$(sdk home java $best_java_version)
        export PATH=$JAVA_HOME/bin:$PATH
        export LD_LIBRARY_PATH=$JAVA_HOME/lib:$LD_LIBRARY_PATH

        echo "‚úÖ Java version set to $best_java_version for Spark & Hadoop compatibility."
        echo "$(date): Java version set to $best_java_version" >> "$log_file"
    fi
}

# Run automatically if JAVA_HOME is missing
set_best_java_version

# Hadoop

function set_latest_hadoop_version {
    echo "üîç Detecting latest compatible Hadoop version for Spark 3.4.0..."

    local latest_version
    local log_file="$HOME/.hadoop_version.log"
    local attempts=0
    local max_attempts=3

    if [[ "$(is_online)" == "online" ]]; then
        # Exclude 3.3.5 from selection
        latest_version=$(sdk list hadoop | grep '3.3' | grep -v '[*>+-]' | grep -v '3.3.5' | sort -r | head -n 1 | awk '{print $1}')

        if [[ -z "$latest_version" ]]; then
            echo "‚ö† No compatible Hadoop version found online! Defaulting to 3.3.6."
            latest_version="3.3.6"
        fi
    else
        echo "‚ö† No internet connection detected! Defaulting to 3.3.6."
        latest_version="3.3.6"
    fi

    echo "‚è≥ Attempting to install Hadoop $latest_version..."
    if sdk install hadoop $latest_version; then
        echo "‚úÖ Successfully installed Hadoop $latest_version."
    else
        echo "‚ùå Failed to install Hadoop $latest_version. Falling back to 3.3.6..."
        sdk install hadoop 3.3.6
        latest_version="3.3.6"
    fi

    export HADOOP_CURRENT_VERSION="$latest_version"
    export HADOOP_HOME=$(sdk home hadoop $HADOOP_CURRENT_VERSION)
    export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
    export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop/
    export PATH="$HADOOP_HOME/bin:$PATH"

    echo "‚úÖ Hadoop version set to $HADOOP_CURRENT_VERSION for Spark compatibility."
    echo "$(date): Hadoop version set to $HADOOP_CURRENT_VERSION" >> "$log_file"
}

set_latest_hadoop_version

# Maven Configuration Variables
DEFAULT_MAVEN_VERSION="3.9.6"
MAVEN_CANDIDATE_PATH="$SDKMAN_CANDIDATES_DIR/maven"
MAVEN_INSTALL_PATH="$MAVEN_CANDIDATE_PATH/$DEFAULT_MAVEN_VERSION"

# Function to check and install Maven via SDKMAN
setup_maven() {
    if command -v mvn &>/dev/null; then
        echo "Maven is installed: $(mvn -version | head -n 1)"
    else
        echo "Maven not found. Installing Maven $DEFAULT_MAVEN_VERSION via SDKMAN..."

        # Install Maven with SDKMAN
        sdk install maven "$DEFAULT_MAVEN_VERSION" || {
            echo "Failed to install Maven. Please check SDKMAN setup."
            return 1
        }

        # Verify installation
        if command -v mvn &>/dev/null; then
            echo "Maven installation successful: $(mvn -version | head -n 1)"
        else
            echo "Maven installation failed."
        fi
    fi
}

function download_maven_jars {
    local libraries="${1:-$DEFAULT_SPARK_JARS}"  # Default to your Spark JAR list
    local target_path="${2:-$LOCAL_SPARK_JAR_PATH}"  # Default to your local JAR storage
    mkdir -p "$target_path"  # Ensure download directory exists

    echo "üöÄ Downloading Maven dependencies..."
    for lib in $(echo "$libraries" | tr ',' ' '); do
        IFS=':' read -r group artifact version <<< "$lib"
        jar_file="${artifact}-${version}.jar"
        mvn_url="https://repo1.maven.org/maven2/$(echo $group | tr '.' '/')/$artifact/$version/$jar_file"

        echo "üîç Fetching: $jar_file"
        curl -sL -o "$target_path/$jar_file" "$mvn_url" && echo "‚úÖ Saved to $target_path/$jar_file" || echo "‚ùå Failed to download $jar_file"
    done
}


# -------------------------------
# Spark Environment Configuration
# -------------------------------
export SPARK_LOCAL_IP="127.0.0.1"
export SPARK_MASTER_HOST="127.0.0.1"
export SPARK_MASTER_PORT="7077"
export SPARK_WORKER_INSTANCES="$(nproc)"  # Auto-set based on detected CPU cores
export SPARK_DRIVER_MEMORY="4g"  # Adjust as needed
export SPARK_EXECUTOR_MEMORY="4g"  # Set memory allocation for Spark jobs
export SPARK_WORKER_MEMORY="2g"  # Control memory allocation for workers
export SPARK_CONF_DIR="$SPARK_HOME/conf"
export SPARK_CLIENT_CONFIG="$HOME/.spark-client-defaults.properties"

export PYSPARK_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"
export PYSPARK_DRIVER_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"

# -------------------------------
# Dependency Configuration (Online & Offline Mode)
# -------------------------------

# Define Maven package dependencies
export DEFAULT_SPARK_JARS="graphframes:graphframes:0.8.3-spark3.5-s_2.12,org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.1,org.datasyslab:geotools-wrapper:1.7.1-28.5,org.apache.spark:spark-core_2.12:3.5.3"
export DEFAULT_LOCAL_SPARK_JARS=$(echo "$DEFAULT_SPARK_JARS" | awk -F ':' '{print ($1 == "graphframes" ? $1 "-" $3 : $1 "_" $2 "-" $3) ".jar"}' | sed 's/,/\n/g' | awk -v path="$LOCAL_SPARK_JAR_PATH" '{print path "/" $0}')

# Define local Spark JAR paths dynamically from Maven-style dependencies
export LOCAL_SPARK_JAR_PATH="$HOME/local_jars"
mkdir -p $LOCAL_SPARK_JAR_PATH

# -------------------------------
# Function to Dynamically Select Dependencies
# -------------------------------
function get_spark_dependencies {
    local online_status=$(is_online)

    if [[ "$online_status" == "online" ]]; then
        echo "--packages $(echo "$DEFAULT_SPARK_JARS" | tr ',' ',')"
    else
        echo "--jars $(echo "$DEFAULT_LOCAL_SPARK_JARS" | tr '\n' ',')"
    fi
}

# -------------------------------
# Default Spark Submit Settings
# -------------------------------
default_spark_submit() {
    CMD="spark-submit \
    $(get_spark_dependencies) \
    --master \"${SPARK_MASTER:-local[*]}\" \
    --driver-memory ${SPARK_DRIVER_MEMORY:-4g} \
    --executor-memory ${SPARK_EXECUTOR_MEMORY:-4g} \
    --conf spark.executor.instances=${SPARK_EXECUTOR_INSTANCES:-4} \
    --properties-file $SPARK_CLIENT_CONFIG \
    $@"
    echo "Executing: $CMD"
    eval "$CMD"
}


# -------------------------------
# Default Spark Shell Settings
# -------------------------------
function default_spark_shell() {
    local cmd="spark-shell \
    --master 'local[*]' \
    --driver-memory ${SPARK_DRIVER_MEMORY} \
    --executor-memory ${SPARK_EXECUTOR_MEMORY} \
    --properties-file '$SPARK_CLIENT_CONFIG' \
    $(get_spark_dependencies)"

    echo "üöÄ Executing: $cmd"
    eval "$cmd"
}

function default_pyspark_shell() {
    local cmd="pyspark \
    --master 'local[*]' \
    --driver-memory ${SPARK_DRIVER_MEMORY} \
    --executor-memory ${SPARK_EXECUTOR_MEMORY} \
    --properties-file '$SPARK_CLIENT_CONFIG' \
    $(get_spark_dependencies)"

    echo "üöÄ Executing: $cmd"
    eval "$cmd"
}
# -------------------------------
# Spark Submission Arguments (Ensuring Correct Dependency Handling)
# -------------------------------
export PYSPARK_SUBMIT_ARGS="$(get_spark_dependencies) pyspark-shell"

# -------------------------------
# Function to Test Spark Setup
# -------------------------------
function test_spark {
    echo "üöÄ Validating Spark functionality..."

    python3 - <<EOF
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr
from graphframes import GraphFrame
from sedona.register import SedonaRegistrator
from sedona.core import SedonaContext

try:
    spark = (
        SparkSession.builder.appName("TestPySpark")
        .config("spark.sql.extensions", "org.apache.sedona.sql.SedonaSqlExtensions")  # Enables Sedona SQL functions
        .getOrCreate()
    )

    # Register Sedona functions
    SedonaRegistrator.registerAll(spark)
    sedona = SedonaContext.create(spark)

    result = spark.sql("SELECT 'Spark is running!' AS status").collect()
    print("‚úÖ Spark SQL Execution Successful:", result[0]["status"])

    # Explicit GraphFrames verification
    vertices = spark.createDataFrame([("A",), ("B",)], ["id"])
    edges = spark.createDataFrame([("A", "B")], ["src", "dst"])
    g = GraphFrame(vertices, edges)
    g.vertices.show()

    # Validate Sedona Functions
    spark.sql("SHOW FUNCTIONS LIKE 'ST_Point'").show()

    # Simple Sedona test: Create a point and check SRID
    df = spark.createDataFrame([("TEST_POINT", -0.1275, 51.5072)], ["id", "longitude", "latitude"])
    df = df.withColumn("geom", expr("ST_SetSRID(ST_Point(longitude, latitude), 4326)"))
    df.select("id", expr("ST_SRID(geom)").alias("srid")).show()

except Exception as e:
    print("‚ùå Spark SQL, GraphFrames, or Sedona Execution Failed:", str(e))
EOF

    echo "üîç Checking Spark worker processes..."
    local worker_processes=$(ps aux | grep -i "spark.deploy.worker.Worker" | grep -v "grep")

    if [[ -z "$worker_processes" ]]; then
        echo "‚ùå No active Spark worker processes found!"
    else
        echo "‚úÖ Active Spark worker processes detected:"
        echo "$worker_processes"
    fi

    echo "‚úÖ Spark functionality test complete!"
}


# -------------------------------
# Function for Detecting Spark Instances
# -------------------------------

function detect_spark_instances {
    echo "üîç Checking active Spark instances..."
    local running_processes=$(ps aux | grep -i 'spark' | grep -v 'grep')

    if [[ -z "$running_processes" ]]; then
        echo "‚ùå No active Spark instances detected."
        return 1
    else
        echo "‚úÖ Active Spark processes detected:"
        echo "$running_processes"
        return 0
    fi
}

# -------------------------------
# Function for a Graceful Spark Restart
# -------------------------------
function graceful_spark_restart {
    echo "üîç Detecting active Spark instances..."
    detect_spark_instances

    read "confirm_stop?Would you like to stop all Spark processes before restarting? (y/n): "
    if [[ "$confirm_stop" == "y" ]]; then
        stop_spark
    fi

    sleep 5

    local retries=5  # Prevent infinite loops
    for attempt in $(seq 1 $retries); do
        echo "üîç Ensuring Spark processes are fully stopped... (Attempt $attempt)"
        if [[ -z "$(ps aux | grep -i 'spark' | grep -v 'grep')" ]]; then
            echo "‚úÖ Spark processes stopped successfully."
            break
        fi

        echo "‚ö† Warning: Workers still running‚Äîretrying shutdown..."
        stop_spark
        sleep 5
        detect_spark_instances
    done

    echo "üöÄ Starting Spark master..."
    ${SPARK_HOME}/sbin/start-master.sh
    echo "‚úÖ Master running at http://${SPARK_MASTER_HOST}:8080"

    echo "üöÄ Starting fresh Spark workers..."
    start_spark_workers

    echo "‚úÖ Spark restart complete!"
}

# -------------------------------
# Spark Worker Management
# -------------------------------
function stop_spark {
    echo "üö® Stopping all Spark processes..."
    pkill -9 -f 'org.apache.spark.deploy.master.Master'
    pkill -9 -f 'org.apache.spark.deploy.worker.Worker'
    pkill -9 -f 'spark'
    pkill -9 -f 'java' || echo "‚úÖ No lingering Spark Java processes found."
    sleep 5
}

function start_spark_workers {
    echo "üöÄ Starting Spark workers..."

    export SPARK_LOCAL_IP="127.0.0.1"
    export SPARK_WORKER_LOG_DIR="${SPARK_HOME}/logs"
    mkdir -p ${SPARK_WORKER_LOG_DIR}

    local instances="${SPARK_WORKER_INSTANCES:-2}"  # Defaults to 2 if unset

    for i in $(seq 1 $instances); do
        nohup java -Xmx${SPARK_WORKER_MEMORY} -cp "${SPARK_HOME}/jars/*" org.apache.spark.deploy.worker.Worker spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} >> ${SPARK_WORKER_LOG_DIR}/worker-$i.log 2>&1 &
        echo "‚úÖ Started Spark worker process $i"
    done

    echo "‚úÖ All Spark workers started successfully!"
}

function stop_spark_workers() {
    echo "üö´ Stopping Spark workers..."

    # Kill all processes executing the Spark Worker class.
    # Using pkill with -f ensures we match the full command line.
    pkill -f 'org.apache.spark.deploy.worker.Worker'

    # Optionally, wait a moment and confirm they're gone.
    sleep 2
    if pgrep -f 'org.apache.spark.deploy.worker.Worker' > /dev/null; then
        echo "‚ö†Ô∏è Some Spark workers are still running!"
    else
        echo "‚úÖ All Spark workers stopped successfully!"
    fi
}

# -------------------------------
# NEW DISTRIBUTED SPARK FUNCTIONS
# -------------------------------

# Additional environment variables for distributed mode
export SPARK_MASTER_URL="${SPARK_MASTER_URL:-spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}}"
export SPARK_NUM_EXECUTORS="${SPARK_NUM_EXECUTORS:-8}"        # More executors for distributed
export SPARK_EXECUTOR_CORES="${SPARK_EXECUTOR_CORES:-4}"      # More cores per executor
export SPARK_DRIVER_MAX_RESULT_SIZE="${SPARK_DRIVER_MAX_RESULT_SIZE:-2g}"

# Kubernetes configuration (if using K8s)
export SPARK_K8S_MASTER="${SPARK_K8S_MASTER:-k8s://https://kubernetes.default.svc:443}"
export SPARK_K8S_IMAGE="${SPARK_K8S_IMAGE:-your-spark-image:latest}"
export SPARK_K8S_NAMESPACE="${SPARK_K8S_NAMESPACE:-default}"
export SPARK_K8S_SERVICE_ACCOUNT="${SPARK_K8S_SERVICE_ACCOUNT:-spark}"

# Distributed Spark submit function for cluster mode
distributed_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: distributed_spark_submit <python_file> [spark_master_url]"
        echo "Example: distributed_spark_submit my_script.py spark://master-node:7077"
        echo "         distributed_spark_submit my_script.py  # Uses \$SPARK_MASTER_URL"
        return 1
    fi

    # Use provided master URL or fall back to environment variable
    local master_url="${2:-$SPARK_MASTER_URL}"
    if [ -z "$master_url" ]; then
        echo "Error: No Spark master URL provided. Set SPARK_MASTER_URL or provide as second argument."
        echo "Example: export SPARK_MASTER_URL='spark://master-node:7077'"
        return 1
    fi

    # Get dependencies using your existing function
    local dependencies=$(get_spark_dependencies)

    # For standalone clusters, Python apps must use client mode
    local deploy_mode="client"
    if [[ "$master_url" == yarn* ]]; then
        deploy_mode="cluster"
    elif [[ "$master_url" == k8s* ]]; then
        deploy_mode="cluster"
    fi

    local cmd="spark-submit \
        --master $master_url \
        --deploy-mode $deploy_mode \
        --num-executors $SPARK_NUM_EXECUTORS \
        --executor-cores $SPARK_EXECUTOR_CORES \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --properties-file $SPARK_CLIENT_CONFIG \
        --conf spark.driver.maxResultSize=$SPARK_DRIVER_MAX_RESULT_SIZE \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.sql.adaptive.coalescePartitions.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        --conf spark.network.timeout=800s \
        --conf spark.executor.heartbeatInterval=60s \
        $dependencies \
        $py_file"

    echo "üöÄ Executing DISTRIBUTED Spark job:"
    echo "   Master: $master_url"
    echo "   Deploy mode: $deploy_mode (standalone clusters require client mode for Python)"
    echo "   Executors: $SPARK_NUM_EXECUTORS x $SPARK_EXECUTOR_CORES cores x $SPARK_EXECUTOR_MEMORY memory"
    echo "   Driver: $SPARK_DRIVER_MEMORY memory"
    echo "   Using properties file: $SPARK_CLIENT_CONFIG"
    echo ""
    echo "Command: $cmd"
    echo ""
    eval $cmd
}

# Flexible Spark submit that can do both local and distributed
flexible_spark_submit() {
    local py_file="$1"
    local mode="${2:-local}"  # local or distributed

    if [ -z "$py_file" ]; then
        echo "Usage: flexible_spark_submit <python_file> [mode]"
        echo "Modes:"
        echo "  local       - Run locally with all cores (default)"
        echo "  distributed - Run on Spark cluster"
        echo "  yarn        - Run on YARN cluster"
        echo "  k8s         - Run on Kubernetes"
        return 1
    fi

    case "$mode" in
        "local")
            echo "üè† Running in LOCAL mode with all cores"
            default_spark_submit "$py_file"
            ;;
        "distributed")
            echo "üåê Running in DISTRIBUTED mode"
            distributed_spark_submit "$py_file"
            ;;
        "yarn")
            echo "üêò Running on YARN cluster"
            yarn_spark_submit "$py_file"
            ;;
        "k8s")
            echo "‚ò∏Ô∏è  Running on Kubernetes"
            k8s_spark_submit "$py_file"
            ;;
        *)
            echo "‚ùå Unknown mode: $mode"
            echo "Available modes: local, distributed, yarn, k8s"
            return 1
            ;;
    esac
}

# YARN cluster version
yarn_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: yarn_spark_submit <python_file>"
        return 1
    fi

    local dependencies=$(get_spark_dependencies)

    local cmd="spark-submit \
        --master yarn \
        --deploy-mode cluster \
        --num-executors $SPARK_NUM_EXECUTORS \
        --executor-cores $SPARK_EXECUTOR_CORES \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --properties-file $SPARK_CLIENT_CONFIG \
        --conf spark.driver.maxResultSize=$SPARK_DRIVER_MAX_RESULT_SIZE \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.sql.adaptive.coalescePartitions.enabled=true \
        --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=$PYSPARK_PYTHON \
        --conf spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON=$PYSPARK_DRIVER_PYTHON \
        $dependencies \
        $py_file"

    echo "üêò Executing YARN Spark job:"
    echo "   Executors: $SPARK_NUM_EXECUTORS x $SPARK_EXECUTOR_CORES cores x $SPARK_EXECUTOR_MEMORY memory"
    echo "   Using properties file: $SPARK_CLIENT_CONFIG"
    echo ""
    echo "Command: $cmd"
    echo ""
    eval $cmd
}

# Kubernetes version (if using K8s)
k8s_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: k8s_spark_submit <python_file>"
        return 1
    fi

    local dependencies=$(get_spark_dependencies)

    local cmd="spark-submit \
        --master $SPARK_K8S_MASTER \
        --deploy-mode cluster \
        --num-executors $SPARK_NUM_EXECUTORS \
        --executor-cores $SPARK_EXECUTOR_CORES \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --properties-file $SPARK_CLIENT_CONFIG \
        --conf spark.kubernetes.container.image=$SPARK_K8S_IMAGE \
        --conf spark.kubernetes.namespace=$SPARK_K8S_NAMESPACE \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName=$SPARK_K8S_SERVICE_ACCOUNT \
        --conf spark.kubernetes.executor.serviceAccountName=$SPARK_K8S_SERVICE_ACCOUNT \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.sql.adaptive.coalescePartitions.enabled=true \
        $dependencies \
        $py_file"

    echo "‚ò∏Ô∏è  Executing Kubernetes Spark job:"
    echo "   Master: $SPARK_K8S_MASTER"
    echo "   Namespace: $SPARK_K8S_NAMESPACE"
    echo "   Container Image: $SPARK_K8S_IMAGE"
    echo "   Service Account: $SPARK_K8S_SERVICE_ACCOUNT"
    echo "   Executors: $SPARK_NUM_EXECUTORS x $SPARK_EXECUTOR_CORES cores x $SPARK_EXECUTOR_MEMORY memory"
    echo "   Using properties file: $SPARK_CLIENT_CONFIG"
    echo ""
    echo "Command: $cmd"
    echo ""
    eval $cmd
}

# Smart submit that detects your environment
smart_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: smart_spark_submit <python_file>"
        return 1
    fi

    echo "ü§ñ Auto-detecting Spark environment..."

    # Check for YARN
    if command -v yarn >/dev/null 2>&1 && [ -n "$HADOOP_CONF_DIR" ]; then
        echo "‚úÖ YARN detected - using YARN mode"
        yarn_spark_submit "$py_file"
        return
    fi

    # Check for Kubernetes
    if command -v kubectl >/dev/null 2>&1 && [ -n "$SPARK_K8S_MASTER" ]; then
        echo "‚úÖ Kubernetes detected - using K8s mode"
        k8s_spark_submit "$py_file"
        return
    fi

    # Check for standalone Spark cluster
    if [ -n "$SPARK_MASTER_URL" ] && [ "$SPARK_MASTER_URL" != "spark://127.0.0.1:7077" ]; then
        echo "‚úÖ Remote Spark master URL found - using distributed mode"
        distributed_spark_submit "$py_file"
        return
    fi

    # Check if local Spark cluster is running
    if ps aux | grep -i "spark.deploy.master.Master" | grep -v "grep" > /dev/null; then
        echo "‚úÖ Local Spark master detected - using distributed mode"
        distributed_spark_submit "$py_file"
        return
    fi

    # Fall back to local mode
    echo "‚ÑπÔ∏è  No cluster detected - falling back to local mode"
    default_spark_submit "$py_file"
}

# Heavy API workload submit (renamed from geocoding_spark_submit as requested)
heavy_api_submit() {
    local py_file="$1"
    local mode="${2:-distributed}"

    if [ -z "$py_file" ]; then
        echo "Usage: heavy_api_submit <python_file> [mode]"
        echo "This function is optimized for API-heavy workloads (geocoding, web scraping, etc.)"
        echo "Modes: local, distributed (default)"
        return 1
    fi

    # API-specific optimizations
    local api_num_executors="${SPARK_NUM_EXECUTORS:-8}"        # More executors for parallel API calls
    local api_executor_cores="${SPARK_EXECUTOR_CORES:-4}"      # More cores per executor
    local api_executor_memory="${SPARK_EXECUTOR_MEMORY:-6g}"   # More memory for batching
    local api_driver_memory="${SPARK_DRIVER_MEMORY:-4g}"       # More driver memory

    case "$mode" in
        "local")
            echo "üè† Running API-heavy job in LOCAL mode (API calls will be limited)"
            default_spark_submit "$py_file"
            ;;
        "distributed"|*)
            local master_url="${SPARK_MASTER_URL}"
            if [ -z "$master_url" ]; then
                echo "‚ùå No SPARK_MASTER_URL set for distributed API processing"
                echo "   Set SPARK_MASTER_URL or start local cluster first"
                return 1
            fi

            local dependencies=$(get_spark_dependencies)

            # For standalone clusters, Python apps must use client mode
            local deploy_mode="client"
            if [[ "$master_url" == yarn* ]]; then
                deploy_mode="cluster"
            elif [[ "$master_url" == k8s* ]]; then
                deploy_mode="cluster"
            fi

            local cmd="spark-submit \
                --master $master_url \
                --deploy-mode $deploy_mode \
                --num-executors $api_num_executors \
                --executor-cores $api_executor_cores \
                --executor-memory $api_executor_memory \
                --driver-memory $api_driver_memory \
                --properties-file $SPARK_CLIENT_CONFIG \
                --conf spark.driver.maxResultSize=$SPARK_DRIVER_MAX_RESULT_SIZE \
                --conf spark.sql.adaptive.enabled=true \
                --conf spark.sql.adaptive.coalescePartitions.enabled=true \
                --conf spark.sql.shuffle.partitions=$((api_num_executors * api_executor_cores * 3)) \
                --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
                --conf spark.sql.execution.arrow.pyspark.enabled=true \
                --conf spark.network.timeout=800s \
                --conf spark.executor.heartbeatInterval=60s \
                --conf spark.task.maxAttempts=3 \
                --conf spark.task.maxFailures=2 \
                $dependencies \
                $py_file"

            echo "üåê Executing DISTRIBUTED heavy API job (optimized for parallel API calls):"
            echo "   Master: $master_url"
            echo "   Deploy mode: $deploy_mode (standalone clusters require client mode for Python)"
            echo "   Executors: $api_num_executors x $api_executor_cores cores x $api_executor_memory memory"
            echo "   Shuffle partitions: $((api_num_executors * api_executor_cores * 3))"
            echo "   Using properties file: $SPARK_CLIENT_CONFIG"
            echo "   Optimizations: Parallel API calls, batching, network timeouts, task retries"
            echo ""
            echo "Command: $cmd"
            echo ""
            eval $cmd
            ;;
    esac
}

# Helper function to show current Spark configuration
show_spark_config() {
    echo "üîç Current Spark Configuration:"
    echo "   SPARK_MASTER_URL: ${SPARK_MASTER_URL:-'Not set (will use local mode)'}"
    echo "   SPARK_NUM_EXECUTORS: ${SPARK_NUM_EXECUTORS:-'8 (default)'}"
    echo "   SPARK_EXECUTOR_CORES: ${SPARK_EXECUTOR_CORES:-'4 (default)'}"
    echo "   SPARK_EXECUTOR_MEMORY: ${SPARK_EXECUTOR_MEMORY:-'4g (default)'}"
    echo "   SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY:-'4g (default)'}"
    echo "   SPARK_CLIENT_CONFIG: ${SPARK_CLIENT_CONFIG:-'Not set'}"
    echo "   DEFAULT_SPARK_JARS: ${DEFAULT_SPARK_JARS:-'Not set'}"
    echo "   LOCAL_SPARK_JAR_PATH: ${LOCAL_SPARK_JAR_PATH:-'Not set'}"
    echo ""
    echo "‚ò∏Ô∏è  Kubernetes Configuration:"
    echo "   SPARK_K8S_MASTER: ${SPARK_K8S_MASTER:-'Not set'}"
    echo "   SPARK_K8S_IMAGE: ${SPARK_K8S_IMAGE:-'Not set'}"
    echo "   SPARK_K8S_NAMESPACE: ${SPARK_K8S_NAMESPACE:-'Not set'}"
    echo "   SPARK_K8S_SERVICE_ACCOUNT: ${SPARK_K8S_SERVICE_ACCOUNT:-'Not set'}"
    echo ""
    echo "üåê Dependency Resolution:"
    echo "   Online status: $(is_online)"
    echo "   Current dependencies: $(get_spark_dependencies)"
    echo ""
    echo "üí° Available submit functions:"
    echo "   default_spark_submit       - Your current local-mode function"
    echo "   distributed_spark_submit   - Use Spark cluster workers"
    echo "   flexible_spark_submit      - Choose mode (local/distributed/yarn/k8s)"
    echo "   smart_spark_submit         - Auto-detect environment"
    echo "   heavy_api_submit           - Optimized for API-heavy jobs (geocoding, scraping, etc.)"
    echo "   yarn_spark_submit          - YARN cluster mode"
    echo "   k8s_spark_submit           - Kubernetes mode"
}

# Quick cluster management helpers
start_local_spark_cluster() {
    echo "üöÄ Starting local Spark cluster..."
    graceful_spark_restart
    export SPARK_MASTER_URL="spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}"
    echo "‚úÖ Local cluster started. SPARK_MASTER_URL set to: $SPARK_MASTER_URL"
    echo "   Master UI: http://${SPARK_MASTER_HOST}:8080"
}

stop_local_spark_cluster() {
    echo "üõë Stopping local Spark cluster..."
    stop_spark
    unset SPARK_MASTER_URL
    echo "‚úÖ Local cluster stopped. SPARK_MASTER_URL unset (will default to local[*])"
}

# Test distributed functionality
test_distributed_spark() {
    if [ -z "$SPARK_MASTER_URL" ]; then
        echo "‚ùå SPARK_MASTER_URL not set. Starting local cluster first..."
        start_local_spark_cluster
        sleep 5
    fi

    echo "üß™ Testing distributed Spark functionality..."

    # Create a simple test script
    local test_script="/tmp/spark_distributed_test.py"
    cat > "$test_script" << 'EOF'
from pyspark.sql import SparkSession
import sys

try:
    spark = SparkSession.builder.appName("DistributedTest").getOrCreate()

    # Get cluster info
    print(f"Spark Master: {spark.sparkContext.master}")
    print(f"App Name: {spark.sparkContext.appName}")
    print(f"Default Parallelism: {spark.sparkContext.defaultParallelism}")

    # Simple distributed computation
    rdd = spark.sparkContext.parallelize(range(1000), 10)
    result = rdd.map(lambda x: x * x).sum()
    print(f"Distributed computation result: {result}")

    # DataFrame test
    df = spark.range(1000).repartition(10)
    count = df.count()
    print(f"DataFrame count: {count}")

    print("‚úÖ Distributed Spark test completed successfully!")

except Exception as e:
    print(f"‚ùå Distributed Spark test failed: {e}")
    sys.exit(1)
finally:
    if 'spark' in locals():
        spark.stop()
EOF

    # Run the test
    distributed_spark_submit "$test_script"

    # Clean up
    rm -f "$test_script"
}

# path export to fix java/spark

export PATH="/opt/homebrew/opt/sdkman-cli/libexec/candidates/spark/current/bin:$PATH"

# USEFUL paths

export GEOCODE="/Users/dheerajchand/Documents/Professional/Siege_Analytics/Clients/TAN/Projects/tan_geocoding_test"
export RESUME_GENERATOR="/Users/dheerajchand/Documents/Professional/resume_generator"

fortune
# SDKMAN Setup
export SDKMAN_DIR=$(brew --prefix sdkman-cli)/libexec
[[ -s "${SDKMAN_DIR}/bin/sdkman-init.sh" ]] && source "${SDKMAN_DIR}/bin/sdkman-init.sh"

### MANAGED BY RANCHER DESKTOP START (DO NOT EDIT)
export PATH="/Users/dheerajchand/.rd/bin:$PATH"
### MANAGED BY RANCHER DESKTOP END (DO NOT EDIT)
