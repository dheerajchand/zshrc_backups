#!/usr/bin/env zsh
# =================================================================
# SPARK - Apache Spark Cluster Management
# =================================================================
# Start/stop Spark cluster, intelligent job submission
# Uses is_online() to decide: local JARs vs Maven downloads
# =================================================================

# Spark environment setup
if [[ -z "$SPARK_HOME" ]]; then
    if [[ -d "$HOME/.sdkman/candidates/spark/current" ]]; then
        export SPARK_HOME="$HOME/.sdkman/candidates/spark/current"
    elif [[ -d "/opt/homebrew/opt/apache-spark/libexec" ]]; then
        export SPARK_HOME="/opt/homebrew/opt/apache-spark/libexec"
    elif [[ -d "/usr/lib/spark" ]]; then
        export SPARK_HOME="/usr/lib/spark"
    elif [[ -d "/usr/local/spark" ]]; then
        export SPARK_HOME="/usr/local/spark"
    fi
fi
export SPARK_MASTER_HOST="${SPARK_MASTER_HOST:-localhost}"
export SPARK_MASTER_PORT="${SPARK_MASTER_PORT:-7077}"
export SPARK_MASTER_URL="spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}"
export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-2g}"
export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-2g}"

# Check if Spark is available
if [[ ! -d "$SPARK_HOME" ]] && command -v spark-submit >/dev/null 2>&1; then
    SPARK_HOME="$(dirname $(dirname $(which spark-submit)))"
    export SPARK_HOME
fi

# Start Spark cluster
spark_start() {
    if [[ ! -d "$SPARK_HOME" ]]; then
        echo "‚ùå SPARK_HOME not found: $SPARK_HOME"
        echo "Install via SDKMAN or set SPARK_HOME"
        return 1
    fi
    
    # Configure Python for Spark (ensures driver and worker use same Python)
    # CRITICAL: Prevents Python version mismatch between driver and workers
    local python_path
    if command -v pyenv >/dev/null 2>&1; then
        python_path="$(pyenv which python 2>/dev/null || which python)"
    else
        python_path="$(which python3 || which python)"
    fi
    
    # Always recreate spark-env.sh to ensure current Python version
    mkdir -p "$SPARK_HOME/conf"
    cat > "$SPARK_HOME/conf/spark-env.sh" << SPARKENV
# Python configuration - auto-generated by zsh config
# Ensures driver and worker use same Python version
export PYSPARK_PYTHON="$python_path"
export PYSPARK_DRIVER_PYTHON="$python_path"
SPARKENV
    
    echo "üöÄ Starting Spark cluster..."
    
    # Start master (use jps - more reliable for Java processes)
    if ! jps | grep -q "Master"; then
        "$SPARK_HOME/sbin/start-master.sh"
        sleep 3
        if jps | grep -q "Master"; then
            echo "‚úÖ Spark Master started"
        else
            echo "‚ùå Spark Master failed to start"
            return 1
        fi
    else
        echo "‚úÖ Spark Master already running"
    fi
    
    # Start worker
    if ! jps | grep -q "Worker"; then
        "$SPARK_HOME/sbin/start-worker.sh" "$SPARK_MASTER_URL"
        sleep 3
        if jps | grep -q "Worker"; then
            echo "‚úÖ Spark Worker started"
        else
            echo "‚ùå Spark Worker failed to start"
            return 1
        fi
    else
        echo "‚úÖ Spark Worker already running"
    fi
    
    echo ""
    echo "Spark Web UI: http://localhost:8080"
    echo "Master URL: $SPARK_MASTER_URL"
}

# Stop Spark cluster
spark_stop() {
    if [[ ! -d "$SPARK_HOME" ]]; then
        echo "‚ùå SPARK_HOME not found"
        return 1
    fi
    
    echo "‚èπÔ∏è  Stopping Spark cluster..."
    
    "$SPARK_HOME/sbin/stop-worker.sh" 2>/dev/null
    "$SPARK_HOME/sbin/stop-master.sh" 2>/dev/null
    
    echo "‚úÖ Spark cluster stopped"
}

# Show Spark status
spark_status() {
    echo "‚ö° Spark Status"
    echo "=============="
    
    if [[ -d "$SPARK_HOME" ]]; then
        echo "SPARK_HOME: $SPARK_HOME"
        echo "Master URL: $SPARK_MASTER_URL"
    else
        echo "‚ùå SPARK_HOME not found"
        return 1
    fi
    
    echo ""
    
    if pgrep -f "spark.deploy.master.Master" >/dev/null; then
        echo "‚úÖ Master: Running (http://localhost:8080)"
    else
        echo "‚ùå Master: Not running"
    fi
    
    if pgrep -f "spark.deploy.worker.Worker" >/dev/null; then
        echo "‚úÖ Worker: Running"
    else
        echo "‚ùå Worker: Not running"
    fi
}

# Get Spark dependencies based on connectivity
# CRITICAL: Uses is_online() to decide local JARs vs Maven
get_spark_dependencies() {
    local deps=""
    
    # Check for local JARs first (for offline use)
    local jar_dirs=("$HOME/spark-jars" "$HOME/.spark/jars")
    
    for jar_dir in "${jar_dirs[@]}"; do
        if [[ -d "$jar_dir" ]]; then
            # Use zsh globbing instead of ls
            local jars=("$jar_dir"/*.jar(N))
            if [[ ${#jars[@]} -gt 0 ]]; then
                # Convert array to comma-separated list
                local jar_list="${(j:,:)jars}"
                deps="--jars $jar_list"
                echo "üì¶ Using local JARs from $jar_dir (${#jars[@]} files)" >&2
                echo "$deps"
                return 0
            fi
        fi
    done
    
    # No local JARs - check if online for Maven
    if is_online; then
        # Use Maven Central for dependencies when online
        deps="--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0"
        echo "üåê Online: Will download Maven packages" >&2
    else
        # Offline and no local JARs - use basic Spark (will work for most jobs)
        echo "‚ö†Ô∏è  Offline and no local JARs - using basic Spark" >&2
        deps=""
    fi
    
    echo "$deps"
}

# Smart Spark job submission
smart_spark_submit() {
    local py_file="$1"
    
    if [[ -z "$py_file" || ! -f "$py_file" ]]; then
        echo "Usage: smart_spark_submit <python_file>"
        return 1
    fi
    
    local dependencies=$(get_spark_dependencies)
    
    # Check for running cluster (use jps, not pgrep)
    if jps | grep -q "Master"; then
        echo "üåê Using cluster mode: $SPARK_MASTER_URL"
        spark-submit \
            --master "$SPARK_MASTER_URL" \
            --driver-memory "$SPARK_DRIVER_MEMORY" \
            --executor-memory "$SPARK_EXECUTOR_MEMORY" \
            $dependencies \
            "$py_file"
    else
        echo "üíª Using local mode"
        spark-submit \
            --master local[*] \
            --driver-memory "$SPARK_DRIVER_MEMORY" \
            $dependencies \
            "$py_file"
    fi
}

# Interactive PySpark shell
pyspark_shell() {
    local dependencies=$(get_spark_dependencies)
    
    if pgrep -f "spark.deploy.master.Master" >/dev/null; then
        pyspark --master "$SPARK_MASTER_URL" $dependencies
    else
        pyspark --master local[*] $dependencies
    fi
}

# Spark history server
spark_history_server() {
    if [[ ! -d "$SPARK_HOME" ]]; then
        echo "‚ùå SPARK_HOME not found"
        return 1
    fi
    
    "$SPARK_HOME/sbin/start-history-server.sh"
    echo "‚úÖ History server started: http://localhost:18080"
}

# Submit to YARN cluster
spark_yarn_submit() {
    local script_file="$1"
    local deploy_mode="${2:-client}"
    
    if [[ -z "$script_file" || ! -f "$script_file" ]]; then
        echo "Usage: spark_yarn_submit <script_file> [client|cluster]"
        return 1
    fi
    
    if ! command -v yarn >/dev/null 2>&1; then
        echo "‚ùå YARN not available"
        echo "üí° Start Hadoop first: start_hadoop"
        return 1
    fi
    
    echo "üöÄ Submitting to YARN..."
    local dependencies=$(get_spark_dependencies)
    
    spark-submit \
        --master yarn \
        --deploy-mode "$deploy_mode" \
        --driver-memory "$SPARK_DRIVER_MEMORY" \
        --executor-memory "$SPARK_EXECUTOR_MEMORY" \
        $dependencies \
        "$script_file"
}

# Interactive Spark shell
spark_shell() {
    local dependencies=$(get_spark_dependencies)
    
    if pgrep -f "spark.deploy.master.Master" >/dev/null; then
        spark-shell --master "$SPARK_MASTER_URL" $dependencies
    else
        spark-shell --master local[*] $dependencies
    fi
}

# Restart Spark cluster
spark_restart() {
    spark_stop
    sleep 2
    spark_start
}

# Aliases
alias spark-ui='open http://localhost:8080'
alias spark-history='open http://localhost:18080'
alias ss='spark_status'
alias pyspark='pyspark_shell'

echo "‚úÖ spark loaded"
