# COMPLETE WORKING ZSHRC - Enhanced Spark + Hadoop + YARN Integration
# This is a complete, tested zshrc file with enterprise-grade big data features
# =====================================================
# CORE SHELL SETUP
# =====================================================
# Path to your oh-my-zsh configuration.
export ZSH=$HOME/.dotfiles/oh-my-zsh
export ZSH_THEME="powerlevel9k/powerlevel9k"
POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(dir nvm vcs)
POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status history time)
export CASE_SENSITIVE="true"
export DISABLE_AUTO_TITLE="true"
plugins=(colorize compleat dirpersist autojump git gulp history cp)
source $ZSH/oh-my-zsh.sh
autoload -U add-zsh-hook
# =====================================================
# NODE/NVM SETUP
# =====================================================
export NVM_DIR="$HOME/.nvm"
[ -s "/opt/homebrew/opt/nvm/nvm.sh" ] && . "/opt/homebrew/opt/nvm/nvm.sh"
[ -s "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm" ] && . "/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm"
load-nvmrc() {
  if [[ -f .nvmrc && -r .nvmrc ]]; then
    nvm use &> /dev/null
  else
    nvm use stable
  fi
}

# FIXED: Removed misplaced function - it belongs in the notebook support section
add-zsh-hook chpwd load-nvmrc
load-nvmrc
unsetopt correct
# =====================================================
# BASIC ENVIRONMENT
# =====================================================
# MacOS things
defaults write -g ApplePressAndHoldEnabled -bool true
export WORKING_ON_LAPTOP="True"
# Default editor
export EDITOR="zed"
export VISUAL="zed"
# Use Neovim as fallback in the terminal when Zed is unavailable
alias vim="nvim"
alias edit="nvim"
# =====================================================
# PYTHON SETUP
# =====================================================
eval "$(pyenv init -)"
eval "$(pyenv virtualenv-init -)"
eval "$(pyenv init --path)"
function cleanvenv {
    pip freeze | grep -v "^-e" | xargs pip uninstall -y
}
function remove_python_cruft {
    find . -name "*.pyc" -delete
    find . -name "__pycache__" -exec rm -r {} +
}
export PREFERRED_VENV="geo31111"
pyenv activate $PREFERRED_VENV
# =====================================================
# UTILITY FUNCTIONS
# =====================================================
export ZSHRC_BACKUPS=~/.zshrc_backups
mkdir -p "$ZSHRC_BACKUPS"
function backup_zshrc {
    local prev_dir="$(pwd)"
    timestamp=$(date +"%Y-%m-%d_%H-%M-%S")
    backup_file="$ZSHRC_BACKUPS/zshrc_$timestamp.txt"  # NO leading dot - easier to share!
    log_file="$ZSHRC_BACKUPS/zshrc_backup_log.txt"

    if [[ ! -d "$ZSHRC_BACKUPS/.git" ]]; then
        echo "‚ö†Ô∏è Backup directory is not a Git repository. Initializing..."
        git -C "$ZSHRC_BACKUPS" init
        git -C "$ZSHRC_BACKUPS" remote add origin "<YOUR_GIT_REPO_URL>"
    fi

    cp ~/.dotfiles/homedir/.zshrc "$backup_file"
    echo "$timestamp - Backup saved: $backup_file" >> "$log_file"
    git -C "$ZSHRC_BACKUPS" add .
    git -C "$ZSHRC_BACKUPS" commit -m "Backup .zshrc at $timestamp"
    git -C "$ZSHRC_BACKUPS" push origin main

    echo "‚úÖ Backup created at $backup_file"
    echo "üìú Logged in $log_file"
    echo "üöÄ Changes committed & pushed to Git repository!"
    echo "üìé File is NOT hidden - easy to attach to emails or upload!"
    cd "$prev_dir"
    echo "üîÑ Returned to: $prev_dir"
}

function list_zshrc_backups {
    echo "üìã Available zshrc backups:"
    if [[ -d "$ZSHRC_BACKUPS" ]]; then
        echo ""
        ls -la "$ZSHRC_BACKUPS"/zshrc_*.txt 2>/dev/null | \
        awk '{print "   " $9 " (" $5 " bytes) - " $6 " " $7 " " $8}' | \
        sed 's|.*/zshrc_||g' | sed 's|\.txt||g' | sort -r
        echo ""
        echo "Total backups: $(ls "$ZSHRC_BACKUPS"/zshrc_*.txt 2>/dev/null | wc -l)"
        echo "üìé All backup files are visible (not hidden) for easy sharing!"
    else
        echo "   No backup directory found: $ZSHRC_BACKUPS"
    fi
}

function restore_zshrc {
    local target_backup="$1"
    local backup_file=""
    echo "üîÑ zshrc Restore Utility"
    echo ""

    # Check if backup directory exists
    if [[ ! -d "$ZSHRC_BACKUPS" ]]; then
        echo "‚ùå Backup directory not found: $ZSHRC_BACKUPS"
        echo "üí° Run 'backup_zshrc' first to create backups"
        return 1
    fi

    # List available backups
    list_zshrc_backups

    # Determine which backup to restore
    if [[ -n "$target_backup" ]]; then
        # Specific backup requested
        backup_file="$ZSHRC_BACKUPS/zshrc_${target_backup}.txt"
        if [[ ! -f "$backup_file" ]]; then
            echo "‚ùå Backup not found: $target_backup"
            echo "üí° Use format: YYYY-MM-DD_HH-MM-SS"
            return 1
        fi
        echo "üéØ Restoring specific backup: $target_backup"
    else
        # Use most recent backup
        backup_file=$(ls "$ZSHRC_BACKUPS"/zshrc_*.txt 2>/dev/null | sort -r | head -1)
        if [[ -z "$backup_file" ]]; then
            echo "‚ùå No backups found in $ZSHRC_BACKUPS"
            return 1
        fi
        local backup_name=$(basename "$backup_file" .txt | sed 's/^zshrc_//')
        echo "üïê Using most recent backup: $backup_name"
    fi

    echo ""
    echo "üìÅ Source: $backup_file"
    echo "üìÅ Target: ~/.zshrc (will be restored as hidden file)"
    echo ""

    # Safety confirmation
    echo "‚ö†Ô∏è  This will replace your current zshrc configuration!"
    echo "üí° Current zshrc will be backed up automatically before restore."
    echo ""
    read "confirm?Continue with restore? (y/N): "
    if [[ "$confirm" != "y" && "$confirm" != "Y" ]]; then
        echo "‚ùå Restore cancelled by user"
        return 0
    fi

    # Create backup of current zshrc before restore
    echo ""
    echo "üíæ Creating backup of current zshrc..."
    backup_zshrc

    # Perform the restore
    echo ""
    echo "üîÑ Restoring zshrc from backup..."
    if cp "$backup_file" ~/.dotfiles/homedir/.zshrc; then
        echo "‚úÖ zshrc restored successfully to hidden file!"
        echo ""
        echo "üîÑ Reloading zshrc..."
        source ~/.zshrc
        echo "‚úÖ zshrc reloaded!"
        echo ""
        echo "üß™ Run 'test_spark_comprehensive' to verify functionality"
    else
        echo "‚ùå Failed to restore zshrc"
        return 1
    fi
}

function restore_zshrc_emergency {
    echo "üö® Emergency zshrc restore (no confirmation)"
    local backup_file=$(ls "$ZSHRC_BACKUPS"/zshrc_*.txt 2>/dev/null | sort -r | head -1)
    if [[ -n "$backup_file" ]]; then
        cp "$backup_file" ~/.dotfiles/homedir/.zshrc && source ~/.zshrc
        echo "‚úÖ Emergency restore completed from: $(basename "$backup_file")"
    else
        echo "‚ùå No emergency backup available"
        return 1
    fi
}
function zshreboot {
    source ~/.zshrc
}
function zshconfig {
    zed ~/.config/zsh/zshrc
}
# =====================================================
# DATABASE SETTINGS
# =====================================================
export PGHOST="localhost"
export PGUSER="dheerajchand"
export PGPASSWORD="dessert"
export PGPORT="5432"
export PGDATABASE="gis"
export GEODJANGO_TEMPLATE_SQL_DATABASE="geodjango_template_db"
export GEODJANGO_TEMPLATE_SQL_USER="dheerajchand"
export GEODJANGO_TEMPLATE_SQL_PASSWORD="dessert"
export GEODJANGO_TEMPLATE_SQL_PORT="5432"
# =====================================================
# DOCKER & GIS
# =====================================================
export PATH="/Users/dheerajchand/.rd/bin:$PATH"
export DEFAULT_DOCKER_CONTEXT="rancher-desktop"
# GIS things
export GDAL_LIBRARY_PATH="$(gdal-config --prefix)/lib/libgdal.dylib"
export GEOS_LIBRARY_PATH="$(geos-config --prefix)/lib/libgeos_c.dylib"
function update_local_repo {
    for remote in `git branch -r`; do git branch --track ${remote#origin/} $remote; done
}
export GIT_DISCOVERY_ACROSS_FILESYSTEM=1
# =====================================================
# INTERNET CONNECTIVITY CHECK
# =====================================================
function is_online {
    ping -c 1 google.com &> /dev/null && echo "online" || echo "offline"
}
# =====================================================
# JAVA SETUP
# =====================================================
export JAVA_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/java/current"
export PATH="$JAVA_HOME/bin:$PATH"
export LD_LIBRARY_PATH="$JAVA_HOME/lib:$LD_LIBRARY_PATH"
# Manual setup function (can be called when needed)
function setup_java_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Setting up Java 17 (optimal for Spark 3.5.3 + Hadoop 3.3.6)..."
        local target_java_version="17.0.12-tem"  # Known good version for Spark/Hadoop
        if ! sdk list java | grep -q "$target_java_version"; then
            echo "üì¶ Installing Java $target_java_version..."
            sdk install java $target_java_version
        fi
        sdk default java $target_java_version
        export JAVA_HOME=$(sdk home java $target_java_version)
        export PATH=$JAVA_HOME/bin:$PATH
        export LD_LIBRARY_PATH=$JAVA_HOME/lib:$LD_LIBRARY_PATH
        echo "‚úÖ Java version set to $target_java_version"
        echo "   Supports: Spark 3.5.3 + Hadoop 3.3.6 integration"
    else
        echo "‚ö†Ô∏è  Offline - using current Java installation"
    fi
}
# =====================================================
# HADOOP SETUP WITH SPARK INTEGRATION
# =====================================================
export HADOOP_CURRENT_VERSION="3.3.6"
export HADOOP_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/hadoop/current"
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH"
export HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop/"
export PATH="$HADOOP_HOME/bin:$PATH"
export PATH="$HADOOP_HOME/sbin:$PATH"
export HADOOP_LOCAL_JARS="$HADOOP_HOME/share/hadoop/common/lib/"
export HADOOP_DATA_DIR="$HOME/hadoop-data"
# YARN configuration (using HADOOP_CONF_DIR instead of deprecated YARN_CONF_DIR)
export MAPRED_CONF_DIR="$HADOOP_HOME/etc/hadoop"
# Function to initialize Hadoop data directories
function init_hadoop_dirs {
    mkdir -p "$HADOOP_DATA_DIR/hdfs/namenode"
    mkdir -p "$HADOOP_DATA_DIR/hdfs/datanode"
    echo "‚úÖ Created Hadoop data directories in $HADOOP_DATA_DIR"
}
# Hadoop + Spark Integration
export HADOOP_CLASSPATH="$HADOOP_HOME/share/hadoop/tools/lib/*:$HADOOP_CLASSPATH"
export SPARK_DIST_CLASSPATH=$(hadoop classpath 2>/dev/null || echo "")
# NEW: Java 17 Hadoop Compatibility Fix
function setup_java17_hadoop_compatibility {
    echo "üîß Applying Java 17 compatibility fixes for Hadoop/YARN..."
    # Add Java 17 module compatibility to hadoop-env.sh
    local hadoop_env="$HADOOP_CONF_DIR/hadoop-env.sh"
    # Check if already applied
    if grep -q "add-opens java.base/java.lang=ALL-UNNAMED" "$hadoop_env" 2>/dev/null; then
        echo "   ‚úÖ Java 17 compatibility already applied"
        return 0
    fi
    # Add JVM module access for Java 17+ compatibility
    cat >> "$hadoop_env" << 'EOF'
# Java 17+ module compatibility for Hadoop/YARN
export HADOOP_OPTS="$HADOOP_OPTS --add-opens java.base/java.lang=ALL-UNNAMED"
export HADOOP_OPTS="$HADOOP_OPTS --add-opens java.base/java.util=ALL-UNNAMED"
export HADOOP_OPTS="$HADOOP_OPTS --add-opens java.base/java.lang.reflect=ALL-UNNAMED"
export HADOOP_OPTS="$HADOOP_OPTS --add-opens java.base/java.net=ALL-UNNAMED"
export HADOOP_OPTS="$HADOOP_OPTS --add-opens java.base/java.util.concurrent=ALL-UNNAMED"
# Additional YARN-specific JVM options
export YARN_RESOURCEMANAGER_OPTS="$YARN_RESOURCEMANAGER_OPTS --add-opens java.base/java.lang=ALL-UNNAMED"
export YARN_NODEMANAGER_OPTS="$YARN_NODEMANAGER_OPTS --add-opens java.base/java.lang=ALL-UNNAMED"
# Hadoop environment (no YARN_ variables to avoid conflicts)
export HADOOP_LOG_DIR="$HADOOP_HOME/logs"
export HADOOP_PID_DIR="/tmp/hadoop-$USER"
# Use HADOOP_DATA_DIR from environment if set, otherwise set default
export HADOOP_DATA_DIR=${HADOOP_DATA_DIR:-$HOME/hadoop-data}
# Use JAVA_HOME from environment if set, otherwise set default
export JAVA_HOME=${JAVA_HOME:-/opt/homebrew/opt/sdkman-cli/libexec/candidates/java/current}
# Process users
export HDFS_NAMENODE_USER=$USER
export HDFS_DATANODE_USER=$USER
export HDFS_SECONDARYNAMENODE_USER=$USER
export YARN_RESOURCEMANAGER_USER=$USER
export YARN_NODEMANAGER_USER=$USER
EOF
    echo "   ‚úÖ Java 17 compatibility applied to hadoop-env.sh"
    echo "   üí° This fixes YARN startup issues with Java 17+"
}
# FIXED: Setup YARN Configuration Function
function setup_yarn_config {
    echo "üîß Setting up YARN configuration..."
    # Create yarn-site.xml
    local yarn_site="$HADOOP_CONF_DIR/yarn-site.xml"
    echo "üìù Creating yarn-site.xml..."
    cat > "$yarn_site" << 'EOF'
<?xml version="1.0"?>
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>localhost</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>localhost:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>localhost:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>localhost:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>localhost:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>localhost:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>
EOF
    # Create mapred-site.xml
    local mapred_site="$HADOOP_CONF_DIR/mapred-site.xml"
    echo "üìù Creating mapred-site.xml..."
    cat > "$mapred_site" << 'EOF'
<?xml version="1.0"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>localhost:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>localhost:19888</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
</configuration>
EOF
    # Update workers file
    echo "üìù Updating workers file..."
    echo "localhost" > "$HADOOP_CONF_DIR/workers"
    # Check/create masters file if it exists
    if [[ -f "$HADOOP_CONF_DIR/masters" ]]; then
        echo "localhost" > "$HADOOP_CONF_DIR/masters"
    fi
    echo "‚úÖ YARN configuration completed!"
    echo "üìä Configuration files created:"
    echo "   ‚Ä¢ yarn-site.xml"
    echo "   ‚Ä¢ mapred-site.xml"
    echo "   ‚Ä¢ workers file updated"
    echo ""
    echo "üí° Test YARN with: start_hadoop"
}
# NEW: Download Hadoop SLF4J JARs function
function download_hadoop_slf4j_jars {
    local target_path="${1:-$HADOOP_LOCAL_JARS}"
    echo "üöÄ Downloading SLF4J JARs for Hadoop..."
    download_maven_jars "org.slf4j:slf4j-log4j12:1.7.30,log4j:log4j:1.2.17" "$target_path"
}
# NEW: YARN Management Functions
function yarn_application_list {
    echo "üìä YARN Applications:"
    yarn application -list -appStates ALL 2>/dev/null || echo "‚ùå YARN not accessible"
}
function yarn_kill_all_apps {
    echo "üõë Killing all YARN applications..."
    yarn application -list -appStates RUNNING,SUBMITTED,ACCEPTED | grep application_ | awk '{print $1}' | while read app; do
        echo "   Killing: $app"
        yarn application -kill "$app"
    done
}
function yarn_logs {
    local app_id="$1"
    if [[ -z "$app_id" ]]; then
        echo "Usage: yarn_logs <application_id>"
        echo "üí° Get application IDs with: yarn_application_list"
        return 1
    fi
    echo "üìú YARN Application Logs for: $app_id"
    yarn logs -applicationId "$app_id"
}
function yarn_cluster_info {
    echo "üèóÔ∏è YARN Cluster Information:"
    echo ""
    echo "=== Resource Manager ==="
    yarn rmadmin -getServiceState rm1 2>/dev/null || echo "ResourceManager status: Unknown"
    echo -e "\n=== Node Status ==="
    yarn node -list -all 2>/dev/null || echo "No node information available"
    echo -e "\n=== Queue Information ==="
    yarn queue -status default 2>/dev/null || echo "No queue information available"
    echo -e "\n=== Cluster Metrics ==="
    curl -s http://localhost:8088/ws/v1/cluster/info 2>/dev/null | \
        python3 -c "import json,sys; data=json.load(sys.stdin); print(f'Cluster ID: {data[\"clusterInfo\"][\"id\"]}\nHadoop Version: {data[\"clusterInfo\"][\"hadoopVersion\"]}\nResource Manager: {data[\"clusterInfo\"][\"resourceManagerVersion\"]}')" 2>/dev/null || \
        echo "Cluster metrics unavailable"
}
# Manual setup function (can be called when needed)
function setup_hadoop_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Setting up Hadoop 3.3.6 (pinned known-good version)..."
        local target_hadoop_version="3.3.6"
        if ! sdk list hadoop | grep -q "$target_hadoop_version"; then
            echo "üì¶ Installing Hadoop $target_hadoop_version..."
            sdk install hadoop $target_hadoop_version
        fi
        sdk default hadoop $target_hadoop_version
        export HADOOP_CURRENT_VERSION="$target_hadoop_version"
        export HADOOP_HOME=$(sdk home hadoop $target_hadoop_version)
        export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
        export LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH"
        export HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop/"
        export PATH="$HADOOP_HOME/bin:$PATH"
        # Update Spark integration
        export HADOOP_CLASSPATH="$HADOOP_HOME/share/hadoop/tools/lib/*:$HADOOP_CLASSPATH"
        export SPARK_DIST_CLASSPATH=$(hadoop classpath 2>/dev/null || echo "")
        echo "‚úÖ Hadoop version pinned to $target_hadoop_version"
        echo "   Compatible with: Java 17, Spark 3.5.3"
        echo "   Spark integration: Configured"
    else
        echo "‚ö†Ô∏è  Offline - using current Hadoop installation"
    fi
}
# FIXED: setup_hadoop_spark_integration function
function setup_hadoop_spark_integration {
    echo "üîó Configuring Hadoop + Spark integration..."
    # Ensure Hadoop is available
    if ! command -v hadoop &>/dev/null; then
        echo "‚ùå Hadoop not found. Installing..."
        setup_hadoop_version
    fi
    # Configure core-site.xml for local HDFS
    local core_site="$HADOOP_CONF_DIR/core-site.xml"
    if [[ ! -f "$core_site" ]] || ! grep -q "fs.defaultFS" "$core_site"; then
        echo "üìù Configuring Hadoop core-site.xml..."
        cat > "$core_site" << 'EOF'
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>${HADOOP_DATA_DIR}/tmp</value>
    </property>
</configuration>
EOF
        echo "‚úÖ core-site.xml configured"
    fi
    # Configure hdfs-site.xml
    local hdfs_site="$HADOOP_CONF_DIR/hdfs-site.xml"
    if [[ ! -f "$hdfs_site" ]] || ! grep -q "dfs.replication" "$hdfs_site"; then
        echo "üìù Configuring Hadoop hdfs-site.xml..."
        cat > "$hdfs_site" << 'EOF'
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>${HADOOP_DATA_DIR}/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>${HADOOP_DATA_DIR}/hdfs/datanode</value>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>localhost:9870</value>
    </property>
    <property>
        <name>dfs.datanode.http.address</name>
        <value>localhost:9864</value>
    </property>
</configuration>
EOF
        echo "‚úÖ hdfs-site.xml configured"
    fi
    # Setup YARN configuration
    setup_yarn_config
    # Apply Java 17 compatibility fixes
    setup_java17_hadoop_compatibility
    # Update Spark integration environment
    export HADOOP_CLASSPATH="$HADOOP_HOME/share/hadoop/tools/lib/*:$HADOOP_CLASSPATH"
    export SPARK_DIST_CLASSPATH=$(hadoop classpath 2>/dev/null || echo "")
    echo "‚úÖ Hadoop + Spark integration configured"
    echo "üí° Test with: test_hadoop_spark_integration"
}
# UPDATED: Enhanced start_hadoop function with YARN
function start_hadoop {
    echo "üöÄ Starting Hadoop services..."
    # Initialize directories if they don't exist
    if [[ ! -d "$HADOOP_DATA_DIR/hdfs/namenode" ]]; then
        echo "üìÅ Initializing Hadoop directories..."
        init_hadoop_dirs
        # Format namenode if needed
        echo "üîß Formatting namenode..."
        hdfs namenode -format -force
    fi
    # Start HDFS
    echo "üîÑ Starting HDFS..."
    start-dfs.sh
    # Start YARN
    echo "üîÑ Starting YARN..."
    start-yarn.sh
    # Wait for services to start
    sleep 5
    # Status check
    echo "üìä Hadoop Services Status:"
    echo "   NameNode: $(jps | grep -q NameNode && echo "‚úÖ Running" || echo "‚ùå Stopped")"
    echo "   DataNode: $(jps | grep -q DataNode && echo "‚úÖ Running" || echo "‚ùå Stopped")"
    echo "   SecondaryNameNode: $(jps | grep -q SecondaryNameNode && echo "‚úÖ Running" || echo "‚ùå Stopped")"
    echo "   ResourceManager: $(jps | grep -q ResourceManager && echo "‚úÖ Running" || echo "‚ùå Stopped")"
    echo "   NodeManager: $(jps | grep -q NodeManager && echo "‚úÖ Running" || echo "‚ùå Stopped")"
    echo "üåê Web UIs:"
    echo "   HDFS NameNode: http://localhost:9870"
    echo "   YARN ResourceManager: http://localhost:8088"
    echo "   HDFS DataNode: http://localhost:9864"
}
function stop_hadoop {
    echo "üõë Stopping Hadoop services..."
    # Stop YARN first
    echo "üîÑ Stopping YARN..."
    stop-yarn.sh 2>/dev/null
    # Stop HDFS
    echo "üîÑ Stopping HDFS..."
    stop-dfs.sh 2>/dev/null
    echo "‚úÖ Hadoop services stopped"
}
function restart_hadoop {
    echo "üîÑ Restarting Hadoop services..."
    stop_hadoop
    sleep 3
    start_hadoop
}
function hadoop_status {
    echo "üìä Hadoop Services Status:"
    # Check Java processes
    local namenode_count=$(jps | grep NameNode | wc -l)
    local datanode_count=$(jps | grep DataNode | wc -l)
    local secondary_count=$(jps | grep SecondaryNameNode | wc -l)
    local resourcemanager_count=$(jps | grep ResourceManager | wc -l)
    local nodemanager_count=$(jps | grep NodeManager | wc -l)
    echo "   NameNode: $([ $namenode_count -gt 0 ] && echo '‚úÖ Running' || echo '‚ùå Stopped')"
    echo "   DataNode: $([ $datanode_count -gt 0 ] && echo '‚úÖ Running' || echo '‚ùå Stopped')"
    echo "   SecondaryNameNode: $([ $secondary_count -gt 0 ] && echo '‚úÖ Running' || echo '‚ùå Stopped')"
    echo "   ResourceManager: $([ $resourcemanager_count -gt 0 ] && echo '‚úÖ Running' || echo '‚ùå Stopped')"
    echo "   NodeManager: $([ $nodemanager_count -gt 0 ] && echo '‚úÖ Running' || echo '‚ùå Stopped')"
    # Check HDFS health if running
    if [[ $namenode_count -gt 0 ]]; then
        echo ""
        echo "üìÅ HDFS Status:"
        hdfs dfsadmin -report 2>/dev/null | head -10 || echo "   Unable to get HDFS report"
        echo ""
        echo "üåê Web UIs:"
        echo "   HDFS NameNode: http://localhost:9870"
        echo "   YARN ResourceManager: http://localhost:8088"
        echo "   HDFS DataNode: http://localhost:9864"
    fi
}
function test_hadoop_integration {
    echo "üß™ Testing Hadoop integration..."
    # Check if Hadoop is running
    if ! jps | grep -q "NameNode"; then
        echo "‚ö†Ô∏è  Hadoop not running. Starting..."
        start_hadoop
        sleep 5
    fi
    # Simple HDFS test
    echo "üîç Testing HDFS operations..."
    # Create test directory
    hdfs dfs -mkdir -p /test 2>/dev/null || echo "   Test directory exists"
    # Create test file
    echo "Hello Hadoop!" > /tmp/test_hadoop.txt
    hdfs dfs -put /tmp/test_hadoop.txt /test/ 2>/dev/null || echo "   Test file upload failed"
    # Read back test file
    local content=$(hdfs dfs -cat /test/test_hadoop.txt 2>/dev/null)
    if [[ "$content" == "Hello Hadoop!" ]]; then
        echo "   ‚úÖ HDFS read/write test passed"
    else
        echo "   ‚ùå HDFS test failed"
    fi
    # Clean up
    rm -f /tmp/test_hadoop.txt
    hdfs dfs -rm -f /test/test_hadoop.txt 2>/dev/null
    echo "‚úÖ Hadoop integration test completed"
}
function test_hadoop_spark_integration {
    echo "üß™ Testing Hadoop + Spark integration..."
    # Check if Hadoop is running
    if ! jps | grep -q "NameNode"; then
        echo "‚ö†Ô∏è  Hadoop not running. Starting..."
        start_hadoop
        sleep 5
    fi
    # Create test script
    local test_script="/tmp/hadoop_spark_test.py"
    cat > "$test_script" << 'EOF'
from pyspark.sql import SparkSession
import sys
try:
    print("üöÄ Testing Hadoop + Spark integration...")
    # Create Spark session with Hadoop configuration
    spark = SparkSession.builder \
        .appName("HadoopSparkIntegrationTest") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    print(f"‚úÖ Spark session created")
    print(f"   Master: {spark.sparkContext.master}")
    print(f"   Hadoop configuration available: {'Yes' if spark.sparkContext._jsc.hadoopConfiguration() else 'No'}")
    # Test 1: Create test data
    print("\nüîç Test 1: Creating test DataFrame...")
    test_data = [(1, "Alice", 25), (2, "Bob", 30), (3, "Charlie", 35)]
    df = spark.createDataFrame(test_data, ["id", "name", "age"])
    print(f"   ‚úÖ DataFrame created with {df.count()} rows")
    # Test 2: Write to HDFS
    print("\nüîç Test 2: Writing to HDFS...")
    hdfs_path = "hdfs://localhost:9000/test/spark_hadoop_test"
    try:
        df.write.mode("overwrite").parquet(hdfs_path)
        print(f"   ‚úÖ Data written to HDFS: {hdfs_path}")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  HDFS write failed: {e}")
        print("   Trying local filesystem...")
        local_path = "/tmp/spark_hadoop_test"
        df.write.mode("overwrite").parquet(local_path)
        print(f"   ‚úÖ Data written to local filesystem: {local_path}")
        hdfs_path = local_path
    # Test 3: Read back from storage
    print("\nüîç Test 3: Reading back from storage...")
    read_df = spark.read.parquet(hdfs_path)
    read_count = read_df.count()
    print(f"   ‚úÖ Data read back: {read_count} rows")
    # Test 4: Basic processing
    print("\nüîç Test 4: Data processing...")
    result = read_df.filter(read_df.age > 28).count()
    print(f"   ‚úÖ Filtered data: {result} rows with age > 28")
    # Test 5: Check Hadoop classpath integration
    print("\nüîç Test 5: Hadoop classpath integration...")
    sc = spark.sparkContext
    hadoop_conf = sc._jsc.hadoopConfiguration()
    fs_default = hadoop_conf.get("fs.defaultFS", "Not configured")
    print(f"   ‚úÖ Hadoop default filesystem: {fs_default}")
    print("\nüéâ Hadoop + Spark integration test completed successfully!")
except Exception as e:
    print(f"\n‚ùå Integration test failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
finally:
    if 'spark' in locals():
        spark.stop()
        print("üõë Spark session stopped")
EOF
    echo "Running Hadoop + Spark integration test..."
    if [[ -n "$SPARK_MASTER_URL" ]]; then
        distributed_spark_submit "$test_script"
    else
        default_spark_submit "$test_script"
    fi
    rm -f "$test_script"
}
function hadoop_spark_demo {
    echo "üé¨ Hadoop + Spark Demo: Processing large dataset..."
    # Ensure services are running
    if ! jps | grep -q "NameNode"; then
        echo "Starting Hadoop..."
        start_hadoop
        sleep 5
    fi
    local demo_script="/tmp/hadoop_spark_demo.py"
    cat > "$demo_script" << 'EOF'
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import random
print("üé¨ Hadoop + Spark Demo: Large Dataset Processing")
# Create Spark session
spark = SparkSession.builder \
    .appName("HadoopSparkDemo") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()
print("‚úÖ Spark session created with Hadoop integration")
# Generate larger test dataset
print("\nüìä Generating test dataset (100K records)...")
data = []
cities = ["New York", "Los Angeles", "Chicago", "Houston", "Phoenix", "Philadelphia", "San Antonio", "San Diego", "Dallas", "San Jose"]
for i in range(100000):
    data.append((
        i,
        f"User_{i}",
        random.randint(18, 80),
        random.choice(cities),
        random.uniform(30000, 150000)
    ))
df = spark.createDataFrame(data, ["id", "name", "age", "city", "salary"])
print(f"‚úÖ Generated dataset with {df.count():,} records")
# Write to distributed storage
print("\nüíæ Writing to distributed storage...")
storage_path = "hdfs://localhost:9000/demo/user_data" if "hdfs" in spark.sparkContext._jsc.hadoopConfiguration().get("fs.defaultFS", "") else "/tmp/demo/user_data"
df.write.mode("overwrite").parquet(storage_path)
print(f"‚úÖ Data written to: {storage_path}")
# Read back and perform analytics
print("\nüìà Performing distributed analytics...")
read_df = spark.read.parquet(storage_path)
# Analytics queries
print("   Computing statistics...")
avg_age = read_df.agg(avg("age")).collect()[0][0]
avg_salary = read_df.agg(avg("salary")).collect()[0][0]
total_records = read_df.count()
print(f"   üìä Total Records: {total_records:,}")
print(f"   üìä Average Age: {avg_age:.1f}")
print(f"   üìä Average Salary: ${avg_salary:,.2f}")
# City analysis
print("\nüèôÔ∏è  City-wise analysis...")
city_stats = read_df.groupBy("city") \
    .agg(count("*").alias("population"),
         avg("age").alias("avg_age"),
         avg("salary").alias("avg_salary")) \
    .orderBy(desc("population"))
city_stats.show(10, truncate=False)
# High earners analysis
print("\nüí∞ High earners analysis (>$100K)...")
high_earners = read_df.filter(col("salary") > 100000)
high_earner_count = high_earners.count()
percentage = (high_earner_count / total_records) * 100
print(f"   üíº High earners: {high_earner_count:,} ({percentage:.1f}%)")
print("\nüéâ Demo completed! Hadoop + Spark successfully processed large dataset.")
spark.stop()
EOF
    echo "üöÄ Running demo..."
    default_spark_submit "$demo_script"
    rm -f "$demo_script"
    echo ""
    echo "üéØ Demo Summary:"
    echo "   ‚úÖ Generated 100K records"
    echo "   ‚úÖ Wrote to distributed storage (HDFS or local)"
    echo "   ‚úÖ Performed distributed analytics"
    echo "   ‚úÖ City-wise and salary analysis"
    echo ""
    echo "üí° This demonstrates Spark's ability to work with Hadoop for:"
    echo "   ‚Ä¢ Distributed storage (HDFS)"
    echo "   ‚Ä¢ Large dataset processing"
    echo "   ‚Ä¢ Complex analytics queries"
    echo "   ‚Ä¢ Fault-tolerant computation"
}
# =====================================================
# MAVEN SETUP
# =====================================================
DEFAULT_MAVEN_VERSION="3.9.6"
function setup_maven() {
    if command -v mvn &>/dev/null; then
        echo "Maven is installed: $(mvn -version | head -n 1)"
    else
        echo "Maven not found. Installing Maven $DEFAULT_MAVEN_VERSION via SDKMAN..."
        sdk install maven "$DEFAULT_MAVEN_VERSION" || {
            echo "Failed to install Maven. Please check SDKMAN setup."
            return 1
        }
        if command -v mvn &>/dev/null; then
            echo "Maven installation successful: $(mvn -version | head -n 1)"
        else
            echo "Maven installation failed."
        fi
    fi
}
function download_maven_jars {
    local libraries="${1:-$DEFAULT_SPARK_JARS}"
    local target_path="${2:-$LOCAL_SPARK_JAR_PATH}"
    mkdir -p "$target_path"
    echo "üöÄ Downloading Maven dependencies..."
    for lib in $(echo "$libraries" | tr ',' ' '); do
        IFS=':' read -r group artifact version <<< "$lib"
        jar_file="${artifact}-${version}.jar"
        mvn_url="https://repo1.maven.org/maven2/$(echo $group | tr '.' '/')/$artifact/$version/$jar_file"
        echo "üîç Fetching: $jar_file"
        curl -sL -o "$target_path/$jar_file" "$mvn_url" && echo "‚úÖ Saved to $target_path/$jar_file" || echo "‚ùå Failed to download $jar_file"
    done
}
# =====================================================
# AUTO-SETUP SYSTEM
# =====================================================
# Control flags
export AUTO_SETUP_ON_STARTUP="${AUTO_SETUP_ON_STARTUP:-false}"
export AUTO_SETUP_CHECK_ONLINE="${AUTO_SETUP_CHECK_ONLINE:-true}"
export AUTO_SETUP_VERBOSE="${AUTO_SETUP_VERBOSE:-false}"
function setup_scala_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Setting up dual Scala versions (2.12.x for Spark + 3.x for modern development)..."
        # Scala 2.12.18 - Required for Spark 3.5.3
        local scala_2_version="2.12.18"
        if ! sdk list scala | grep -q "$scala_2_version"; then
            echo "üì¶ Installing Scala $scala_2_version (Spark compatibility)..."
            sdk install scala $scala_2_version
        fi
        # Scala 3.3.4 - Latest stable Scala 3
        local scala_3_version="3.3.4"
        if ! sdk list scala | grep -q "$scala_3_version"; then
            echo "üì¶ Installing Scala $scala_3_version (modern Scala)..."
            sdk install scala $scala_3_version
        fi
        # Default to Scala 2.12 for Spark compatibility
        sdk default scala $scala_2_version
        echo "‚úÖ Scala versions installed:"
        echo "   Default: $scala_2_version (Spark 3.5.3 compatible)"
        echo "   Available: $scala_3_version (use: sdk use scala $scala_3_version)"
        echo "üí° Switch versions: sdk use scala [version]"
    else
        echo "‚ö†Ô∏è  Offline - using current Scala installation"
    fi
}
function setup_spark_version {
    if [[ "$(is_online)" == "online" ]]; then
        echo "üîç Setting up Spark 3.5.3 (pinned known-good version)..."
        local target_spark_version="3.5.3"
        if ! sdk list spark | grep -q "$target_spark_version"; then
            echo "üì¶ Installing Spark $target_spark_version..."
            sdk install spark $target_spark_version
        fi
        sdk default spark $target_spark_version
        export SPARK_HOME=$(sdk home spark $target_spark_version)
        export PATH="$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"
        echo "‚úÖ Spark version pinned to $target_spark_version"
        echo "   Compatible with: Java 17, Scala 2.12.18, Hadoop 3.3.6"
    else
        echo "‚ö†Ô∏è  Offline - using current Spark installation"
    fi
}
function auto_setup_environment {
    local start_time=$(date +%s)
    echo "üöÄ Auto-setting up development environment..."
    # Quick connectivity check first
    if [[ "$AUTO_SETUP_CHECK_ONLINE" == "true" ]]; then
        local online_status=$(is_online)
        if [[ "$online_status" == "offline" ]]; then
            echo "‚ö†Ô∏è  Offline mode - skipping version updates"
            return 0
        fi
    fi
    # Run setup functions
    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Java..."
    setup_java_version 2>/dev/null || echo "‚ö†Ô∏è  Java setup skipped"
    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Scala..."
    setup_scala_version 2>/dev/null || echo "‚ö†Ô∏è  Scala setup skipped"
    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Hadoop..."
    setup_hadoop_version 2>/dev/null || echo "‚ö†Ô∏è  Hadoop setup skipped"
    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Spark..."
    setup_spark_version 2>/dev/null || echo "‚ö†Ô∏è  Spark setup skipped"
    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîß Setting up Maven..."
    setup_maven 2>/dev/null || echo "‚ö†Ô∏è  Maven setup skipped"
    [[ "$AUTO_SETUP_VERBOSE" == "true" ]] && echo "üîó Configuring Hadoop+Spark integration..."
    setup_hadoop_spark_integration 2>/dev/null || echo "‚ö†Ô∏è  Hadoop+Spark integration skipped"
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    echo "‚úÖ Environment auto-setup completed in ${duration}s"
    echo "üí° Test integration with: test_hadoop_spark_integration"
}
function enable_auto_setup {
    export AUTO_SETUP_ON_STARTUP="true"
    echo "‚úÖ Auto-setup enabled for future shell sessions"
    echo "üí° Run 'disable_auto_setup' to turn off"
    echo "üí° Run 'auto_setup_environment' to run it now"
}
function disable_auto_setup {
    export AUTO_SETUP_ON_STARTUP="false"
    echo "‚úÖ Auto-setup disabled"
    echo "üí° Run 'enable_auto_setup' to turn back on"
    echo "üí° You can still run 'auto_setup_environment' manually"
}
function show_version_strategy {
    echo "üìå Pinned Known-Good Version Strategy:"
    echo ""
    echo "üîß Target Versions (tested compatibility):"
    echo "   Java:    17.0.12-tem  (LTS with Spark/Hadoop support)"
    echo "   Scala:   2.12.18      (Spark 3.5.3 compatible) + 3.3.4 (modern)"
    echo "   Spark:   3.5.3        (your current working version)"
    echo "   Hadoop:  3.3.6        (stable with Spark 3.5.3)"
    echo "   Maven:   3.9.6        (latest stable)"
    echo ""
    echo "‚úÖ Compatibility Matrix:"
    echo "   Java 17 + Spark 3.5.3 + Hadoop 3.3.6 = ‚úÖ Tested & Working"
    echo "   Scala 2.12.18 required for Spark compatibility"
    echo "   Scala 3.3.4 available for modern Scala development"
    echo ""
    echo "üí° Version Management:"
    echo "   ‚Ä¢ Pinned versions prevent unexpected breaks"
    echo "   ‚Ä¢ All versions tested together as a stack"
    echo "   ‚Ä¢ Scala dual-version setup for flexibility"
    echo "   ‚Ä¢ Switch Scala: sdk use scala [2.12.18|3.3.4]"
    echo ""
    echo "üîÑ To modify pinned versions, edit the setup functions in your zshrc"
}
function verify_version_compatibility {
    echo "üîç Verifying installed version compatibility..."
    echo ""
    local java_version=$(java -version 2>&1 | head -1 | grep -o '"[^"]*"' | tr -d '"' || echo "Not found")
    local scala_version=$(scala -version 2>&1 | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+' | head -1 || echo "Not found")
    local spark_version=$(spark-submit --version 2>&1 | grep -o 'version [0-9]\+\.[0-9]\+\.[0-9]\+' | cut -d' ' -f2 || echo "Not found")
    local hadoop_version=$(hadoop version 2>/dev/null | head -1 | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+' || echo "Not found")
    echo "üìä Installed Versions:"
    echo "   Java:    $java_version"
    echo "   Scala:   $scala_version"
    echo "   Spark:   $spark_version"
    echo "   Hadoop:  $hadoop_version"
    echo ""
    # Check compatibility
    local java_ok="‚ùå"
    local scala_ok="‚ùå"
    local spark_ok="‚ùå"
    local hadoop_ok="‚ùå"
    [[ "$java_version" =~ ^17\. ]] && java_ok="‚úÖ"
    [[ "$scala_version" =~ ^2\.12\. ]] && scala_ok="‚úÖ"
    [[ "$spark_version" == "3.5.3" ]] && spark_ok="‚úÖ"
    [[ "$hadoop_version" =~ ^3\.3\. ]] && hadoop_ok="‚úÖ"
    echo "üéØ Compatibility Check:"
    echo "   Java 17.x:     $java_ok"
    echo "   Scala 2.12.x:  $scala_ok"
    echo "   Spark 3.5.3:   $spark_ok"
    echo "   Hadoop 3.3.x:  $hadoop_ok"
    echo ""
    if [[ "$java_ok$scala_ok$spark_ok$hadoop_ok" == "‚úÖ‚úÖ‚úÖ‚úÖ" ]]; then
        echo "üéâ All versions compatible! Your stack should work perfectly."
    else
        echo "‚ö†Ô∏è  Some versions may need adjustment. Run 'auto_setup_environment' to fix."
    fi
}
function setup_environment_status {
    echo "üîç Environment Setup Status:"
    echo "   Auto-setup on startup: $AUTO_SETUP_ON_STARTUP"
    echo "   Check online: $AUTO_SETUP_CHECK_ONLINE"
    echo "   Verbose mode: $AUTO_SETUP_VERBOSE"
    echo ""
    echo "Current versions:"
    echo "   Java: $(java -version 2>&1 | head -1 || echo 'Not found')"
    echo "   Scala: $(scala -version 2>&1 | head -1 || echo 'Not found')"
    echo "   Spark: $(spark-submit --version 2>&1 | head -1 || echo 'Not found')"
    echo "   Hadoop: $(hadoop version 2>/dev/null | head -1 || echo 'Not found')"
    echo "   Maven: $(mvn -version 2>/dev/null | head -1 || echo 'Not found')"
    echo ""
    echo "üí° Controls:"
    echo "   enable_auto_setup         - Enable auto-setup on shell startup"
    echo "   disable_auto_setup        - Disable auto-setup"
    echo "   auto_setup_environment    - Run setup manually"
    echo "   show_version_strategy     - Show pinned version strategy"
    echo "   verify_version_compatibility - Check version compatibility"
}
# =====================================================
# FUNCTION BACKUP SYSTEM
# =====================================================
function backup_critical_functions {
    echo "üíæ Creating backup of critical functions..."
    local backup_dir="$HOME/.zsh_function_backups"
    mkdir -p "$backup_dir"
    # Backup the working test function
    cat > "$backup_dir/test_spark_comprehensive_backup.sh" << 'EOF'
function test_spark_comprehensive {
    echo "üß™ Comprehensive Spark functionality test (Sedona + GraphFrames)..."
    echo "üî• RUNNING UPDATED VERSION FROM ARTIFACT - BANANA HAMMOCK! üî•"
    local test_script="/tmp/spark_comprehensive_test.py"
    cat > "$test_script" << 'PYTHON_EOF'
from pyspark.sql import SparkSession
print("üöÄ Starting comprehensive Spark test...")
spark = SparkSession.builder \
    .appName("ComprehensiveSparkTest") \
    .config("spark.sql.extensions", "org.apache.sedona.sql.SedonaSqlExtensions") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator") \
    .getOrCreate()
print("‚úÖ Spark Context created successfully")
print(f"   Master: {spark.sparkContext.master}")
print(f"   Default Parallelism: {spark.sparkContext.defaultParallelism}")
print("\nüîç Test 1: Basic RDD operations...")
rdd = spark.sparkContext.parallelize(range(100), 4)
result = rdd.map(lambda x: x * x).sum()
print(f"   ‚úÖ RDD computation result: {result}")
print("\nüîç Test 2: DataFrame operations...")
df = spark.range(100)
count = df.count()
print(f"   ‚úÖ DataFrame count: {count}")
print("\nüîç Test 3: SQL operations...")
df.createOrReplaceTempView("test_table")
sql_result = spark.sql("SELECT COUNT(*) as count FROM test_table").collect()[0]["count"]
print(f"   ‚úÖ SQL result: {sql_result}")
print("\nüîç Test 4: Sedona functionality...")
sedona_works = False
try:
    # Modern Sedona 1.7.1+ initialization
    from sedona.spark import SedonaContext
    sedona = SedonaContext.create(spark)
    print("   ‚úÖ Sedona context created (modern method)")
    # Test basic spatial function
    point_result = sedona.sql("SELECT ST_Point(1.0, 2.0) as point").collect()
    print("   ‚úÖ Sedona ST_Point works")
    # Test distance calculation
    distance_result = sedona.sql("""
        SELECT ST_Distance(
            ST_Point(-0.1275, 51.5072),
            ST_Point(-74.0060, 40.7128)
        ) as distance_degrees
    """).collect()[0]["distance_degrees"]
    print(f"   ‚úÖ Sedona distance calculation: {distance_result:.4f} degrees")
    # Test spatial operations with DataFrame
    spatial_df = sedona.createDataFrame([
        ("London", -0.1275, 51.5072),
        ("NYC", -74.0060, 40.7128),
    ], ["city", "longitude", "latitude"])
    spatial_with_geom = spatial_df.selectExpr(
        "city",
        "ST_Point(longitude, latitude) as geom"
    )
    geom_count = spatial_with_geom.count()
    print(f"   ‚úÖ Sedona DataFrame operations: {geom_count} geometries created")
    sedona_works = True
except Exception as e:
    print(f"   ‚ö†Ô∏è  Sedona failed: {e}")
print("\nüîç Test 5: GraphFrames functionality...")
graphframes_works = False
try:
    from graphframes import GraphFrame
    vertices = spark.createDataFrame([("A", "Node A"), ("B", "Node B"), ("C", "Node C")], ["id", "name"])
    edges = spark.createDataFrame([("A", "B", "edge1"), ("B", "C", "edge2")], ["src", "dst", "relationship"])
    g = GraphFrame(vertices, edges)
    v_count = g.vertices.count()
    e_count = g.edges.count()
    print(f"   ‚úÖ GraphFrame created with {v_count} vertices, {e_count} edges")
    # Test PageRank
    pagerank_result = g.pageRank(resetProbability=0.01, maxIter=2)
    pr_vertices = pagerank_result.vertices.count()
    print(f"   ‚úÖ PageRank completed: {pr_vertices} vertices processed")
    graphframes_works = True
except Exception as e:
    print(f"   ‚ö†Ô∏è  GraphFrames failed: {e}")
print("\nüéâ Summary:")
print("   ‚úÖ Core Spark: Working")
print(f"   {'‚úÖ' if sedona_works else '‚ö†Ô∏è '} Sedona: {'Working (Modern)' if sedona_works else 'Failed'}")
print(f"   {'‚úÖ' if graphframes_works else '‚ö†Ô∏è '} GraphFrames: {'Working' if graphframes_works else 'Failed'}")
spark.stop()
print("üõë Test completed")
PYTHON_EOF
    echo "Running test..."
    if [[ -n "$SPARK_MASTER_URL" ]]; then
        distributed_spark_submit "$test_script"
    else
        default_spark_submit "$test_script"
    fi
    rm -f "$test_script"
}
EOF
    echo "‚úÖ Critical functions backed up to: $backup_dir"
    echo "üí° Use 'restore_critical_functions' if they get lost"
}
function restore_critical_functions {
    echo "üîÑ Restoring critical functions from backup..."
    local backup_dir="$HOME/.zsh_function_backups"
    if [[ -f "$backup_dir/test_spark_comprehensive_backup.sh" ]]; then
        source "$backup_dir/test_spark_comprehensive_backup.sh"
        echo "‚úÖ test_spark_comprehensive function restored!"
        echo "üß™ Test it: test_spark_comprehensive"
    else
        echo "‚ùå No backup found. Run 'backup_critical_functions' first."
    fi
}
function emergency_restore_test_function {
    echo "üö® Emergency restore of test_spark_comprehensive function..."
    function test_spark_comprehensive {
        echo "üß™ Comprehensive Spark functionality test (Sedona + GraphFrames)..."
        echo "üî• RUNNING UPDATED VERSION FROM ARTIFACT - BANANA HAMMOCK! üî•"
        local test_script="/tmp/spark_comprehensive_test.py"
        cat > "$test_script" << 'EOF'
from pyspark.sql import SparkSession
print("üöÄ Starting comprehensive Spark test...")
spark = SparkSession.builder \
    .appName("ComprehensiveSparkTest") \
    .config("spark.sql.extensions", "org.apache.sedona.sql.SedonaSqlExtensions") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator") \
    .getOrCreate()
print("‚úÖ Spark Context created successfully")
print(f"   Master: {spark.sparkContext.master}")
print(f"   Default Parallelism: {spark.sparkContext.defaultParallelism}")
print("\nüîç Test 1: Basic RDD operations...")
rdd = spark.sparkContext.parallelize(range(100), 4)
result = rdd.map(lambda x: x * x).sum()
print(f"   ‚úÖ RDD computation result: {result}")
print("\nüîç Test 2: DataFrame operations...")
df = spark.range(100)
count = df.count()
print(f"   ‚úÖ DataFrame count: {count}")
print("\nüîç Test 3: SQL operations...")
df.createOrReplaceTempView("test_table")
sql_result = spark.sql("SELECT COUNT(*) as count FROM test_table").collect()[0]["count"]
print(f"   ‚úÖ SQL result: {sql_result}")
print("\nüîç Test 4: Sedona functionality...")
sedona_works = False
try:
    # Modern Sedona 1.7.1+ initialization
    from sedona.spark import SedonaContext
    sedona = SedonaContext.create(spark)
    print("   ‚úÖ Sedona context created (modern method)")
    # Test basic spatial function
    point_result = sedona.sql("SELECT ST_Point(1.0, 2.0) as point").collect()
    print("   ‚úÖ Sedona ST_Point works")
    # Test distance calculation
    distance_result = sedona.sql("""
        SELECT ST_Distance(
            ST_Point(-0.1275, 51.5072),
            ST_Point(-74.0060, 40.7128)
        ) as distance_degrees
    """).collect()[0]["distance_degrees"]
    print(f"   ‚úÖ Sedona distance calculation: {distance_result:.4f} degrees")
    # Test spatial operations with DataFrame
    spatial_df = sedona.createDataFrame([
        ("London", -0.1275, 51.5072),
        ("NYC", -74.0060, 40.7128),
    ], ["city", "longitude", "latitude"])
    spatial_with_geom = spatial_df.selectExpr(
        "city",
        "ST_Point(longitude, latitude) as geom"
    )
    geom_count = spatial_with_geom.count()
    print(f"   ‚úÖ Sedona DataFrame operations: {geom_count} geometries created")
    sedona_works = True
except Exception as e:
    print(f"   ‚ö†Ô∏è  Sedona failed: {e}")
print("\nüîç Test 5: GraphFrames functionality...")
graphframes_works = False
try:
    from graphframes import GraphFrame
    vertices = spark.createDataFrame([("A", "Node A"), ("B", "Node B"), ("C", "Node C")], ["id", "name"])
    edges = spark.createDataFrame([("A", "B", "edge1"), ("B", "C", "edge2")], ["src", "dst", "relationship"])
    g = GraphFrame(vertices, edges)
    v_count = g.vertices.count()
    e_count = g.edges.count()
    print(f"   ‚úÖ GraphFrame created with {v_count} vertices, {e_count} edges")
    # Test PageRank
    pagerank_result = g.pageRank(resetProbability=0.01, maxIter=2)
    pr_vertices = pagerank_result.vertices.count()
    print(f"   ‚úÖ PageRank completed: {pr_vertices} vertices processed")
    graphframes_works = True
except Exception as e:
    print(f"   ‚ö†Ô∏è  GraphFrames failed: {e}")
print("\nüéâ Summary:")
print("   ‚úÖ Core Spark: Working")
print(f"   {'‚úÖ' if sedona_works else '‚ö†Ô∏è '} Sedona: {'Working (Modern)' if sedona_works else 'Failed'}")
print(f"   {'‚úÖ' if graphframes_works else '‚ö†Ô∏è '} GraphFrames: {'Working' if graphframes_works else 'Failed'}")
spark.stop()
print("üõë Test completed")
EOF
        echo "Running test..."
        if [[ -n "$SPARK_MASTER_URL" ]]; then
            distributed_spark_submit "$test_script"
        else
            default_spark_submit "$test_script"
        fi
        rm -f "$test_script"
    }
    echo "üöë Emergency function restored! Test it: test_spark_comprehensive"
}
function list_function_backups {
    local backup_dir="$HOME/.zsh_function_backups"
    echo "üìã Available function backups:"
    if [[ -d "$backup_dir" ]]; then
        ls -la "$backup_dir"
    else
        echo "   No backups found. Run 'backup_critical_functions' first."
    fi
}
# =====================================================
# SPARK ENVIRONMENT
# =====================================================
export SPARK_HOME="/opt/homebrew/opt/sdkman-cli/libexec/candidates/spark/current"
export SPARK_LOCAL_IP="127.0.0.1"
export SPARK_MASTER_HOST="127.0.0.1"
export SPARK_MASTER_PORT="7077"
export SPARK_WORKER_INSTANCES="4"
export SPARK_DRIVER_MEMORY="2g"
export SPARK_EXECUTOR_MEMORY="1g"
export SPARK_WORKER_MEMORY="2g"
export SPARK_CONF_DIR="$SPARK_HOME/conf"
export SPARK_CLIENT_CONFIG="$HOME/.spark-client-defaults.properties"
# Python paths
export PYSPARK_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"
export PYSPARK_DRIVER_PYTHON="/Users/dheerajchand/.pyenv/shims/python3"
# Add Spark to PATH
export PATH="$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"
# Additional environment variables for distributed mode
export SPARK_NUM_EXECUTORS="${SPARK_NUM_EXECUTORS:-4}"
export SPARK_EXECUTOR_CORES="${SPARK_EXECUTOR_CORES:-1}"
export SPARK_DRIVER_MAX_RESULT_SIZE="${SPARK_DRIVER_MAX_RESULT_SIZE:-2g}"
# =====================================================
# ENHANCED DEPENDENCY MANAGEMENT
# =====================================================
export DEFAULT_SPARK_JARS="org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.1,org.datasyslab:geotools-wrapper:1.7.1-28.5,graphframes:graphframes:0.8.3-spark3.5-s_2.12"
export LOCAL_SPARK_JAR_PATH="$HOME/local_jars"
mkdir -p "$LOCAL_SPARK_JAR_PATH"
# Enhanced dependency resolution with debugging
function get_spark_dependencies {
    local online_status=$(is_online)
    echo "üîç Dependency resolution:" >&2
    echo "   Online status: $online_status" >&2
    if [[ "$online_status" == "online" ]]; then
        echo "   Using online packages: $DEFAULT_SPARK_JARS" >&2
        echo "--packages $DEFAULT_SPARK_JARS"
    else
        echo "   Checking local JARs in: $LOCAL_SPARK_JAR_PATH" >&2
        local local_jars=$(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | tr '\n' ',' | sed 's/,$//')
        if [[ -n "$local_jars" ]]; then
            echo "   Using local JARs: $(echo $local_jars | tr ',' ' ' | wc -w) found" >&2
            echo "--jars $local_jars"
        else
            echo "   ‚ö†Ô∏è  No local JARs found, downloading recommended..." >&2
            download_spark_jars_if_needed
            # Try again after download
            local_jars=$(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | tr '\n' ',' | sed 's/,$//')
            if [[ -n "$local_jars" ]]; then
                echo "   Using downloaded JARs" >&2
                echo "--jars $local_jars"
            else
                echo "   ‚ö†Ô∏è  Proceeding without additional JARs" >&2
                echo ""
            fi
        fi
    fi
}
# Download essential JARs when offline
function download_spark_jars_if_needed {
    echo "üì¶ Downloading essential Spark JARs for offline use..."
    # Core JAR URLs (Maven Central)
    local jar_urls=(
        "https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.5_2.12/1.7.1/sedona-spark-shaded-3.5_2.12-1.7.1.jar"
        "https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.7.1-28.5/geotools-wrapper-1.7.1-28.5.jar"
        "https://repo1.maven.org/maven2/graphframes/graphframes/0.8.3-spark3.5-s_2.12/graphframes-0.8.3-spark3.5-s_2.12.jar"
    )
    for url in "${jar_urls[@]}"; do
        local jar_name=$(basename "$url")
        local jar_path="$LOCAL_SPARK_JAR_PATH/$jar_name"
        if [[ ! -f "$jar_path" ]]; then
            echo "  Downloading: $jar_name"
            if curl -sL "$url" -o "$jar_path"; then
                echo "  ‚úÖ Downloaded: $jar_name"
            else
                echo "  ‚ùå Failed: $jar_name"
                rm -f "$jar_path"  # Remove failed download
            fi
        else
            echo "  ‚úÖ Already exists: $jar_name"
        fi
    done
}
# Test dependency resolution
function test_spark_dependencies {
    echo "üß™ Testing Spark dependency resolution..."
    echo ""
    echo "Current status:"
    local deps=$(get_spark_dependencies 2>&1)
    echo "Dependencies resolved: $(echo "$deps" | tail -1)"
    echo ""
    echo "Local JAR inventory:"
    if [[ -d "$LOCAL_SPARK_JAR_PATH" ]]; then
        find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" -exec basename {} \; | sort
        echo "Total JARs: $(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" | wc -l)"
    else
        echo "No local JAR directory found"
    fi
}
# =====================================================
# SPARK CLUSTER MANAGEMENT
# =====================================================
function spark_start {
    echo "üöÄ Starting Spark cluster..."
    # Stop any existing processes
    pkill -f 'org.apache.spark.deploy.master.Master' 2>/dev/null
    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null
    sleep 3
    # Start master
    echo "Starting master..."
    $SPARK_HOME/sbin/start-master.sh
    sleep 5
    # Start workers (4 workers for stability)
    echo "Starting workers..."
    for i in {1..4}; do
        $SPARK_HOME/sbin/start-worker.sh spark://127.0.0.1:7077
        sleep 1
    done
    # Set master URL
    export SPARK_MASTER_URL="spark://127.0.0.1:7077"
    echo "‚úÖ Cluster started!"
    echo "üìä Master UI: http://127.0.0.1:8080"
    echo "üéØ Master URL: $SPARK_MASTER_URL"
    # Simple functionality test
    sleep 10
    spark_test_simple
}
function spark_stop {
    echo "üõë Stopping Spark cluster..."
    # Use Spark's stop scripts
    if [[ -f "$SPARK_HOME/sbin/stop-all.sh" ]]; then
        $SPARK_HOME/sbin/stop-all.sh
    fi
    # Force kill any remaining
    pkill -f 'org.apache.spark.deploy.master.Master' 2>/dev/null
    pkill -f 'org.apache.spark.deploy.worker.Worker' 2>/dev/null
    unset SPARK_MASTER_URL
    echo "‚úÖ Cluster stopped"
}
function spark_restart {
    echo "üîÑ Restarting Spark cluster..."
    spark_stop
    sleep 3
    spark_start
}
function spark_status {
    echo "üìä Spark Cluster Status:"
    echo "   Master processes: $(ps aux | grep 'spark.deploy.master.Master' | grep -v grep | wc -l)"
    echo "   Worker processes: $(ps aux | grep 'spark.deploy.worker.Worker' | grep -v grep | wc -l)"
    echo "   Master URL: ${SPARK_MASTER_URL:-'Not set'}"
    echo "   Master UI: http://127.0.0.1:8080"
    # Quick functional test
    if [[ -n "$SPARK_MASTER_URL" ]]; then
        echo "   Testing functionality..."
        spark_test_simple
    fi
}
# =====================================================
# SPARK TESTING FUNCTIONS
# =====================================================
function spark_test_simple {
    echo "üß™ Quick Spark functionality test..."
    python3 -c "
import sys
try:
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.master('spark://127.0.0.1:7077').appName('QuickTest').getOrCreate()
    result = spark.sparkContext.parallelize([1,2,3,4,5]).sum()
    print(f'‚úÖ Cluster functional: sum = {result}')
    spark.stop()
except Exception as e:
    print(f'‚ùå Test failed: {e}')
    sys.exit(1)
" 2>/dev/null
}
function test_spark_comprehensive {
    echo "üß™ Comprehensive Spark functionality test (Sedona + GraphFrames)..."
    echo "üî• RUNNING UPDATED VERSION FROM ARTIFACT - BANANA HAMMOCK! üî•"
    local test_script="/tmp/spark_comprehensive_test.py"
    cat > "$test_script" << 'EOF'
from pyspark.sql import SparkSession
print("üöÄ Starting comprehensive Spark test...")
spark = SparkSession.builder \
    .appName("ComprehensiveSparkTest") \
    .config("spark.sql.extensions", "org.apache.sedona.sql.SedonaSqlExtensions") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator") \
    .getOrCreate()
print("‚úÖ Spark Context created successfully")
print(f"   Master: {spark.sparkContext.master}")
print(f"   Default Parallelism: {spark.sparkContext.defaultParallelism}")
print("\nüîç Test 1: Basic RDD operations...")
rdd = spark.sparkContext.parallelize(range(100), 4)
result = rdd.map(lambda x: x * x).sum()
print(f"   ‚úÖ RDD computation result: {result}")
print("\nüîç Test 2: DataFrame operations...")
df = spark.range(100)
count = df.count()
print(f"   ‚úÖ DataFrame count: {count}")
print("\nüîç Test 3: SQL operations...")
df.createOrReplaceTempView("test_table")
sql_result = spark.sql("SELECT COUNT(*) as count FROM test_table").collect()[0]["count"]
print(f"   ‚úÖ SQL result: {sql_result}")
print("\nüîç Test 4: Sedona functionality...")
sedona_works = False
try:
    # Modern Sedona 1.7.1+ initialization
    from sedona.spark import SedonaContext
    sedona = SedonaContext.create(spark)
    print("   ‚úÖ Sedona context created (modern method)")
    # Test basic spatial function
    point_result = sedona.sql("SELECT ST_Point(1.0, 2.0) as point").collect()
    print("   ‚úÖ Sedona ST_Point works")
    # Test distance calculation
    distance_result = sedona.sql("""
        SELECT ST_Distance(
            ST_Point(-0.1275, 51.5072),
            ST_Point(-74.0060, 40.7128)
        ) as distance_degrees
    """).collect()[0]["distance_degrees"]
    print(f"   ‚úÖ Sedona distance calculation: {distance_result:.4f} degrees")
    # Test spatial operations with DataFrame
    spatial_df = sedona.createDataFrame([
        ("London", -0.1275, 51.5072),
        ("NYC", -74.0060, 40.7128),
    ], ["city", "longitude", "latitude"])
    spatial_with_geom = spatial_df.selectExpr(
        "city",
        "ST_Point(longitude, latitude) as geom"
    )
    geom_count = spatial_with_geom.count()
    print(f"   ‚úÖ Sedona DataFrame operations: {geom_count} geometries created")
    sedona_works = True
except Exception as e:
    print(f"   ‚ö†Ô∏è  Sedona failed: {e}")
print("\nüîç Test 5: GraphFrames functionality...")
graphframes_works = False
try:
    from graphframes import GraphFrame
    vertices = spark.createDataFrame([("A", "Node A"), ("B", "Node B"), ("C", "Node C")], ["id", "name"])
    edges = spark.createDataFrame([("A", "B", "edge1"), ("B", "C", "edge2")], ["src", "dst", "relationship"])
    g = GraphFrame(vertices, edges)
    v_count = g.vertices.count()
    e_count = g.edges.count()
    print(f"   ‚úÖ GraphFrame created with {v_count} vertices, {e_count} edges")
    # Test PageRank
    pagerank_result = g.pageRank(resetProbability=0.01, maxIter=2)
    pr_vertices = pagerank_result.vertices.count()
    print(f"   ‚úÖ PageRank completed: {pr_vertices} vertices processed")
    graphframes_works = True
except Exception as e:
    print(f"   ‚ö†Ô∏è  GraphFrames failed: {e}")
print("\nüéâ Summary:")
print("   ‚úÖ Core Spark: Working")
print(f"   {'‚úÖ' if sedona_works else '‚ö†Ô∏è '} Sedona: {'Working (Modern)' if sedona_works else 'Failed'}")
print(f"   {'‚úÖ' if graphframes_works else '‚ö†Ô∏è '} GraphFrames: {'Working' if graphframes_works else 'Failed'}")
spark.stop()
print("üõë Test completed")
EOF
    echo "Running test..."
    if [[ -n "$SPARK_MASTER_URL" ]]; then
        distributed_spark_submit "$test_script"
    else
        default_spark_submit "$test_script"
    fi
    rm -f "$test_script"
}
# Enhanced default submit with dependency resolution
function default_spark_submit() {
    local py_file="$1"
    if [[ -z "$py_file" ]]; then
        echo "Usage: default_spark_submit <python_file>"
        return 1
    fi
    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi
    echo "üöÄ Local Spark submit with enhanced dependencies..."
    local dependencies=$(get_spark_dependencies)
    # Use eval to properly expand dependencies
    eval "spark-submit \
        --master 'local[*]' \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        $dependencies \
        '$py_file'"
}
# Enhanced distributed submit
function distributed_spark_submit() {
    local py_file="$1"
    local master_url="${2:-$SPARK_MASTER_URL}"
    if [[ -z "$py_file" ]]; then
        echo "Usage: distributed_spark_submit <python_file> [master_url]"
        return 1
    fi
    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi
    if [[ -z "$master_url" ]]; then
        echo "‚ùå No master URL. Run: spark_start"
        return 1
    fi
    echo "üåê Distributed Spark submit with enhanced dependencies..."
    local dependencies=$(get_spark_dependencies)
    # Use eval to properly expand dependencies
    eval "spark-submit \
        --master '$master_url' \
        --deploy-mode client \
        --driver-memory $SPARK_DRIVER_MEMORY \
        --executor-memory $SPARK_EXECUTOR_MEMORY \
        --executor-cores 1 \
        --num-executors 4 \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        --conf spark.network.timeout=300s \
        $dependencies \
        '$py_file'"
}
# Smart environment detection submit
function smart_spark_submit() {
    local py_file="$1"
    if [ -z "$py_file" ]; then
        echo "Usage: smart_spark_submit <python_file>"
        return 1
    fi
    echo "ü§ñ Smart environment detection..."
    # Check for standalone Spark cluster
    if [ -n "$SPARK_MASTER_URL" ] && ps aux | grep -i "spark.deploy.master.Master" | grep -v "grep" > /dev/null; then
        echo "‚úÖ Local Spark cluster detected - using distributed mode"
        distributed_spark_submit "$py_file"
        return
    fi
    # Check if we can start a local cluster
    if [[ -n "$SPARK_HOME" ]] && [[ -f "$SPARK_HOME/sbin/start-master.sh" ]]; then
        echo "‚ÑπÔ∏è  No running cluster found - would you like to start one? (y/n)"
        read "start_cluster?"
        if [[ "$start_cluster" == "y" ]]; then
            spark_start
            distributed_spark_submit "$py_file"
            return
        fi
    fi
    # Fall back to local mode
    echo "‚ÑπÔ∏è  Using local mode"
    default_spark_submit "$py_file"
}
function spark_yarn_submit {
    local script_file="$1"
    local deploy_mode="${2:-client}"  # client or cluster
    if [[ -z "$script_file" ]]; then
        echo "Usage: spark_yarn_submit <script_file> [client|cluster]"
        return 1
    fi
    echo "üöÄ Submitting Spark job to YARN..."
    local dependencies=$(get_spark_dependencies)
    eval "spark-submit \
        --master yarn \
        --deploy-mode '$deploy_mode' \
        --driver-memory 2g \
        --executor-memory 1g \
        --executor-cores 2 \
        --num-executors 2 \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        $dependencies \
        '$script_file'"
}
# Enhanced Heavy API Submit Function with Better Log Control
function heavy_api_submit {
    local py_file="$1"
    local mode="${2:-auto}"  # auto, local, distributed, yarn
    if [[ -z "$py_file" ]]; then
        echo "Usage: heavy_api_submit <python_file> [mode]"
        echo "Modes: auto (default), local, distributed, yarn"
        return 1
    fi
    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi
    echo "üöÄ Heavy API Workload Submit - Optimized for API-intensive processing..."

    # Enhanced Heavy API workload optimizations with better log control
    local heavy_api_configs=(
        "--conf spark.sql.adaptive.enabled=true"
        "--conf spark.sql.adaptive.coalescePartitions.enabled=true"
        "--conf spark.serializer=org.apache.spark.serializer.KryoSerializer"
        "--conf spark.network.timeout=600s"
        "--conf spark.executor.heartbeatInterval=60s"
        "--conf spark.sql.execution.arrow.pyspark.enabled=true"
        "--conf spark.sql.adaptive.skewJoin.enabled=true"
        "--conf spark.dynamicAllocation.enabled=false"
        "--conf spark.python.worker.reuse=true"
        "--conf spark.sql.adaptive.localShuffleReader.enabled=true"
        # ENHANCED LOG CONTROL - These reduce verbosity significantly
        "--conf spark.log.level=ERROR"
        "--conf spark.sql.execution.arrow.pyspark.fallback.enabled=false"
        "--conf spark.ui.showConsoleProgress=false"
        "--conf spark.eventLog.enabled=false"
        "--conf spark.sql.execution.arrow.maxRecordsPerBatch=1000"
        "--conf spark.task.maxFailures=3"
        "--conf spark.stage.maxConsecutiveAttempts=8"
        # these are here to make Spark work Better
        "--conf spark.sql.ui.retainedExecutions=1"
        "--conf spark.sql.ui.retainedTasks=100"
        "--conf spark.ui.retainedJobs=10"
        "--conf spark.ui.retainedStages=10"
        "--conf spark.worker.ui.retainedExecutors=10"
        "--conf spark.worker.ui.retainedDrivers=10"
        "--conf spark.streaming.ui.retainedBatches=10"
        "--conf spark.eventLog.compress=true"
    )

    # Set environment variables to reduce verbosity
    export SPARK_LOG_LEVEL=ERROR
    export PYTHONHASHSEED=0
    export SPARK_LOCAL_DIRS="/tmp/spark-temp"

    local dependencies=$(get_spark_dependencies)
    local config_string="${heavy_api_configs[*]}"

    case "$mode" in
        auto)
            echo "   ü§ñ Auto-detecting best execution environment for heavy API workload..."
            if [[ -n "$SPARK_MASTER_URL" ]] && jps | grep -q ResourceManager; then
                echo "   ‚úÖ YARN available - using YARN mode for better resource management"
                mode="yarn"
            elif [[ -n "$SPARK_MASTER_URL" ]] && ps aux | grep -q "spark.deploy.master.Master"; then
                echo "   ‚úÖ Spark cluster available - using distributed mode"
                mode="distributed"
            else
                echo "   ‚ÑπÔ∏è  Using local mode with heavy API optimizations"
                mode="local"
            fi
            ;;
    esac

    case "$mode" in
        local)
            echo "   üè† Local mode with API-heavy optimizations..."
            eval "spark-submit \
                --master 'local[*]' \
                --driver-memory 4g \
                --conf 'spark.driver.maxResultSize=2g' \
                $config_string \
                $dependencies \
                '$py_file'"
            ;;
        distributed)
            if [[ -z "$SPARK_MASTER_URL" ]]; then
                echo "‚ùå No Spark cluster URL. Run: spark_start"
                return 1
            fi
            echo "   üåê Distributed mode with API-heavy optimizations..."
            eval "spark-submit \
                --master '$SPARK_MASTER_URL' \
                --deploy-mode client \
                --driver-memory 4g \
                --executor-memory 2g \
                --executor-cores 2 \
                --num-executors 4 \
                --conf 'spark.driver.maxResultSize=2g' \
                $config_string \
                $dependencies \
                '$py_file'"
            ;;
        yarn)
            if ! jps | grep -q ResourceManager; then
                echo "‚ùå YARN not running. Run: start_hadoop"
                return 1
            fi
            echo "   üéØ YARN mode with API-heavy optimizations and resource management..."
            eval "spark-submit \
                --master yarn \
                --deploy-mode client \
                --driver-memory 4g \
                --executor-memory 2g \
                --executor-cores 2 \
                --num-executors 4 \
                --conf 'spark.driver.maxResultSize=2g' \
                $config_string \
                $dependencies \
                '$py_file'"
            ;;
        *)
            echo "‚ùå Invalid mode: $mode. Use: auto, local, distributed, yarn"
            return 1
            ;;
    esac

    echo ""
    echo "üí° Heavy API Submit Optimizations Applied:"
    echo "   ‚Ä¢ Increased timeouts for slow API responses"
    echo "   ‚Ä¢ Enhanced serialization for complex data structures"
    echo "   ‚Ä¢ Adaptive query execution for varying data sizes"
    echo "   ‚Ä¢ Python worker reuse for faster API client initialization"
    echo "   ‚Ä¢ Reduced logging verbosity for cleaner output"
    echo "   ‚Ä¢ Optimized for geocoding, web scraping, and API-heavy workloads"
}

function local_heavy_api_submit() {
    local py_file=""
    local extra_paths=()
    local memory_driver="$SPARK_DRIVER_MEMORY"
    local memory_executor="$SPARK_EXECUTOR_MEMORY"
    local master="local[*]"
    local auto_detect_path=true

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            --file)
                py_file="$2"
                shift 2
                ;;
            --path)
                extra_paths+=("$2")
                shift 2
                ;;
            --driver-memory)
                memory_driver="$2"
                shift 2
                ;;
            --executor-memory)
                memory_executor="$2"
                shift 2
                ;;
            --master)
                master="$2"
                shift 2
                ;;
            --no-auto-path)
                auto_detect_path=false
                shift
                ;;
            --help|-h)
                echo "Usage: local_heavy_api_submit [OPTIONS] <python_file>"
                echo ""
                echo "Heavy API optimized local Spark submit with path management"
                echo ""
                echo "Options:"
                echo "  --file <file>           Python file to submit (or just pass as first arg)"
                echo "  --path <path>           Add additional Python path for imports"
                echo "  --driver-memory <mem>   Driver memory (default: \$SPARK_DRIVER_MEMORY)"
                echo "  --executor-memory <mem> Executor memory (default: \$SPARK_EXECUTOR_MEMORY)"
                echo "  --master <master>       Spark master (default: local[*])"
                echo "  --no-auto-path         Don't auto-detect script directory"
                echo "  --help, -h             Show this help"
                echo ""
                echo "Examples:"
                echo "  local_heavy_api_submit my_script.py"
                echo "  local_heavy_api_submit --path /my/project --path /other/libs script.py"
                echo "  local_heavy_api_submit --master local[1] --no-auto-path script.py"
                echo ""
                echo "Optimizations:"
                echo "  ‚úÖ Local filesystem only (no HDFS)"
                echo "  ‚úÖ Heavy API workload optimizations (timeouts, serialization)"
                echo "  ‚úÖ Reduced logging verbosity"
                echo "  ‚úÖ Python worker reuse for faster API clients"
                echo "  ‚úÖ Perfect for geocoding, web scraping, API-heavy tasks"
                return 0
                ;;
            -*)
                echo "‚ùå Unknown option: $1"
                echo "Use --help for usage information"
                return 1
                ;;
            *)
                if [[ -z "$py_file" ]]; then
                    py_file="$1"
                else
                    echo "‚ùå Multiple files specified: $py_file and $1"
                    return 1
                fi
                shift
                ;;
        esac
    done

    # Validate required arguments
    if [[ -z "$py_file" ]]; then
        echo "‚ùå No Python file specified"
        echo "Usage: local_heavy_api_submit [OPTIONS] <python_file>"
        echo "Use --help for more information"
        return 1
    fi

    if [[ ! -f "$py_file" ]]; then
        echo "‚ùå File not found: $py_file"
        return 1
    fi

    # Auto-detect script directory unless disabled
    if [[ "$auto_detect_path" == true ]]; then
        local script_dir=$(dirname "$(realpath "$py_file")")
        extra_paths=("$script_dir" "${extra_paths[@]}")
    fi

    # Build PYTHONPATH from extra_paths
    local python_path=""
    if [[ ${#extra_paths[@]} -gt 0 ]]; then
        python_path=$(IFS=:; echo "${extra_paths[*]}")
        echo "üìÅ Adding to PYTHONPATH: $python_path"
    fi

    echo "üöÄ Local Heavy API Submit - Optimized for geocoding/API workloads..."
    echo "üìÑ File: $py_file"
    echo "üíæ Driver Memory: $memory_driver"
    echo "‚ö° Executor Memory: $memory_executor"
    echo "üéØ Master: $master"

    # Enhanced Heavy API workload optimizations
    local heavy_api_configs=(
        "--conf spark.sql.adaptive.enabled=true"
        "--conf spark.sql.adaptive.coalescePartitions.enabled=true"
        "--conf spark.serializer=org.apache.spark.serializer.KryoSerializer"
        "--conf spark.network.timeout=600s"
        "--conf spark.executor.heartbeatInterval=60s"
        "--conf spark.sql.execution.arrow.pyspark.enabled=true"
        "--conf spark.sql.adaptive.skewJoin.enabled=true"
        "--conf spark.dynamicAllocation.enabled=false"
        "--conf spark.python.worker.reuse=true"
        "--conf spark.sql.adaptive.localShuffleReader.enabled=true"
        # LOG CONTROL - Reduce verbosity
        "--conf spark.log.level=ERROR"
        "--conf spark.sql.execution.arrow.pyspark.fallback.enabled=false"
        "--conf spark.ui.showConsoleProgress=false"
        "--conf spark.eventLog.enabled=false"
        "--conf spark.sql.execution.arrow.maxRecordsPerBatch=1000"
        "--conf spark.task.maxFailures=3"
        "--conf spark.stage.maxConsecutiveAttempts=8"
        # UI retention settings
        "--conf spark.sql.ui.retainedExecutions=1"
        "--conf spark.sql.ui.retainedTasks=100"
        "--conf spark.ui.retainedJobs=10"
        "--conf spark.ui.retainedStages=10"
        "--conf spark.worker.ui.retainedExecutors=10"
        "--conf spark.worker.ui.retainedDrivers=10"
        "--conf spark.streaming.ui.retainedBatches=10"
        "--conf spark.eventLog.compress=true"
        # LOCAL-ONLY FIXES - Prevent HDFS connection attempts
        "--conf spark.hadoop.fs.defaultFS=file:///"
        "--conf spark.sql.warehouse.dir=file:///tmp/spark-warehouse"
    )

    # Set environment variables to reduce verbosity
    export SPARK_LOG_LEVEL=ERROR
    export PYTHONHASHSEED=0
    export SPARK_LOCAL_DIRS="/tmp/spark-temp"

    # Create temp directory if it doesn't exist
    mkdir -p "/tmp/spark-temp"

    local dependencies=$(get_spark_dependencies)

    # Build the spark-submit command
    local spark_cmd="spark-submit \
        --master '$master' \
        --driver-memory $memory_driver \
        --executor-memory $memory_executor \
        --conf 'spark.driver.maxResultSize=2g'"

    # Add heavy API configurations
    for config in "${heavy_api_configs[@]}"; do
        spark_cmd="$spark_cmd $config"
    done

    # Add PYTHONPATH if we have extra paths
    if [[ -n "$python_path" ]]; then
        spark_cmd="$spark_cmd \
            --conf spark.executorEnv.PYTHONPATH='$python_path' \
            --conf spark.yarn.appMasterEnv.PYTHONPATH='$python_path'"
    fi

    # Add dependencies and file
    spark_cmd="$spark_cmd $dependencies '$py_file'"

    # Execute the command
    eval "$spark_cmd"

    echo ""
    echo "üí° Local Heavy API Submit Features Applied:"
    echo "   ‚úÖ Local filesystem only (no HDFS connection attempts)"
    echo "   ‚úÖ Python path configured for utilities module"
    echo "   ‚úÖ Increased timeouts for slow API responses (600s)"
    echo "   ‚úÖ Enhanced serialization for complex data structures"
    echo "   ‚úÖ Adaptive query execution for varying data sizes"
    echo "   ‚úÖ Python worker reuse for faster API client initialization"
    echo "   ‚úÖ Reduced logging verbosity for cleaner output"
    echo "   ‚úÖ Optimized for geocoding, web scraping, and API-heavy workloads"
}

# Convenience aliases for different use cases
alias lhas='local_heavy_api_submit'
alias lhas1='local_heavy_api_submit --master local[1]'  # Single-threaded debugging
alias lhas_debug='local_heavy_api_submit --master local[1]'  # Same as above, clearer name

# Flexible Spark Submit with All Modes
function flexible_spark_submit {
    local py_file="$1"
    local mode="${2:-smart}"  # smart, local, distributed, yarn, k8s
    if [[ -z "$py_file" ]]; then
        echo "Usage: flexible_spark_submit <python_file> [mode]"
        echo "Modes: smart (default), local, distributed, yarn, k8s"
        return 1
    fi
    case "$mode" in
        smart)
            smart_spark_submit "$py_file"
            ;;
        local)
            default_spark_submit "$py_file"
            ;;
        distributed)
            distributed_spark_submit "$py_file"
            ;;
        yarn)
            spark_yarn_submit "$py_file"
            ;;
        k8s)
            echo "üöÄ Kubernetes mode (if configured)..."
            if [[ -z "$SPARK_K8S_MASTER" ]]; then
                echo "‚ùå Kubernetes not configured. Set SPARK_K8S_MASTER"
                return 1
            fi
            local dependencies=$(get_spark_dependencies)
            eval "spark-submit \
                --master '$SPARK_K8S_MASTER' \
                --deploy-mode cluster \
                --name 'spark-k8s-app' \
                --conf spark.executor.instances=2 \
                --conf spark.kubernetes.container.image='$SPARK_K8S_IMAGE' \
                --conf spark.kubernetes.namespace='$SPARK_K8S_NAMESPACE' \
                --conf spark.kubernetes.authenticate.driver.serviceAccountName='$SPARK_K8S_SERVICE_ACCOUNT' \
                $dependencies \
                '$py_file'"
            ;;
        *)
            echo "‚ùå Invalid mode: $mode"
            echo "Available modes: smart, local, distributed, yarn, k8s"
            return 1
            ;;
    esac
}
# Backwards compatibility for your existing names
function spark_submit_local {
    default_spark_submit "$@"
}
function spark_submit_cluster {
    distributed_spark_submit "$@"
}
function graceful_spark_restart {
    spark_restart
}
function start_local_spark_cluster {
    spark_start
}
function stop_local_spark_cluster {
    spark_stop
}
function check_spark_cluster_health {
    spark_status
}
# =====================================================
# SPARK CONFIGURATION
# =====================================================
function spark_fix_logging {
    echo "üîß Reducing Spark logging noise..."
    # Create log4j2.properties to reduce INFO spam
    cat > $SPARK_HOME/conf/log4j2.properties << 'EOF'
# Reduce Spark logging noise
rootLogger.level = WARN
rootLogger.appenderRefs = stdout
rootLogger.appenderRef.stdout.ref = console
appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
# Set Spark components to WARN
logger.spark.name = org.apache.spark
logger.spark.level = WARN
logger.hadoop.name = org.apache.hadoop
logger.hadoop.level = WARN
logger.akka.name = akka
logger.akka.level = WARN
logger.jetty.name = org.eclipse.jetty
logger.jetty.level = WARN
EOF
    echo "‚úÖ Logging reduced to WARN level"
}
function show_spark_config() {
    echo "‚öôÔ∏è  Enhanced Spark Configuration:"
    echo ""
    echo "üè† Environment:"
    echo "   SPARK_HOME: ${SPARK_HOME:-'Not set'}"
    echo "   SPARK_MASTER_URL: ${SPARK_MASTER_URL:-'Not set'}"
    echo "   Java: ${JAVA_HOME:-'Not set'}"
    echo "   Hadoop: ${HADOOP_HOME:-'Not set'}"
    echo ""
    echo "üîó Hadoop Integration:"
    echo "   HADOOP_CLASSPATH: $(echo ${HADOOP_CLASSPATH:-'Not set'} | cut -c1-50)..."
    echo "   SPARK_DIST_CLASSPATH: $(echo ${SPARK_DIST_CLASSPATH:-'Not set'} | cut -c1-50)..."
    echo "   HDFS Status: $(jps 2>/dev/null | grep -q NameNode && echo '‚úÖ Running' || echo '‚ùå Stopped')"
    echo "   YARN Status: $(jps 2>/dev/null | grep -q ResourceManager && echo '‚úÖ Running' || echo '‚ùå Stopped')"
    echo ""
    echo "ü§ñ Auto-Setup System:"
    echo "   Enabled on startup: $AUTO_SETUP_ON_STARTUP"
    echo "   Check online: $AUTO_SETUP_CHECK_ONLINE"
    echo "   Verbose mode: $AUTO_SETUP_VERBOSE"
    echo ""
    echo "üì¶ Dependencies:"
    echo "   Online status: $(is_online)"
    echo "   Default JARs: $DEFAULT_SPARK_JARS"
    echo "   Local JAR path: $LOCAL_SPARK_JAR_PATH"
    echo "   Local JARs available: $(find "$LOCAL_SPARK_JAR_PATH" -name "*.jar" 2>/dev/null | wc -l)"
    echo ""
    echo "üí° Available functions:"
    echo "   # Core Spark functions"
    echo "   default_spark_submit     - Local with dependency resolution"
    echo "   distributed_spark_submit - Cluster with dependency resolution"
    echo "   smart_spark_submit       - Auto-detect execution environment"
    echo "   spark_yarn_submit        - Submit to YARN cluster"
    echo "   heavy_api_submit         - Optimized for API-heavy workloads (geocoding, web scraping)"
    echo "   local_heavy_api_submit   - Enhanced local submit with path management"
    echo "   flexible_spark_submit    - Multi-mode execution (local/distributed/yarn/k8s/smart)"
    echo ""
    echo "   # Hadoop + Spark integration"
    echo "   setup_hadoop_spark_integration - Configure Hadoop+Spark"
    echo "   setup_yarn_config        - Configure YARN"
    echo "   setup_java17_hadoop_compatibility - Fix Java 17 compatibility"
    echo "   start_hadoop             - Start Hadoop HDFS + YARN services"
    echo "   stop_hadoop              - Stop Hadoop services"
    echo "   restart_hadoop           - Restart Hadoop services"
    echo "   hadoop_status            - Check Hadoop service status"
    echo "   test_hadoop_integration  - Test basic Hadoop functionality"
    echo "   test_hadoop_spark_integration - Test Spark+Hadoop integration"
    echo "   hadoop_spark_demo        - Run comprehensive demo"
    echo ""
    echo "   # YARN Management"
    echo "   yarn_application_list    - List all YARN applications"
    echo "   yarn_kill_all_apps       - Kill all YARN applications"
    echo "   yarn_logs               - View YARN application logs"
    echo "   yarn_cluster_info        - Show YARN cluster information"
    echo ""
    echo "   # Dependency management"
    echo "   download_hadoop_slf4j_jars - Download SLF4J JARs for Hadoop"
    echo "   download_maven_jars      - Download Maven dependencies"
    echo ""
    echo "   # Testing & diagnostics"
    echo "   test_spark_comprehensive - Full Sedona + GraphFrames test"
    echo "   test_spark_dependencies  - Test dependency resolution"
    echo ""
    echo "   # Auto-setup system"
    echo "   enable_auto_setup        - Enable auto-setup on shell startup"
    echo "   disable_auto_setup       - Disable auto-setup"
    echo "   auto_setup_environment   - Run setup manually"
    echo "   setup_environment_status - Check setup status"
    echo "   show_version_strategy    - Show pinned version strategy"
    echo "   verify_version_compatibility - Check version compatibility"
    echo ""
    echo "   # Configuration backup & restore"
    echo "   backup_zshrc             - Create versioned zshrc backup"
    echo "   list_zshrc_backups       - List available backups"
    echo "   restore_zshrc [backup]   - Restore from backup (defaults to latest)"
    echo "   restore_zshrc_emergency  - Emergency restore without confirmation"
    echo ""
    echo "   # Function backup system"
    echo "   backup_critical_functions - Backup important functions"
    echo "   restore_critical_functions - Restore from backup"
    echo "   emergency_restore_test_function - Emergency restore test function"
}

# =====================================================
# ENHANCED NOTEBOOK SUPPORT (Jupyter + DataSpell)
# =====================================================

# Enhanced notebook functions for Jupyter and DataSpell
function check_notebook_dependencies {
    local notebook_type="$1"
    case "$notebook_type" in
        jupyter|jupyterlab)
            if ! command -v pyspark &> /dev/null; then
                echo "‚ùå PySpark not found! Install it with: pip install pyspark"
                return 1
            fi
            if ! command -v jupyter &> /dev/null; then
                echo "‚ùå Jupyter not found! Install it with: pip install notebook jupyterlab"
                return 1
            fi
            ;;
        dataspell)
            if ! command -v pyspark &> /dev/null; then
                echo "‚ùå PySpark not found! Install it with: pip install pyspark"
                return 1
            fi
            echo "üí° For DataSpell, ensure you have:"
            echo "   ‚Ä¢ JetBrains DataSpell installed"
            echo "   ‚Ä¢ Python interpreter configured to: $PYSPARK_PYTHON"
            echo "   ‚Ä¢ Environment variables properly set"
            ;;
        databricks)
            if ! command -v databricks &> /dev/null; then
                echo "‚ùå Databricks CLI not found! Install it with: pip install databricks-cli"
                return 1
            fi
            ;;
        vscode)
            if ! command -v code &> /dev/null; then
                echo "‚ùå VS Code not found! Ensure it's installed and available in PATH."
                return 1
            fi
            ;;
        *)
            echo "‚ùå Unknown notebook type: $notebook_type"
            echo "Available types: jupyter, jupyterlab, dataspell, databricks, vscode"
            return 1
            ;;
    esac
    echo "‚úÖ Dependencies verified for $notebook_type!"
    return 0
}

function notebook_manager {
    local notebook_type="$1"
    local port="${2:-8888}"  # Default to port 8888
    local notebook_dir="${3:-$(pwd)}"  # Default to current directory

    # Validate dependencies dynamically based on requested notebook type
    check_notebook_dependencies "$notebook_type" || return 1

    case "$notebook_type" in
        jupyter)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="notebook --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting PySpark Jupyter Notebook on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Notebook launch failed!"; return 1; }
            ;;
        jupyterlab)
            export PYSPARK_DRIVER_PYTHON="jupyter"
            export PYSPARK_DRIVER_PYTHON_OPTS="lab --no-browser --port=$port --notebook-dir=$notebook_dir"
            echo "üöÄ Starting Jupyter Lab on http://localhost:$port..."
            pyspark || { echo "‚ùå Jupyter Lab launch failed!"; return 1; }
            ;;
        dataspell)
            echo "üöÄ Setting up DataSpell environment..."
            echo "üìã DataSpell Configuration Instructions:"
            echo ""
            echo "1. Open JetBrains DataSpell"
            echo "2. Create/Open your project in: $notebook_dir"
            echo "3. Configure Python interpreter:"
            echo "   ‚Ä¢ Go to File ‚Üí Settings ‚Üí Project ‚Üí Python Interpreter"
            echo "   ‚Ä¢ Set interpreter to: $PYSPARK_PYTHON"
            echo "   ‚Ä¢ Add environment variables:"
            echo "     SPARK_HOME=$SPARK_HOME"
            echo "     HADOOP_HOME=$HADOOP_HOME"
            echo "     JAVA_HOME=$JAVA_HOME"
            echo ""
            echo "4. In your notebook, start with:"
            echo "   from pyspark.sql import SparkSession"
            echo "   spark = SparkSession.builder.appName('DataSpell').getOrCreate()"
            echo ""
            echo "üí° Your Spark configuration is already optimized for DataSpell!"

            # Open DataSpell if available
            if command -v dataspell &> /dev/null; then
                echo "üöÄ Launching DataSpell..."
                dataspell "$notebook_dir" &
            else
                echo "üí° Please open DataSpell manually and navigate to: $notebook_dir"
            fi
            ;;
        databricks)
            echo "üöÄ Launching Databricks CLI..."
            databricks workspace import_dir "$notebook_dir" || { echo "‚ùå Databricks CLI failed!"; return 1; }
            ;;
        vscode)
            echo "üöÄ Opening VS Code for notebook editing..."
            echo "üí° Install the Python and Jupyter extensions for full notebook support"
            code "$notebook_dir" || { echo "‚ùå VS Code launch failed!"; return 1; }
            ;;
        *)
            echo "‚ùå Invalid notebook type: $notebook_type. Available options: jupyter, jupyterlab, dataspell, databricks, vscode."
            return 1
            ;;
    esac
}

function pyspark_notebook {
    local notebook_type="${1:-jupyter}"  # Default to Jupyter if no type is provided
    notebook_manager "$notebook_type"
}

# Enhanced DataSpell support functions
function setup_dataspell_spark {
    echo "üîß Setting up optimal Spark configuration for DataSpell..."

    # Create a DataSpell-specific Spark configuration
    local dataspell_config="$HOME/.dataspell_spark_config.py"

    cat > "$dataspell_config" << EOF
# DataSpell + Spark Configuration
# Copy this into your DataSpell notebook cells for optimal Spark setup

import os
import sys

# Set Spark environment variables
os.environ['SPARK_HOME'] = '$SPARK_HOME'
os.environ['HADOOP_HOME'] = '$HADOOP_HOME'
os.environ['JAVA_HOME'] = '$JAVA_HOME'
os.environ['PYSPARK_PYTHON'] = '$PYSPARK_PYTHON'
os.environ['PYSPARK_DRIVER_PYTHON'] = '$PYSPARK_DRIVER_PYTHON'

# Add Spark to Python path
sys.path.append('$SPARK_HOME/python')
sys.path.append('$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip')

from pyspark.sql import SparkSession
from pyspark.conf import SparkConf

# Create optimized Spark session for DataSpell
def create_spark_session(app_name="DataSpell", memory_driver="4g", memory_executor="2g"):
    conf = SparkConf()
    conf.set("spark.driver.memory", memory_driver)
    conf.set("spark.executor.memory", memory_executor)
    conf.set("spark.sql.adaptive.enabled", "true")
    conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    conf.set("spark.hadoop.fs.defaultFS", "file:///")
    conf.set("spark.sql.warehouse.dir", "file:///tmp/spark-warehouse")
    conf.set("spark.ui.showConsoleProgress", "false")
    conf.set("spark.log.level", "WARN")

    spark = SparkSession.builder \\
        .appName(app_name) \\
        .config(conf=conf) \\
        .getOrCreate()

    # Reduce logging verbosity
    spark.sparkContext.setLogLevel("WARN")

    print(f"‚úÖ Spark session created: {spark.version}")
    print(f"‚úÖ Master: {spark.sparkContext.master}")
    print(f"‚úÖ Driver memory: {memory_driver}")
    print(f"‚úÖ UI: {spark.sparkContext.uiWebUrl}")

    return spark

# Usage in DataSpell:
# spark = create_spark_session("MyDataSpellProject")
# df = spark.range(100)
# df.show()
EOF

    echo "‚úÖ DataSpell configuration created at: $dataspell_config"
    echo ""
    echo "üìã Next steps:"
    echo "1. Open DataSpell"
    echo "2. Copy the contents of $dataspell_config into your notebook"
    echo "3. Run: spark = create_spark_session('YourProjectName')"
    echo "4. Start analyzing data!"
    echo ""
    echo "üí° This configuration includes:"
    echo "   ‚úÖ Optimized memory settings"
    echo "   ‚úÖ Reduced logging verbosity"
    echo "   ‚úÖ Local filesystem only"
    echo "   ‚úÖ Adaptive query execution"
}

function ensure_sedona_jars {
    echo "üîç Ensuring Sedona JARs are available..."

    if [[ $(is_online) == "online" ]]; then
        echo "‚úÖ Online - Sedona will be downloaded automatically via packages"
        return 0
    fi

    # Check if local JARs exist
    local sedona_jars=(
        "sedona-spark-shaded-3.5_2.12-1.7.1.jar"
        "geotools-wrapper-1.7.1-28.5.jar"
    )

    local missing_jars=()
    for jar in "${sedona_jars[@]}"; do
        if [[ ! -f "$LOCAL_SPARK_JAR_PATH/$jar" ]]; then
            missing_jars+=("$jar")
        fi
    done

    if [[ ${#missing_jars[@]} -eq 0 ]]; then
        echo "‚úÖ All Sedona JARs found locally"
        return 0
    fi

    echo "üì¶ Missing Sedona JARs: ${missing_jars[*]}"
    echo "üöÄ Downloading..."
    download_spark_jars_if_needed

    # Verify download
    local still_missing=()
    for jar in "${missing_jars[@]}"; do
        if [[ ! -f "$LOCAL_SPARK_JAR_PATH/$jar" ]]; then
            still_missing+=("$jar")
        fi
    done

    if [[ ${#still_missing[@]} -eq 0 ]]; then
        echo "‚úÖ All Sedona JARs now available"
        return 0
    else
        echo "‚ùå Failed to download: ${still_missing[*]}"
        echo "üí° Notebooks will work but without Sedona geospatial functions"
        return 1
    fi
}

function start_jupyter_with_sedona {
    local port="${1:-8889}"
    local directory="${2:-$(pwd)}"

    echo "üåç Starting Jupyter Lab with Sedona geospatial support..."

    # Ensure Sedona JARs are available
    ensure_sedona_jars

    # Start Jupyter with Sedona-optimized configuration
    start_jupyter_with_spark "$port" "$directory"
}

function start_jupyter_with_spark {

}

function test_notebook_sedona {
    echo "üß™ Testing Sedona in current environment..."

    python3 -c "
import sys
print('üöÄ Testing Sedona access...')

try:
    from pyspark.sql import SparkSession

    # Create basic Spark session
    spark = SparkSession.builder.appName('SedonaTest').getOrCreate()
    print('‚úÖ Spark session created')

    # Test Sedona
    from sedona.spark import SedonaContext
    sedona = SedonaContext.create(spark)
    print('‚úÖ Sedona context created')

    # Test spatial function
    result = sedona.sql('SELECT ST_Point(1.0, 2.0) as point').collect()
    print('‚úÖ Sedona spatial functions working!')

    # Test distance calculation
    distance = sedona.sql('''
        SELECT ST_Distance(
            ST_Point(-0.1275, 51.5072),  -- London
            ST_Point(-74.0060, 40.7128)   -- NYC
        ) as distance_degrees
    ''').collect()[0]['distance_degrees']

    print(f'‚úÖ Distance London to NYC: {distance:.4f} degrees')

    spark.stop()
    print('üéâ Sedona test completed successfully!')

except ImportError as e:
    print(f'‚ùå Import error: {e}')
    print('üí° Install Sedona: pip install apache-sedona')
    sys.exit(1)
except Exception as e:
    print(f'‚ùå Sedona test failed: {e}')
    print('üí° Try: ensure_sedona_jars')
    sys.exit(1)
"
}

function create_notebook_templates {
    local template_dir="$HOME/.notebook_templates"
    mkdir -p "$template_dir"

    echo "üìù Creating notebook templates..."

    # Jupyter template
    cat > "$template_dir/spark_jupyter_template.ipynb" << 'EOF'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Analysis Template\n",
    "\n",
    "This template provides a starting point for Spark analysis with your optimized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark is already initialized by PySpark integration\n",
    "print(f\"‚úÖ Spark version: {spark.version}\")\n",
    "print(f\"‚úÖ Master: {spark.sparkContext.master}\")\n",
    "print(f\"‚úÖ Default parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"‚úÖ UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "# Test basic functionality\n",
    "df = spark.range(100)\n",
    "print(f\"Test DataFrame count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Sedona (geospatial) functionality\n",
    "try:\n",
    "    from sedona.spark import SedonaContext\n",
    "    sedona = SedonaContext.create(spark)\n",
    "    \n",
    "    # Test basic spatial function\n",
    "    result = sedona.sql(\"SELECT ST_Point(-0.1275, 51.5072) as london_point\").collect()\n",
    "    print(\"‚úÖ Sedona geospatial functionality working!\")\n",
    "    \n",
    "    # Test distance calculation\n",
    "    distance_result = sedona.sql(\"\"\"\n",
    "        SELECT ST_Distance(\n",
    "            ST_Point(-0.1275, 51.5072),  -- London\n",
    "            ST_Point(-74.0060, 40.7128)   -- NYC\n",
    "        ) as distance_degrees\n",
    "    \"\"\").collect()[0][\"distance_degrees\"]\n",
    "    \n",
    "    print(f\"‚úÖ Distance London to NYC: {distance_result:.4f} degrees\")\n",
    "    \n",
    "    # Create a simple spatial DataFrame\n",
    "    cities_data = [\n",
    "        (\"London\", -0.1275, 51.5072),\n",
    "        (\"NYC\", -74.0060, 40.7128),\n",
    "        (\"Tokyo\", 139.6917, 35.6895)\n",
    "    ]\n",
    "    \n",
    "    cities_df = sedona.createDataFrame(cities_data, [\"city\", \"lon\", \"lat\"])\n",
    "    cities_with_geom = cities_df.selectExpr(\n",
    "        \"city\",\n",
    "        \"ST_Point(lon, lat) as geometry\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created spatial DataFrame with {cities_with_geom.count()} cities\")\n",
    "    cities_with_geom.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Sedona not available: {e}\")\n",
    "    print(\"üí° Run 'ensure_sedona_jars' in terminal to fix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GraphFrames functionality\n",
    "try:\n",
    "    from graphframes import GraphFrame\n",
    "    \n",
    "    # Create sample graph data\n",
    "    vertices = spark.createDataFrame([\n",
    "        (\"A\", \"Alice\", 34),\n",
    "        (\"B\", \"Bob\", 36),\n",
    "        (\"C\", \"Charlie\", 30)\n",
    "    ], [\"id\", \"name\", \"age\"])\n",
    "    \n",
    "    edges = spark.createDataFrame([\n",
    "        (\"A\", \"B\", \"friend\"),\n",
    "        (\"B\", \"C\", \"colleague\")\n",
    "    ], [\"src\", \"dst\", \"relationship\"])\n",
    "    \n",
    "    g = GraphFrame(vertices, edges)\n",
    "    print(f\"‚úÖ GraphFrame created with {g.vertices.count()} vertices and {g.edges.count()} edges\")\n",
    "    \n",
    "    # Show the graph\n",
    "    print(\"\\nüìä Vertices:\")\n",
    "    g.vertices.show()\n",
    "    print(\"üìä Edges:\")\n",
    "    g.edges.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è GraphFrames not available: {e}\")\n",
    "    print(\"üí° Run 'ensure_sedona_jars' in terminal (includes GraphFrames)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF

    # DataSpell Python template
    cat > "$template_dir/spark_dataspell_template.py" << 'EOF'
"""
DataSpell Spark Template
========================

This template provides optimized Spark setup for DataSpell development.
Copy this into your DataSpell notebook cells.
"""

import os
import sys

# Configure Spark environment
os.environ['SPARK_HOME'] = '/opt/homebrew/opt/sdkman-cli/libexec/candidates/spark/current'
os.environ['JAVA_HOME'] = '/opt/homebrew/opt/sdkman-cli/libexec/candidates/java/current'

# Add Spark to Python path
sys.path.append(os.environ['SPARK_HOME'] + '/python')

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *

# Create optimized Spark session
spark = SparkSession.builder \
    .appName("DataSpell") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "2g") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.ui.showConsoleProgress", "false") \
    .config("spark.log.level", "WARN") \
    .getOrCreate()

# Reduce logging
spark.sparkContext.setLogLevel("WARN")

print(f"‚úÖ Spark {spark.version} ready!")
print(f"‚úÖ Master: {spark.sparkContext.master}")
print(f"‚úÖ UI: {spark.sparkContext.uiWebUrl}")

# Test basic functionality
test_df = spark.range(100)
print(f"‚úÖ Test DataFrame count: {test_df.count()}")

# Your analysis code goes here...
EOF

    echo "‚úÖ Templates created in: $template_dir"
    echo ""
    echo "üìã Available templates:"
    echo "   ‚Ä¢ $template_dir/spark_jupyter_template.ipynb (Jupyter)"
    echo "   ‚Ä¢ $template_dir/spark_dataspell_template.py (DataSpell)"
    echo ""
    echo "üí° Usage:"
    echo "   Jupyter: Copy template to your working directory"
    echo "   DataSpell: Copy Python template content into notebook cells"
}

# Convenience functions
function jupyter_spark {
    start_jupyter_with_spark "$@"
}

function jupyter_sedona {
    start_jupyter_with_sedona "$@"
}

function dataspell_spark {
    setup_dataspell_spark
}

function notebook_help {
    echo "üìì Enhanced Notebook Support - Quick Reference:"
    echo ""
    echo "üöÄ Launch Commands:"
    echo "   notebook_manager jupyter [port] [dir]    - Start Jupyter Notebook"
    echo "   notebook_manager jupyterlab [port] [dir] - Start Jupyter Lab"
    echo "   notebook_manager dataspell [port] [dir]  - Setup DataSpell environment"
    echo "   pyspark_notebook [type]                  - Quick notebook launcher"
    echo ""
    echo "üéØ Shortcuts:"
    echo "   jupyter_spark [port] [dir]               - Optimized Jupyter Lab with Spark"
    echo "   jupyter_sedona [port] [dir]              - Jupyter Lab with Sedona geospatial"
    echo "   dataspell_spark                          - Setup DataSpell configuration"
    echo "   start_jupyter_with_spark [port] [dir]    - Enhanced Jupyter startup"
    echo ""
    echo "üåç Sedona Geospatial Functions:"
    echo "   ensure_sedona_jars                       - Download/verify Sedona JARs"
    echo "   test_notebook_sedona                     - Test Sedona functionality"
    echo "   start_jupyter_with_sedona [port] [dir]   - Jupyter with Sedona pre-configured"
    echo ""
    echo "üîß Setup Functions:"
    echo "   setup_dataspell_spark                    - Create DataSpell config file"
    echo "   create_notebook_templates                - Create starter templates"
    echo "   check_notebook_dependencies [type]      - Verify notebook dependencies"
    echo ""
    echo "üìù Templates & Examples:"
    echo "   Templates created in: ~/.notebook_templates/"
    echo "   ‚Ä¢ spark_jupyter_template.ipynb (ready-to-use Jupyter notebook)"
    echo "   ‚Ä¢ spark_dataspell_template.py (DataSpell starter code)"
    echo "   ‚Ä¢ Both include Sedona geospatial and GraphFrames examples"
    echo ""
    echo "üåç Geospatial Capabilities:"
    echo "   ‚úÖ Apache Sedona for spatial analytics"
    echo "   ‚úÖ ST_Point, ST_Distance, ST_Buffer functions"
    echo "   ‚úÖ Spatial joins and operations"
    echo "   ‚úÖ GeoJSON and WKT support"
    echo "   ‚úÖ GraphFrames for network analysis"
    echo ""
    echo "üí° Supported notebook types: jupyter, jupyterlab, dataspell, vscode, databricks"
    echo ""
    echo "üöÄ Quick Start:"
    echo "   1. ensure_sedona_jars"
    echo "   2. jupyter_sedona 8889"
    echo "   3. Open the template notebook and start coding!"
}

# =====================================================
# PATH EXPORTS & FINAL SETUP
# =====================================================
# USEFUL paths
export SIEGE="/Users/dheerajchand/Documents/Professional/Siege_Analytics"
export GEOCODE="/Users/dheerajchand/Documents/Professional/Siege_Analytics/Clients/TAN/Projects/tan_geocoding_test"
export MASAI="/Users/dheerajchand/Documents/Professional/Siege_Analytics/Clients/MI"
export RESUME_GENERATOR="/Users/dheerajchand/Documents/Professional/resume_generator"
# SDKMAN Setup
export SDKMAN_DIR=$(brew --prefix sdkman-cli)/libexec
[[ -s "${SDKMAN_DIR}/bin/sdkman-init.sh" ]] && source "${SDKMAN_DIR}/bin/sdkman-init.sh"
### MANAGED BY RANCHER DESKTOP START (DO NOT EDIT)
export PATH="/Users/dheerajchand/.rd/bin:$PATH"
### MANAGED BY RANCHER DESKTOP END (DO NOT EDIT)
# Smart conditional startup - only run if enabled
if [[ "$AUTO_SETUP_ON_STARTUP" == "true" ]]; then
    echo "üîÑ Auto-setup is enabled, running environment setup..."
    auto_setup_environment &  # Run in background to not block shell startup
fi
# Auto-backup critical functions on startup (silent)
backup_critical_functions > /dev/null 2>&1
# Auto-fix logging on startup
spark_fix_logging 2>/dev/null

# Display fortune
fortune
